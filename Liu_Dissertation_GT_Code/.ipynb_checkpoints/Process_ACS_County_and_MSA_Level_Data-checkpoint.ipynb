{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63634312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pingouin import cronbach_alpha\n",
    "from sklearn.preprocessing import scale\n",
    "import os\n",
    "import re\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25994aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore the first row and read second row as the column name\n",
    "def read_sceond_row_as_column_name(path):\n",
    "    data_df = pd.read_csv(path, skiprows=1)\n",
    "    return data_df\n",
    "\n",
    "def fill_missing_values_by_index(df, index):\n",
    "    for column_name in df.columns:\n",
    "        df[column_name] = df.groupby(index)[column_name].transform(lambda x: x.interpolate())\n",
    "        df[column_name] = df.groupby(index)[column_name].ffill()\n",
    "        df[column_name] = df.groupby(index)[column_name].bfill()\n",
    "\n",
    "    return round(df, 1)\n",
    "\n",
    "def process_acs_msa_to_dma_and_calculate_acs_variables(df):\n",
    "    df = df.groupby(['DMA', 'Year']).sum()\n",
    "    try:\n",
    "        # Households with an internet subscription / total households in the area\n",
    "        df[\"Percentage of Internet Subscription per Household\"] = df[\"B28002002\"] / df[\"B28002001\"] * 100\n",
    "    except KeyError:\n",
    "        df[\"Percentage of Internet Subscription per Household\"] = np.nan\n",
    "\n",
    "    df[\"Percentage of Foreign Born\"] = df[\"SE_A06001_003\"] / df[\"SE_A00001_001\"] * 100\n",
    "    df[\"Percentage of Moved In\"] = (\n",
    "        df[\"SE_A08001_003\"] +\n",
    "        df[\"SE_A08001_004\"] +\n",
    "        df[\"SE_A08001_005\"] +\n",
    "        df[\"SE_A08001_006\"]\n",
    "    ) / df[\"SE_A00001_001\"] * 100\n",
    "    \n",
    "    move_in_data = pd.DataFrame({\n",
    "        \"Move Within Same County\": df[\"SE_A08001_003\"],\n",
    "        \"Move from different county within same state\": df[\"SE_A08001_004\"],\n",
    "        \"Move from different state\": df[\"SE_A08001_005\"],\n",
    "        \"Move from abroad\": df[\"SE_A08001_006\"]\n",
    "    })\n",
    "    # Assuming cronbach_alpha is correctly imported and used\n",
    "    print(\"Alpha Test of Percentage of Moved In:\", cronbach_alpha(data=move_in_data))\n",
    "    \n",
    "    df[\"Percentage of Renter\"] = df[\"SE_A10062B_001\"] / df[\"SE_A00001_001\"] * 100\n",
    "    df[\"Percentage of Unemployed\"] = df[\"SE_A17002_006\"]/df[\"SE_A00001_001\"] * 100\n",
    "    df[\"Percentage of Single Parent Households\"] = df[\"SE_A10009_005\"] / df[\"SE_A10008_001\"]* 100\n",
    "    df[\"Percentage of Poverty\"] = df[\"SE_A13002_002\"] / df[\"SE_A10008_001\"]* 100\n",
    "\n",
    "    df[\"Mobility Index\"] = (\n",
    "        scale(df[\"SE_A10062B_001\"] / df[\"SE_A00001_001\"]) +\n",
    "        scale(df[\"Percentage of Moved In\"])\n",
    "    )\n",
    "    \n",
    "    \n",
    "    df[\"Average Vehicle HH\"] = (df[\"SE_A10030_003\"]*1 + #one vehicle HH\n",
    "                                df[\"SE_A10030_004\"]*2 + #two vehicle HH\n",
    "                                df[\"SE_A10030_005\"]*3 + #three vehicle HH\n",
    "                                df[\"SE_A10030_006\"]*4 + #four vehicle HH\n",
    "                                df[\"SE_A10030_007\"]*5)/df[\"SE_A10030_001\"] #five or more vehicle HH\n",
    "    \n",
    "    \n",
    "\n",
    "    disadvantage_data = pd.DataFrame({\n",
    "        \"Unemployed\": df[\"SE_A17002_006\"] / df[\"SE_A00001_001\"],\n",
    "        \"Single Parent Households\": df[\"SE_A10009_005\"] / df[\"SE_A10008_001\"],\n",
    "        \"Families Income Below Poverty\": df[\"SE_A13002_002\"] / df[\"SE_A10008_001\"],\n",
    "        \"Less than High School\": df[\"SE_A12001_002\"] / df[\"SE_A00001_001\"]\n",
    "    })\n",
    "    # Assuming cronbach_alpha is correctly imported and used\n",
    "    print(\"Alpha Test of Concentrated Disadvantage Index:\", cronbach_alpha(data=disadvantage_data))\n",
    "    \n",
    "    df[\"Concentrated Disadvantaged Index\"] = scale(disadvantage_data).sum(axis=1)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    df[\"race_total\"] = (df[\"SE_A04001_003\"] + \n",
    "    df[\"SE_A04001_010\"] +\n",
    "    df[\"SE_A04001_005\"] +\n",
    "    df[\"SE_A04001_006\"] +\n",
    "    df[\"SE_A04001_007\"] +\n",
    "    df[\"SE_A04001_008\"] +\n",
    "    df[\"SE_A04001_009\"] +\n",
    "    df[\"SE_A04001_004\"])\n",
    "    # Calculate Heterogeneity Index\n",
    "    df[\"Heterogeneity Index\"] = 1 - (\n",
    "        (df[\"SE_A04001_003\"] / df[\"race_total\"] )**2 + \n",
    "        (df[\"SE_A04001_010\"] / df[\"race_total\"] )**2 +\n",
    "        (df[\"SE_A04001_005\"] / df[\"race_total\"] )**2 +\n",
    "        (df[\"SE_A04001_006\"] / df[\"race_total\"] )**2 +\n",
    "        (df[\"SE_A04001_007\"] / df[\"race_total\"] )**2 +\n",
    "        (df[\"SE_A04001_004\"] / df[\"race_total\"] )**2\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Young Males\n",
    "    df[\"Percentage of Young Males\"] = (\n",
    "        (df[\"SE_A02002_007\"] + df[\"SE_A02002_006\"]) / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Dropped Out\n",
    "    df[\"Percentage of Dropped Out\"] = (\n",
    "        df[\"SE_A12003_002\"] / df[\"SE_A12003_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Divorced\n",
    "    df[\"Percentage of Divorced\"] = (\n",
    "        df[\"SE_A11001_006\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Population (logged)\n",
    "    df[\"Population (logged)\"] = np.log(df[\"SE_A00001_001\"])\n",
    "\n",
    "    # Calculate Percentage of Less Than High School\n",
    "    df[\"Less than High School\"] = (\n",
    "        df[\"SE_A12001_002\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Non-Hispanic White\n",
    "    df[\"Percentage of White\"] = (\n",
    "        df[\"SE_A04001_003\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Non-Hispanic Black\n",
    "    df[\"Percentage of Black\"] = (\n",
    "        df[\"SE_A04001_004\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Hispanic\n",
    "    df[\"Percentage of Hispanic\"] = (\n",
    "        df[\"SE_A04001_010\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Non-Hispanic Native and Indian\n",
    "    df[\"Percentage of Native Americans\"] = (\n",
    "        df[\"SE_A04001_005\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    df = df.rename(columns={\"SE_A00001_001\": \"Population\"})\n",
    "    # Make sure to complete any calculations and include all necessary parts\n",
    "    df.reset_index(inplace = True)\n",
    "    # Return selected columns\n",
    "    return df[[\n",
    "        \"DMA\",\n",
    "        \"Year\",\n",
    "        \"Percentage of Foreign Born\",\n",
    "        \"Percentage of Moved In\",\n",
    "        \"Percentage of Renter\",\n",
    "        \"Mobility Index\",\n",
    "        \"Concentrated Disadvantaged Index\",\n",
    "        \"Percentage of Unemployed\",\n",
    "        \"Percentage of Single Parent Households\",\n",
    "        \"Percentage of Poverty\",\n",
    "        \"Heterogeneity Index\",\n",
    "        \"Percentage of Young Males\",\n",
    "        \"Percentage of Dropped Out\",\n",
    "        \"Percentage of Divorced\",\n",
    "        \"Population (logged)\",\n",
    "        \"Less than High School\",\n",
    "        \"Percentage of White\",\n",
    "        \"Percentage of Black\",\n",
    "        \"Percentage of Hispanic\",\n",
    "        \"Percentage of Native Americans\",\n",
    "        \"Percentage of Internet Subscription per Household\",\n",
    "        \"Average Vehicle HH\",\n",
    "        \"Population\"\n",
    "    ]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# For the five-year estimate file, the Geo_FIPS is from 6 to 7 digits, they added 1 - 72 in front of the original \n",
    "# Geo_FIPS provided in the one-year estimate. In order to get rid of these numbers, I wrote a function to get the \n",
    "# last five digits of the Geo_FIPS\n",
    "'''\n",
    "\n",
    "def last_five_digits(num):\n",
    "    num_str = str(num)\n",
    "    if len(num_str) >= 6:\n",
    "        return num_str[-5:]\n",
    "    else:\n",
    "        return num_str\n",
    "\n",
    "def get_walk_acs_msa_dma(df):\n",
    "    msa_dma_crosswalk = pd.read_csv(r\"D:\\0dissertation_code_data\\dma_msa_walk2.csv\")\n",
    "    msa_dma_crosswalk = msa_dma_crosswalk.dropna(subset=[\"MSA\"])\n",
    "    msa_dma_crosswalk[\"MSA\"] = msa_dma_crosswalk[\"MSA\"].astype(int)\n",
    "    df['Geo_FIPS'] = df['Geo_FIPS'].apply(last_five_digits).astype(int)\n",
    "    \n",
    "    merged_df = pd.merge(df, msa_dma_crosswalk, left_on='Geo_FIPS', right_on='MSA')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def get_csv_files_in_root_path(root_path):\n",
    "    import os\n",
    "    file_list = []\n",
    "    # Directory containing CSV files\n",
    "    directory = root_path\n",
    "    # Get all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    # Filter CSV files\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            full_path = os.path.join(directory, file)\n",
    "            file_list.append(full_path)\n",
    "    return sorted(file_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1f51f",
   "metadata": {},
   "source": [
    "# ACS@MSA Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd76b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_msa\\acs_msa_2011.csv\n",
      "2012 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_msa\\acs_msa_2012.csv\n",
      "2013 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_msa\\acs_msa_2013.csv\n",
      "2014 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_msa\\acs_msa_2014.csv\n",
      "2015 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_msa\\acs_msa_2015.csv\n",
      "2016 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_msa\\acs_msa_2016.csv\n",
      "2017 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_msa\\acs_msa_2017.csv\n",
      "2018 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_msa\\acs_msa_2018.csv\n",
      "2019 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_msa\\acs_msa_2019.csv\n",
      "2020 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_msa\\acs_msa_2020_five_year_use_2022.csv\n",
      "2021 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_msa\\acs_msa_2021.csv\n",
      "2022 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_msa\\acs_msa_2022.csv\n"
     ]
    }
   ],
   "source": [
    "msa_df = pd.DataFrame()\n",
    "\n",
    "for path, year in  zip(get_csv_files_in_root_path(\n",
    "    r'D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_msa'), range(2011, 2023)):\n",
    "    print(year, \"\\n\", path)\n",
    "    temp_df = read_sceond_row_as_column_name(path)\n",
    "    temp_df['Year'] = year\n",
    "    # deal with internet special names (ACS13_B28002_001, ACS14_B28002_001...)\n",
    "    # Define a regex pattern to find and replace unwanted texts\n",
    "    # This pattern assumes the unwanted text ends with '_' followed by 'B28002'\n",
    "    pattern = re.compile(r'ACS\\d+_')\n",
    "    # Rename columns using regex to remove the unwanted text\n",
    "    temp_df.rename(columns={col: pattern.sub('', col) for col in temp_df.columns if 'B28002' in col}, inplace=True)\n",
    "\n",
    "    \n",
    "    msa_df = pd.concat([msa_df, temp_df], ignore_index=True)\n",
    "    msa_df.sort_values(by=['Geo_FIPS', 'Year'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e695fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_df = fill_missing_values_by_index(msa_df, \"Geo_FIPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06fe7e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_msa = get_walk_acs_msa_dma(filled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baab3165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha Test of Percentage of Moved In: (0.7185745513695108, array([0.698, 0.738]))\n",
      "Alpha Test of Concentrated Disadvantage Index: (0.8352932053188744, array([0.823, 0.847]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tosea\\AppData\\Local\\Temp\\ipykernel_15776\\374036033.py:15: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.groupby(['DMA', 'Year']).sum()\n"
     ]
    }
   ],
   "source": [
    "grouped_msa = process_acs_msa_to_dma_and_calculate_acs_variables(walk_msa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebaa5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_msa_filled = fill_missing_values_by_index(grouped_msa, \"DMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90511bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_msa_filled.to_csv(\"acs_msa_2011_2022.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec5e07",
   "metadata": {},
   "source": [
    "# ACS@County Level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1edaa9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_acs_county_to_dma_and_calculate_acs_variables(df):\n",
    "    df = df.groupby(['DMA', 'Year']).sum()\n",
    "    try:\n",
    "        # Households with an internet subscription / total households in the area\n",
    "        df[\"Percentage of Internet Subscription per Household\"] = df[\"B28002002\"] / df[\"B28002001\"] * 100\n",
    "    except KeyError:\n",
    "        df[\"Percentage of Internet Subscription per Household\"] = np.nan\n",
    "        \n",
    "    df[\"Percentage of Foreign Born\"] = df[\"SE_A06001_003\"] / df[\"SE_A00001_001\"] * 100\n",
    "    \n",
    "    df[\"Percentage of Moved In\"] = (\n",
    "        df[\"SE_A08001_003\"] +\n",
    "        df[\"SE_A08001_004\"] +\n",
    "        df[\"SE_A08001_005\"] +\n",
    "        df[\"SE_A08001_006\"]\n",
    "    ) / df[\"SE_A00001_001\"] * 100\n",
    "    \n",
    "    move_in_data = pd.DataFrame({\n",
    "        \"Move Within Same County\": df[\"SE_A08001_003\"],\n",
    "        \"Move from different county within same state\": df[\"SE_A08001_004\"],\n",
    "        \"Move from different state\": df[\"SE_A08001_005\"],\n",
    "        \"Move from abroad\": df[\"SE_A08001_006\"]\n",
    "    })\n",
    "    # Assuming cronbach_alpha is correctly imported and used\n",
    "    print(\"Alpha Test of Percentage of Moved In:\", cronbach_alpha(data=move_in_data))\n",
    "    \n",
    "    df[\"Percentage of Renter\"] = df[\"SE_A10062B_001\"] / df[\"SE_A00001_001\"] * 100\n",
    "    df[\"Percentage of Unemployed\"] = df[\"SE_A17002_006\"]/df[\"SE_A00001_001\"] * 100\n",
    "    df[\"Percentage of Single Parent Households\"] = df[\"SE_A10009_005\"] / df[\"SE_A10008_001\"]* 100\n",
    "    df[\"Percentage of Poverty\"] = df[\"SE_A13002_002\"] / df[\"SE_A10008_001\"]* 100\n",
    "\n",
    "    df[\"Mobility Index\"] = (\n",
    "        scale(df[\"SE_A10062B_001\"] / df[\"SE_A00001_001\"]) +\n",
    "        scale(df[\"Percentage of Moved In\"])\n",
    "    )\n",
    "    \n",
    "    df[\"Average Vehicle HH\"] = (df[\"SE_A10030_003\"]*1 + \n",
    "                                df[\"SE_A10030_004\"]*2 + \n",
    "                                df[\"SE_A10030_005\"]*3 + \n",
    "                                df[\"SE_A10030_006\"]*4 + \n",
    "                                df[\"SE_A10030_007\"]*5)/df[\"SE_A10030_001\"]\n",
    "\n",
    "    disadvantage_data = pd.DataFrame({\n",
    "        \"Unemployed\": df[\"SE_A17002_006\"] / df[\"SE_A00001_001\"],\n",
    "        \"Single Parent Households\": df[\"SE_A10009_005\"] / df[\"SE_A10008_001\"],\n",
    "        \"Families Income Below Poverty\": df[\"SE_A13002_002\"] / df[\"SE_A10008_001\"],\n",
    "        \"Less than High School\": df[\"SE_A12001_002\"] / df[\"SE_A00001_001\"]\n",
    "    })\n",
    "    # Assuming cronbach_alpha is correctly imported and used\n",
    "    print(\"Alpha Test of Concentrated Disadvantage Index:\", cronbach_alpha(data=disadvantage_data))\n",
    "    \n",
    "    df[\"Concentrated Disadvantaged Index\"] = scale(disadvantage_data).sum(axis=1)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    df[\"race_total\"] = (df[\"SE_A04001_003\"] + \n",
    "    df[\"SE_A04001_010\"] +\n",
    "    df[\"SE_A04001_005\"] +\n",
    "    df[\"SE_A04001_006\"] +\n",
    "    df[\"SE_A04001_007\"] +\n",
    "    df[\"SE_A04001_008\"] +\n",
    "    df[\"SE_A04001_009\"] +\n",
    "    df[\"SE_A04001_004\"])\n",
    "    # Calculate Heterogeneity Index\n",
    "    df[\"Heterogeneity Index\"] = 1 - (\n",
    "        (df[\"SE_A04001_003\"] / df[\"race_total\"] )**2 + \n",
    "        (df[\"SE_A04001_010\"] / df[\"race_total\"] )**2 +\n",
    "        (df[\"SE_A04001_005\"] / df[\"race_total\"] )**2 +\n",
    "        (df[\"SE_A04001_006\"] / df[\"race_total\"] )**2 +\n",
    "        (df[\"SE_A04001_007\"] / df[\"race_total\"] )**2 +\n",
    "        (df[\"SE_A04001_004\"] / df[\"race_total\"] )**2\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Young Males\n",
    "    df[\"Percentage of Young Males\"] = (\n",
    "        (df[\"SE_A02002_007\"] + df[\"SE_A02002_006\"]) / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Dropped Out\n",
    "    df[\"Percentage of Dropped Out\"] = (\n",
    "        df[\"SE_A12003_002\"] / df[\"SE_A12003_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Divorced\n",
    "    df[\"Percentage of Divorced\"] = (\n",
    "        df[\"SE_A11001_006\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Population (logged)\n",
    "    df[\"Population (logged)\"] = np.log(df[\"SE_A00001_001\"])\n",
    "\n",
    "    # Calculate Percentage of Less Than High School\n",
    "    df[\"Less than High School\"] = (\n",
    "        df[\"SE_A12001_002\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Non-Hispanic White\n",
    "    df[\"Percentage of White\"] = (\n",
    "        df[\"SE_A04001_003\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Non-Hispanic Black\n",
    "    df[\"Percentage of Black\"] = (\n",
    "        df[\"SE_A04001_004\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Hispanic\n",
    "    df[\"Percentage of Hispanic\"] = (\n",
    "        df[\"SE_A04001_010\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Non-Hispanic Native and Indian\n",
    "    df[\"Percentage of Native Americans\"] = (\n",
    "        df[\"SE_A04001_005\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    df = df.rename(columns={\"SE_A00001_001\": \"Population\"})\n",
    "    # Make sure to complete any calculations and include all necessary parts\n",
    "    df.reset_index(inplace = True)\n",
    "    # Return selected columns\n",
    "    return df[[\n",
    "        \"DMA\",\n",
    "        \"Year\",\n",
    "        \"Percentage of Foreign Born\",\n",
    "        \"Percentage of Moved In\",\n",
    "        \"Percentage of Renter\",\n",
    "        \"Mobility Index\",\n",
    "        \"Concentrated Disadvantaged Index\",\n",
    "        \"Percentage of Unemployed\",\n",
    "        \"Percentage of Single Parent Households\",\n",
    "        \"Percentage of Poverty\",\n",
    "        \"Heterogeneity Index\",\n",
    "        \"Percentage of Young Males\",\n",
    "        \"Percentage of Dropped Out\",\n",
    "        \"Percentage of Divorced\",\n",
    "        \"Population (logged)\",\n",
    "        \"Less than High School\",\n",
    "        \"Percentage of White\",\n",
    "        \"Percentage of Black\",\n",
    "        \"Percentage of Hispanic\",\n",
    "        \"Percentage of Native Americans\",\n",
    "        \"Percentage of Internet Subscription per Household\",\n",
    "        \"Average Vehicle HH\",\n",
    "        \"Population\"\n",
    "    ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79db2211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acs_county_walk(df):\n",
    "    county = df\n",
    "    county_dma_crosswalk = pd.read_csv(r\"D:\\0dissertation_code_data\\county_dma_crosswalk_harvard.csv\")\n",
    "    county_dma_crosswalk = county_dma_crosswalk.dropna(subset=[\"FIPS\"])\n",
    "    county_dma_crosswalk[\"FIPS\"] = county_dma_crosswalk[\"FIPS\"].astype(int)\n",
    "    county['Geo_FIPS'] = county['Geo_FIPS'].apply(last_five_digits).astype(int)\n",
    "    \n",
    "    merged_df = pd.merge(county, county_dma_crosswalk, left_on='Geo_FIPS', right_on='FIPS')\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59dfb848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_county_over_65000\\acs_county_2011.csv \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\five_year_estimates_all_county\\acs_county_5years_2011.csv\n",
      "2012 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_county_over_65000\\acs_county_2012.csv \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\five_year_estimates_all_county\\acs_county_5years_2012.csv\n",
      "2013 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_county_over_65000\\acs_county_2013.csv \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\five_year_estimates_all_county\\acs_county_5years_2013.csv\n",
      "2014 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_county_over_65000\\acs_county_2014.csv \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\five_year_estimates_all_county\\acs_county_5years_2014.csv\n",
      "2015 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_county_over_65000\\acs_county_2015.csv \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\five_year_estimates_all_county\\acs_county_5years_2015.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tosea\\AppData\\Local\\Temp\\ipykernel_15776\\611427469.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  concatenated_df['Year'] = year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_county_over_65000\\acs_county_2016.csv \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\five_year_estimates_all_county\\acs_county_5years_2016.csv\n",
      "2017 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_county_over_65000\\acs_county_2017.csv \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\five_year_estimates_all_county\\acs_county_5years_2017.csv\n",
      "2018 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_county_over_65000\\acs_county_2018.csv \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\five_year_estimates_all_county\\acs_county_5years_2018.csv\n",
      "2019 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_county_over_65000\\acs_county_2019.csv \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\five_year_estimates_all_county\\acs_county_5years_2019.csv\n",
      "2020 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_county_over_65000\\acs_county_2020_five_year_use_2022.csv \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\five_year_estimates_all_county\\acs_county_5years_2020.csv\n",
      "2021 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_county_over_65000\\acs_county_2021.csv \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\five_year_estimates_all_county\\acs_county_5years_2021.csv\n",
      "2022 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_county_over_65000\\acs_county_2022.csv \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\five_year_estimates_all_county\\acs_county_5years_2022.csv\n"
     ]
    }
   ],
   "source": [
    "# Get the ACS county data over 65,000 from one year estimate\n",
    "county_df = pd.DataFrame()\n",
    "for path_one_year_county, path_five_year_county, year in  zip(get_csv_files_in_root_path(\n",
    "    r'D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_county_over_65000'), \n",
    "                                                              get_csv_files_in_root_path(\n",
    "    r'D:\\0dissertation_code_data\\acs_2011_2022\\five_year_estimates_all_county'),\n",
    "                                                              range(2011, 2023)):\n",
    "\n",
    "    df1 = read_sceond_row_as_column_name(path_one_year_county)\n",
    "    df5 = read_sceond_row_as_column_name(path_five_year_county)\n",
    "    # Concatenate df1 and the filtered df5\n",
    "    # I use df5 to compensate the missing data for the counties of population under 65000 \n",
    "    # and those counties only have data in the five year estimates\n",
    "    unique_geo_fips = df5[~df5['Geo_FIPS'].isin(df1['Geo_FIPS'])]\n",
    "    concatenated_df = pd.concat([df1, unique_geo_fips])\n",
    "    print(year, \"\\n\", path_one_year_county, \"\\n\", path_five_year_county)\n",
    "    concatenated_df['Year'] = year\n",
    "    \n",
    "    # deal with internet special names (ACS13_B28002_001, ACS14_B28002_001...)\n",
    "    # Define a regex pattern to find and replace unwanted texts\n",
    "    # This pattern assumes the unwanted text ends with '_' followed by 'B28002'\n",
    "    pattern = re.compile(r'ACS\\d+_')\n",
    "    # Rename columns using regex to remove the unwanted text\n",
    "    concatenated_df.rename(columns={col: pattern.sub('', col) for col in concatenated_df.columns if 'B28002' in col}, inplace=True)\n",
    "\n",
    "    \n",
    "    county_df = pd.concat([county_df, concatenated_df], ignore_index=True)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79055061",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15776\\2800458320.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfilled_county_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfill_missing_values_by_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounty_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Geo_FIPS\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15776\\374036033.py\u001b[0m in \u001b[0;36mfill_missing_values_by_index\u001b[1;34m(df, index)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfill_missing_values_by_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mffill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_transform_template\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         return self._transform(\n\u001b[0m\u001b[0;32m    447\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1851\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1853\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_kernel_allowlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36m_transform_general\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[1;31m# this setattr is needed for test_transform_lambda_with_datetimetz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m             \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15776\\374036033.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfill_missing_values_by_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mffill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 )\n\u001b[1;32m--> 331\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36minterpolate\u001b[1;34m(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)\u001b[0m\n\u001b[0;32m   6061\u001b[0m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6062\u001b[0m     ) -> Series | None:\n\u001b[1;32m-> 6063\u001b[1;33m         return super().interpolate(\n\u001b[0m\u001b[0;32m   6064\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6065\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36minterpolate\u001b[1;34m(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)\u001b[0m\n\u001b[0;32m   7566\u001b[0m                 \u001b[1;34m\"those NaNs before interpolating.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7567\u001b[0m             )\n\u001b[1;32m-> 7568\u001b[1;33m         new_data = obj._mgr.interpolate(\n\u001b[0m\u001b[0;32m   7569\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7570\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36minterpolate\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"interpolate\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperiods\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36minterpolate\u001b[1;34m(self, method, axis, index, inplace, limit, limit_direction, limit_area, fill_value, downcast, **kwargs)\u001b[0m\n\u001b[0;32m   1256\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# bc overridden by ExtensionBlock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1258\u001b[1;33m         missing.interpolate_array_2d(\n\u001b[0m\u001b[0;32m   1259\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\missing.py\u001b[0m in \u001b[0;36minterpolate_array_2d\u001b[1;34m(data, method, axis, index, limit, limit_direction, limit_area, fill_value, coerce, downcast, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# for mypy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         _interpolate_2d_with_fill(\n\u001b[0m\u001b[0;32m    250\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m             \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\missing.py\u001b[0m in \u001b[0;36m_interpolate_2d_with_fill\u001b[1;34m(data, index, axis, method, limit, limit_direction, limit_area, fill_value, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[1;31m# Sequence[Sequence[Sequence[_SupportsArray[dtype[<nothing>]]]]],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;31m# Sequence[Sequence[Sequence[Sequence[_SupportsArray[dtype[<nothing>]]]]]]]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;31m# compute indices for the iteration axes, and append a trailing ellipsis to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[1;31m# prevent 0d arrays decaying to scalars, which fixes gh-8642\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m     \u001b[0minds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mndindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m     \u001b[0minds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mind\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mEllipsis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\index_tricks.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *shape)\u001b[0m\n\u001b[0;32m    665\u001b[0m             \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         x = as_strided(_nx.zeros(1), shape=shape,\n\u001b[1;32m--> 667\u001b[1;33m                        strides=_nx.zeros_like(shape))\n\u001b[0m\u001b[0;32m    668\u001b[0m         self._it = _nx.nditer(x, flags=['multi_index', 'zerosize_ok'],\n\u001b[0;32m    669\u001b[0m                               order='C')\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mzeros_like\u001b[1;34m(a, dtype, order, subok, shape)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;31m# needed instead of a 0 to get same result as zeros for string dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filled_county_df = fill_missing_values_by_index(county_df, \"Geo_FIPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_county_acs = get_acs_county_walk(filled_county_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64539db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_acs = process_acs_county_to_dma_and_calculate_acs_variables(walk_county_acs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96397e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_county_acs = fill_missing_values_by_index(county_acs, \"DMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_county_acs.to_csv(\"acs_county_2011_2022.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee0b4e",
   "metadata": {},
   "source": [
    "# ACS@State Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f2828c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_acs_state_to_dma_and_calculate_acs_variables(df):\n",
    "    df = df.groupby(['Geo_NAME', 'Year']).sum()\n",
    "    try:\n",
    "        # Households with an internet subscription / total households in the area\n",
    "        df[\"Percentage of Internet Subscription per Household\"] = df[\"B28002002\"] / df[\"B28002001\"] * 100\n",
    "    except KeyError:\n",
    "        df[\"Percentage of Internet Subscription per Household\"] = np.nan\n",
    "        \n",
    "    df[\"Percentage of Foreign Born\"] = df[\"SE_A06001_003\"] / df[\"SE_A00001_001\"] * 100\n",
    "    \n",
    "    df[\"Percentage of Moved In\"] = (\n",
    "        df[\"SE_A08001_003\"] +\n",
    "        df[\"SE_A08001_004\"] +\n",
    "        df[\"SE_A08001_005\"] +\n",
    "        df[\"SE_A08001_006\"]\n",
    "    ) / df[\"SE_A00001_001\"] * 100\n",
    "    \n",
    "    move_in_data = pd.DataFrame({\n",
    "        \"Move Within Same County\": df[\"SE_A08001_003\"],\n",
    "        \"Move from different county within same state\": df[\"SE_A08001_004\"],\n",
    "        \"Move from different state\": df[\"SE_A08001_005\"],\n",
    "        \"Move from abroad\": df[\"SE_A08001_006\"]\n",
    "    })\n",
    "    # Assuming cronbach_alpha is correctly imported and used\n",
    "    print(\"Alpha Test of Percentage of Moved In:\", cronbach_alpha(data=move_in_data))\n",
    "    \n",
    "    df[\"Percentage of Renter\"] = df[\"SE_A10062B_001\"] / df[\"SE_A00001_001\"] * 100\n",
    "    df[\"Percentage of Unemployed\"] = df[\"SE_A17002_006\"]/df[\"SE_A00001_001\"] * 100\n",
    "    df[\"Percentage of Single Parent Households\"] = df[\"SE_A10009_005\"] / df[\"SE_A10008_001\"]* 100\n",
    "    df[\"Percentage of Poverty\"] = df[\"SE_A13002_002\"] / df[\"SE_A10008_001\"]* 100\n",
    "    df[\"Less than High School\"] = df[\"SE_A12001_002\"] / df[\"SE_A00001_001\"]*100\n",
    "\n",
    "    df[\"Mobility Index\"] = (\n",
    "        scale(df[\"SE_A10062B_001\"] / df[\"SE_A00001_001\"]) +\n",
    "        scale(df[\"Percentage of Moved In\"])\n",
    "    )\n",
    "    \n",
    "    df[\"Average Vehicle HH\"] = (df[\"SE_A10030_003\"]*1 + \n",
    "                                df[\"SE_A10030_004\"]*2 + \n",
    "                                df[\"SE_A10030_005\"]*3 + \n",
    "                                df[\"SE_A10030_006\"]*4 + \n",
    "                                df[\"SE_A10030_007\"]*5)/df[\"SE_A10030_001\"]\n",
    "\n",
    "    disadvantage_data = pd.DataFrame({\n",
    "        \"Unemployed\": df[\"SE_A17002_006\"] / df[\"SE_A00001_001\"],\n",
    "        \"Single Parent Households\": df[\"SE_A10009_005\"] / df[\"SE_A10008_001\"],\n",
    "        \"Families Income Below Poverty\": df[\"SE_A13002_002\"] / df[\"SE_A10008_001\"],\n",
    "        \"Less than High School\": df[\"SE_A12001_002\"] / df[\"SE_A00001_001\"]\n",
    "    })\n",
    "    # Assuming cronbach_alpha is correctly imported and used\n",
    "    print(\"Alpha Test of Concentrated Disadvantage Index:\", cronbach_alpha(data=disadvantage_data))\n",
    "    \n",
    "    df[\"Concentrated Disadvantaged Index\"] = (scale(df[\"Percentage of Unemployed\"])+\n",
    "                                            scale(df[\"Percentage of Single Parent Households\"])+\n",
    "                                            scale(df[\"Percentage of Poverty\"])+\n",
    "                                            scale(df[\"Less than High School\"]))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    df[\"race_total\"] = (df[\"SE_A04001_003\"] + \n",
    "    df[\"SE_A04001_010\"] +\n",
    "    df[\"SE_A04001_005\"] +\n",
    "    df[\"SE_A04001_006\"] +\n",
    "    df[\"SE_A04001_007\"] +\n",
    "    df[\"SE_A04001_008\"] +\n",
    "    df[\"SE_A04001_009\"] +\n",
    "    df[\"SE_A04001_004\"])\n",
    "    # Calculate Heterogeneity Index\n",
    "    df[\"Heterogeneity Index\"] = 1 - (\n",
    "        (df[\"SE_A04001_003\"] / df[\"race_total\"] )**2 + \n",
    "        (df[\"SE_A04001_010\"] / df[\"race_total\"] )**2 +\n",
    "        (df[\"SE_A04001_005\"] / df[\"race_total\"] )**2 +\n",
    "        (df[\"SE_A04001_006\"] / df[\"race_total\"] )**2 +\n",
    "        (df[\"SE_A04001_007\"] / df[\"race_total\"] )**2 +\n",
    "        (df[\"SE_A04001_004\"] / df[\"race_total\"] )**2\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Young Males\n",
    "    df[\"Percentage of Young Males\"] = (\n",
    "        (df[\"SE_A02002_007\"] + df[\"SE_A02002_006\"]) / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Dropped Out\n",
    "    df[\"Percentage of Dropped Out\"] = (\n",
    "        df[\"SE_A12003_002\"] / df[\"SE_A12003_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Divorced\n",
    "    df[\"Percentage of Divorced\"] = (\n",
    "        df[\"SE_A11001_006\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Population (logged)\n",
    "    df[\"Population (logged)\"] = np.log(df[\"SE_A00001_001\"])\n",
    "\n",
    "    # Calculate Percentage of Less Than High School\n",
    "    df[\"Less than High School\"] = (\n",
    "        df[\"SE_A12001_002\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Non-Hispanic White\n",
    "    df[\"Percentage of White\"] = (\n",
    "        df[\"SE_A04001_003\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Non-Hispanic Black\n",
    "    df[\"Percentage of Black\"] = (\n",
    "        df[\"SE_A04001_004\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Hispanic\n",
    "    df[\"Percentage of Hispanic\"] = (\n",
    "        df[\"SE_A04001_010\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    # Calculate Percentage of Non-Hispanic Native and Indian\n",
    "    df[\"Percentage of Native Americans\"] = (\n",
    "        df[\"SE_A04001_005\"] / df[\"SE_A00001_001\"] * 100\n",
    "    )\n",
    "\n",
    "    df = df.rename(columns={\"SE_A00001_001\": \"Population\"})\n",
    "    # Make sure to complete any calculations and include all necessary parts\n",
    "    df.reset_index(inplace = True)\n",
    "    # Return selected columns\n",
    "    return df[[\n",
    "        \"Geo_NAME\",\n",
    "        \"Year\",\n",
    "        \"Percentage of Foreign Born\",\n",
    "        \"Percentage of Moved In\",\n",
    "        \"Percentage of Renter\",\n",
    "        \"Mobility Index\",\n",
    "        \"Concentrated Disadvantaged Index\",\n",
    "        \"Percentage of Unemployed\",\n",
    "        \"Percentage of Single Parent Households\",\n",
    "        \"Percentage of Poverty\",\n",
    "        \"Heterogeneity Index\",\n",
    "        \"Percentage of Young Males\",\n",
    "        \"Percentage of Dropped Out\",\n",
    "        \"Percentage of Divorced\",\n",
    "        \"Population (logged)\",\n",
    "        \"Less than High School\",\n",
    "        \"Percentage of White\",\n",
    "        \"Percentage of Black\",\n",
    "        \"Percentage of Hispanic\",\n",
    "        \"Percentage of Native Americans\",\n",
    "        \"Percentage of Internet Subscription per Household\",\n",
    "        \"Average Vehicle HH\",\n",
    "        \"Population\"\n",
    "    ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f56531ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_state\\acs_state_2011.csv\n",
      "2012 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_state\\acs_state_2012.csv\n",
      "2013 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_state\\acs_state_2013.csv\n",
      "2014 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_state\\acs_state_2014.csv\n",
      "2015 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_state\\acs_state_2015.csv\n",
      "2016 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_state\\acs_state_2016.csv\n",
      "2017 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_state\\acs_state_2017.csv\n",
      "2018 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_state\\acs_state_2018.csv\n",
      "2019 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_state\\acs_state_2019.csv\n",
      "2020 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_state\\acs_state_2020_five_year_use_2022.csv\n",
      "2021 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_state\\acs_state_2021.csv\n",
      "2022 \n",
      " D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_state\\acs_state_2022.csv\n"
     ]
    }
   ],
   "source": [
    "# Get the ACS county data over 65,000 from one year estimate\n",
    "state_df = pd.DataFrame()\n",
    "for state_one_year, year in  zip(get_csv_files_in_root_path(\n",
    "    r'D:\\0dissertation_code_data\\acs_2011_2022\\one_year_estimates_state'), \n",
    "                                 range(2011, 2023)):\n",
    "    print(year, \"\\n\", state_one_year)\n",
    "    df1 = read_sceond_row_as_column_name(state_one_year)\n",
    "    df1['Year'] = year\n",
    "    # deal with internet special names (ACS13_B28002_001, ACS14_B28002_001...)\n",
    "    # Define a regex pattern to find and replace unwanted texts\n",
    "    # This pattern assumes the unwanted text ends with '_' followed by 'B28002'\n",
    "    pattern = re.compile(r'ACS\\d+_')\n",
    "    # Rename columns using regex to remove the unwanted text\n",
    "    df1.rename(columns={col: pattern.sub('', col) for col in df1.columns if 'B28002' in col}, inplace=True)\n",
    "\n",
    "    \n",
    "    state_df = pd.concat([state_df, df1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21256c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_acs_filled = fill_missing_values_by_index(state_df, \"Geo_FIPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74996dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha Test of Percentage of Moved In: (0.7466877641776941, array([0.713, 0.778]))\n",
      "Alpha Test of Concentrated Disadvantage Index: (0.8610040649263704, array([0.842, 0.878]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tosea\\AppData\\Local\\Temp\\ipykernel_15776\\2377971010.py:2: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.groupby(['Geo_NAME', 'Year']).sum()\n"
     ]
    }
   ],
   "source": [
    "state_acs = process_acs_state_to_dma_and_calculate_acs_variables(state_acs_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "921cc518",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_acs = state_acs.rename(columns={'Geo_NAME': 'State'})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "346e7bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.995653225167935e-16"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_acs['Concentrated Disadvantaged Index'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c584db37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5546291841125961"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_acs[state_acs.State.str.contains('District of Columbia', 'Puerto')]['Concentrated Disadvantaged Index'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2de63de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
       "       'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n",
       "       'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n",
       "       'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
       "       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n",
       "       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n",
       "       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n",
       "       'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n",
       "       'Puerto Rico', 'Rhode Island', 'South Carolina', 'South Dakota',\n",
       "       'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
       "       'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_acs.State.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_acs.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835100bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_acs.to_csv(\"acs_state_2011_2022.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7651c09",
   "metadata": {},
   "source": [
    "# Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dbe4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dramatically_changed_rows(df, threshold):\n",
    "    df = df.dropna(subset=['DMA'])\n",
    "    df = df.set_index(['DMA', 'Year'])\n",
    "    unique_indexes = df.index.unique()\n",
    "    print(unique_indexes)\n",
    "    # Iterate over unique index values and inspect corresponding values\n",
    "    for index_value in unique_indexes:\n",
    "        for colname in df.columns:\n",
    "            df[colname] = pd.to_numeric(df[colname], errors='coerce')\n",
    "            # Calculate percentage change between consecutive rows\n",
    "            df[colname+'_percentage_change'] = df[colname].pct_change() * 100\n",
    "            # Replace NaNs with np.nan so they won't be considered in the comparison\n",
    "            df.fillna(value=np.nan, inplace=True)\n",
    "            # Find rows where percentage change exceeds the threshold\n",
    "            abnormal_rows = df[abs(df[colname+'_percentage_change']) > threshold][[colname+'_percentage_change']]\n",
    "            # Display abnormal rows\n",
    "            print(index_value, \"\\n\",\n",
    "                  colname, \"\\n\",\n",
    "                  abnormal_rows, \"\\n\",\n",
    "                  \"******\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e349009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dramatically_changed_rows(county_df, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dramatically_changed_rows(grouped_msa, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda0760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
