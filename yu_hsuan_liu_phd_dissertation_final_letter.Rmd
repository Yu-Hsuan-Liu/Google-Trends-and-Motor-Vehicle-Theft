
---
title: 'Exploring the Correlation Between Google Trends and Crime Statistics: A Focus
  on Motor Vehicle Theft in the United States'
author: "Yu-Hsuan `Sean' Liu"
documentclass: DissertateCUNY2

output:
  pdf_document:
    latex_engine: xelatex
    includes:
      in_header: preamble.tex
    keep_tex: true
    number_sections: true
  html_document:
    df_print: paged
  word_document: default

classoption: letterpaper
margin: 1inch
mainfont: Times New Roman
bibliography: dissertation_references_main.bib
csl: apa7.csl

nocite: |
  @Burnham2002
  @Iacobucci2007
  @Mood2010
  @Norton2012
  @Edwards2007
  @Hastie2009
params:
  run_figure: FALSE
  year: '2024'
  yearapproved: 'July 2024'
  degree: Doctor of Philosophy
  field: Criminal Justice
  chair: Kevin T. Wolff
  advisor: Kevin T. Wolff
  committee1: Amy Adamczyk
  committee2: Gohar Petrossian
  committee3: Eric Piza
  executiveOfficer: Brian Lawton

  abstract: "This dissertation examines the linear relationships between motor vehicle
    theft (MVT) estimates from Google Trends--a form of digital trace data--and official
    crime statistics, including the Uniform Crime Reports (UCR), National Crime Victimization
    Survey (NCVS), National Insurance Crime Bureau (NICB), National Incident-Based
    Reporting System (NIBRS), and 911 calls for service (CFS 911). It offers a comprehensive
    analysis of the limitations inherent in current crime statistics, discusses measurement
    errors, and explores statistical models to mitigate the impact of measurement
    error, applying these methods to validate Google Trends MVT data and other official
    MVT statistics. Furthermore, this study incorporates social disorganization theory
    and routine activity theory to test the concurrent validity of Google Trends
    MVT estimates across three different levels--state and year, Designated Market
    Area (DMA) and year, and DMA and month. The methodology employed includes generalized
    least squares (GLS), fixed effect models, natural experiment and interrupted time
    series, and instrumental variables, aiming to enhance the understanding of how
    user-generated digital trace data and its ability to make crime estimates.
    The findings confirm a linear relationship between MVT estimates from Google Trends
    and other official crime statistics, except for the NCVS. Moreover, MVT estimates
    from Google Trends show strong concurrent validity, mirroring trends and reacting
    similarly to crime covariates as other official statistics, except for temperature
    and precipitation. The conclusion discusses the further applications of Google
    Trends data, how population size influences crime data estimates, and the potentials
    and limitations of digital trace data such as Google Trends. To the best of the
    author's knowledge, this dissertation is the first to apply multiple-level crime
    statistics derived from Google Trends using robust and rigorous methods (multiple
    samples from GT) and to discuss in-depth GT's relationship with crime statistics
    and its applications in scientific studies. This dissertation not only examines
    the reliability of digital trace data but also explores its ability to illuminate
    the dark figure of crime."
---

```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
library(broom)
library(nlme)
library(corrplot)
library(cowplot)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(glue)
library(grid)
library(gridExtra)
library(gtable)
library(hablar)
library(haven)
library(janitor)
library(kableExtra)
library(knitr)
library(lmtest)
library(MASS)
library(mapproj)
library(maps)
library(MatchIt)
library(multiwayvcov)
library(pastecs)
library(plm)
library(prettyR)
library(psych)
library(psycho)
library(readxl)
library(reshape2)
library(RColorBrewer)
library(rnaturalearth)
library(rnaturalearthdata)
library(sf)
library(semPlot)
library(sp)
library(stargazer)
library(tidyr)
library(tidyverse)
library(viridis)
library(wesanderson)
library(xtable)

# Set global options
eval = TRUE
options(scipen = 999)
options(xtable.comment = FALSE)
options(knitr.kable.NA = '')

# Set knitr options
knitr::opts_chunk$set(cache = TRUE, message = FALSE, warning = FALSE, echo = FALSE)

# Set the default ggplot2 theme
theme_set(theme_bw())

```



```{r global_options, include=FALSE}
## This saves you from having to do this for every chunk
knitr::opts_chunk$set(fig.path = 'figures/',
                      echo = FALSE, 
                      cache = TRUE,
                      warning = FALSE, 
                      message = FALSE)
options(xtable.comment = FALSE)
options(knitr.kable.NA = '')


## Do NOT Remove / adds params to doc
cat(paste0("\\usepackage{xspace}\n",
      "\\newcommand{\\yeardegree}{",params$year,"\\xspace}",
      "\\newcommand{\\degree}{",params$degree,"\\xspace}\n",
      "\\newcommand{\\field}{",params$field,"\\xspace}\n",
      "\\newcommand{\\chairperson}{", params$chair, "\\xspace}\n",
      "\\newcommand{\\committeeone}{", params$committee1, "\\xspace}\n",
      "\\newcommand{\\committeetwo}{", params$committee2, "\\xspace}\n",
      "\\newcommand{\\committeethree}{", params$committee3, "\\xspace}\n",
      "\\newcommand{\\committeefour}{", params$committee4, "\\xspace}\n",
      "\\newcommand{\\gradschoolguy}{", params$gradschool, "\\xspace}\n",
      "\\newcommand{\\EO}{", params$executiveOfficer, "\\xspace}\n",
      "\\newcommand{\\advisor}{", params$advisor, "\\xspace}\n",
      "\\newcommand{\\abstract}{", params$abstract, "\\xspace}\n",
      "\\newcommand{\\blandscape}{\\begin{landscape}}\n",
      "\\newcommand{\\elandscape}{\\end{landscape}}\n",
      "\\newcommand{\\beginsupplement}{\\setcounter{table}{0}\\renewcommand{\\thetable}{A\\arabic{table}}\\setcounter{figure}{0}\\renewcommand{\\thefigure}{A\\arabic{figure}}}\n",

      "% Tables
      \\usepackage{subcaption}
      \\usepackage{lipsum}
      \\usepackage{rotating}
      \\usepackage{tikz}
      \\usepackage{hyperref}  
      \\usepackage[all]{nowidow}
      \\usepackage{xeCJK}
      \\setCJKmainfont{微軟正黑體}
      \\usepackage{dcolumn}
      \\usepackage{graphicx}
      \\usepackage{ragged2e}
      \\usepackage{pdflscape}
      \\usepackage{fontspec}
      \\usepackage[usegeometry]{typearea}
      \\newenvironment{uselscape}{%
        \\KOMAoptions{paper=landscape, DIV=current}
        \\newgeometry{top=1in, bottom=1in, right=1in, left=1in}
        \\fancyheadoffset{0\\linewidth}
      }{%
      \\cleardoublepage
      }
      \\usepackage{caption}
      \\usepackage{enumitem}
      \\usepackage{tocloft}
      \\usepackage{amsmath}
      \\usepackage[none]{hyphenat}
      \\usepackage{booktabs}
      \\usepackage{threeparttable}
      \\usepackage[utf8]{inputenc}
      \\usepackage[font=itshape]{quoting}
      \\usepackage{array}
      \\usepackage [provide=english]{babel}
      \\usepackage [autostyle, english = american]{csquotes}
      \\MakeOuterQuote{\"}
      \\newcolumntype{x}[1]{%
      >{\\centering\\arraybackslash}m{#1}}%
      \\usepackage{placeins}
      \\usepackage{chngcntr}
      \\counterwithin{figure}{chapter}
      \\counterwithin{table}{chapter}
      \\usepackage{lipsum}
      \\usepackage[letterpaper, margin = 1in]{geometry}
      \\usepackage[makeroom]{cancel}\n"
      ),file = "preamble.tex")
```





<!-- Title page printed automatically -->
\copyrightpage
\approvalpage
\abstractpage




<!-- Acknowledgments -->
\newpage
\fancyhead[L]{Acknowledgments}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}
\chapter*{ACKNOWLEDGMENTS}


<!--\addcontentsline{toc}{section}{Acknowledgments}-->

I wish to express profound gratitude to my dissertation chairperson, Dr. Kevin T. Wolff, for his insightful guidance. I am incredibly fortunate to have had such an exemplary chairperson. The commitment to meet with a student every Wednesday for almost one year is tremendous, and you excelled in every aspect. Your support has been the driving force behind my progress, and I am deeply grateful for it.

My heartfelt thanks go to my committee members: Dr. Amy Adamczyk, Dr. Gohar Petrossian, and Dr. Eric Piza, who provided invaluable suggestions and dedicated their time in reviewing this dissertation. Special thanks to Dr. Michael Maxfield for his insightful suggestions in the early phases of this work. I am deeply grateful to Dr. Hung-En Sung, Dr. Ivan Sun, Dr. Chia-Liang Dai, and Dr. Ching-Chen Chen, who not only provided valuable suggestions but also offered emotional support during our challenging times.

Special thanks to my professors in Taiwan who provided their warmth and wisdom to support us: Dr. Sandy Yu, Dr. Roberto Ren-Rang Chyou, Dr. Charles Kuang-Ming Chang, and Dr. Yung-Lien Lai.

Heartfelt appreciation to Jamie Yeh, Joan Shen, Allies Lo, and Marquis Yeh for their unwavering support in helping us transition from Taiwan to New York and New Jersey. Spending our holidays with you has truly made us feel at home.

Gratitude is also due to Taiwanese community in South Brunswick, New Jersey, Dr. Tian-Jian Hsu and Dr. Chi-Wei Lu, Jason Tsai and Cheri Chu, Dr. Hung-Lung Wei and Jolie Chuang, along with our South Ridgewoods neighbors, Sumen, Simon, and Koti, for their friendship and encouragement.

Special thanks to Dr. Wei-Chih Chen, the best classmate in middle school in 南門國中, I am glad our path crossed again here in New York.

Kudos to my peers in the PhD program at John Jay, the class of 2023: Sarah, Molly, Lauren, Paul, Monique, Irina, Michelle, Samantha, and Sabrina. Our "Third Thirsty Thursday" gatherings consistently left us eager for the next. Additionally, heartfelt thanks to friends at JJ and GC, Dr. Yu-Chen Ho and Dr. Yen-Chiao Liao.

I am immensely thankful for the warm support from my family members in Taiwan, both in Taipei and Kaohsiung.

The adventure of Ph.D. began in 2018, and I am immensely grateful to learn, study, and work alongside my best friend, co-author, esteemed colleague, and beloved wife, Tzu-Ying Lo. Thank you for embarking on this journey with me. I cherish every moment by your side, and I am confident that together, we can achieve even greater things.

This dissertation has received funding from: Dr. James Fyfe Award: \$3,000; CUNY Graduate Center Provost’s Pre-Dissertation Research Fellowship: \$4,000, and  Taiwan Government Scholarship to Study Abroad (GSSA): \$32,000


<!-- Front Matter -->
\newpage
\fancyhead[L]{Table of Contents}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}
\tableofcontents

\newpage
\fancyhead[L]{List of Tables}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}
\makeatletter
\renewcommand*\l@table{\@dottedtocline{1}{0em}{2.3em}}
\makeatother
\listoftables

\newpage
\fancyhead[L]{List of Figures}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}
\makeatletter
\renewcommand*\l@figure{\@dottedtocline{1}{0em}{2.3em}}
\makeatother
\listoffigures




\newpage
\pagenumbering{arabic}

<!-- Dedication -->
\newpage
\fancyhead[L]{Dedication}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}
\chapter*{DEDICATION}


\addcontentsline{toc}{section}{Dedication}

<!-- text can be added directly -->

\begin{center}
\textit{\normalsize{Dedicated To}}\newline
\textit{\normalsize{Mother, 吳寶春 (Pao-Chun Wu), Father, 劉忠桓 (Jong-Hwan Liou)}} \newline
\textit{\normalsize{Mother In Law, 蔡宜蓁 (Yi-Chen Tsai), Father In Law, 駱文雄 (Wen-Hsiung Lo)}} \newline
\textit{\normalsize{Son, 劉品序 (Pin-Hsu `Justin' Liu), and Love, 駱姿螢 (Tzu-Ying `Michelle' Lo)}}
\end{center}

<!-- Introduction -->
\newpage
\fancyhead[L]{Chapter 1: Introduction}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}

\chapter{INTRODUCTION}

<!-- text can also be added by referring to another rmd file
{r child = 'samplEntroduction.Rmd'} -->

\begin{quote}
\emph{Concepts, definitions, quantitative models, and theories must be adjusted to the fact that the data are not some objectively observable universe of ``criminal acts,'' but rather those events defined, captured, and processed as such by some institutional mechanism.}\end{quote}[@biderman1967exploring]

# Problem Statement
## Issues of Measuring Crime
The concept of the "dark figure of crime" refers to criminal incidents that remain unreported, either because victims choose not to report them or due to recording errors by law enforcement agencies. This term underscores a significant limitation in current crime statistics, which only include crimes that are "known to the police." To address this issue, initiatives such as the National Crime Victimization Survey (NCVS) have been developed. The NCVS serves as an alternative crime statistic intended to mitigate the impact of the dark figure of crime by surveying households in the United States, intending to capture a broader spectrum of victimization. Despite these efforts, challenges such as budget constraints and methodological limitations hinder the NCVS from fully depicting the true extent of victimization, underscoring the complexities involved in accurately measuring the prevalence of criminal activities.


## Introduction to Google Trends
The global prevalence of the internet enables the use of the records of people's online behaviors in social science research. In 2018, 92% of American homes possessed at least one internet-equipped device (desktops, laptops, tablets, and smartphones), and 85% had a broadband internet subscription [@bureau2021computer]. In 2021, over 93% of adults in the United States reported using the internet [@center2021internetbroadband]. Additionally, internet users are distributed similarly to the broader population regarding ethnicity, wealth, gender, and age [@center2021internetbroadband]. Furthermore, 91% of internet users use search engines, with 86.95% of search engine users using Google [@center2012searchengine; @statcounter2022search]. The extensive use of Google's search engine, along with the varied demographics of internet users, presents a distinctive potential to exploit Google Search data for the discovery of insights and metrics that were previously inaccessible.

Google Trends (GT) is a Google search extension that collects user-generated data on Google searches. It provides information of the search's time, location, and keywords. GT generates data on the search rates for keywords based on the acquired data to predict users' search interests. Due to the widespread use of Google Search, a vast amount of data is recorded from the extensive user base during these processes. The GT data generally reflect user's interest toward a topic across regions or time. Therefore, GT data represents a valuable resource for studies where existing properties are unavailable or challenging to measure.

Google Trends has proven valuable across multiple academic fields. Studies utilizing this tool have explored topics such as racial animus towards a Black political candidate, religious demographics, concerns about police violence, and spikes in domestic violence during the COVID-19 pandemic [@adamczyk2021understanding; @stephens2014cost; @gross2017there; @anderberg2022quantifying]. A study by @liu2023big examined the correlation between crime estimates derived from GT and official crime statistics from the UCR for the period 2010 to 2019 at Designated Market Areas (DMA). According to @dma_nielsen, DMAs are television market zones established to monitor local viewership across the United States, including Hawaii and parts of Alaska. These 210 distinct regions, defined by Nielsen based on television viewership dominance, are delineated by zip codes and county lines. DMAs provide critical data for marketers, researchers, and businesses aiming to tailor their strategies to specific locations. Google Trends utilizes DMAs as one of its regional levels to aggregate search activity across the United States. Demographic data within DMAs can be aggregated from the county level using a county-DMA crosswalk [@sood2016]. @liu2023big revealed that using GT to estimate the prevalence of rape may be more reliable than using the data from traditional sources like the UCR. However, the study's cross-sectional design, which lacked a temporal dimension and was confined to the DMA level, highlighted significant limitations, underscoring the need for more comprehensive research to unravel the complex dynamics between Google Trends and official crime statistics. To address these limitations, this dissertation aims to enhance the understanding of GT crime estimates by examining the relationship between GT estimates of crime and official crime statistics, particularly motor vehicle theft, known for its reliability. Further, this research employed multiple official crime data sources, including UCR, NCVS, NICB, NIBRS, and CFS 911, conducting analyses at three different levels--state-year, DMA-year, and DMA-month--using panel data and various statistical models. This investigation includes a temporal analysis across various geographic levels, aiming to provide a detailed perspective and rigorous evidence on the reliability and validity of GT crime estimates.


## Outline of Chapters

This dissertation comprises eight chapters that advance the exploration of the correlation between Google Trends Motor Vehicle Theft (GT MVT) and official MVT statistics. Chapter 1 lays the foundation with an introductory overview, setting the stage for a comprehensive investigation into crime reporting and analysis. 

Chapter 2 introduces different types of traditional crime statistics, offering an in-depth look at how data on traditional estimates of criminal activity are collected and applied in criminology. This chapter also examines the limitations and discussed the challenges stemming from the "dark figure of crime," police recording issues, and other critiques posited on official crime statistics.

Chapter 3 shifts focus to a detailed examination of measurement error, introducing the classic measurement error model and methods to mitigate its impact. This sets the stage for a comprehensive methodological framework for further analysis. The discussion is extended to include measurement errors in official crime statistics, the misuse of these statistics, and strategies, such as instrumental variables model multiplicative and log-transformed regression models to mitigate reporting errors. 

Chapter 4 explores the application of big data and digital trace data in social science studies. It introduces Google Trends data, discussing how it is constructed and its applications in social sciences. The chapter also examines the current use of Google Trends in criminal justice studies and highlights the limitations of GT data. Critical considerations for utilizing this novel data source are emphasized throughout the chapter.

Chapter 5 explains why Motor Vehicle Theft (MVT) was chosen as the focal crime category for this dissertation and discusses how existing criminological theories are applied to understand MVT. It examines the impact of social disorganization by looking at such factors as concentrated disadvantage, heterogeneity, and residential mobility and their connections with MVT. Additionally, the chapter discusses routine activity theory with variables like product price, temperature, precipitation, and the percentage of young males, and how these factors are associated with MVT.


Chapter 6 outlines the research questions, the units of analysis, and the data and methods applied in this dissertation. The study is conducted at three different level: state-year, DMA-year, and DMA-month. This chapter discusses data sourced from UCR, NCVS, National Insurance Crime Bureau (NICB), NIBRS, and CFS 911, and introduces other variables deriving from criminological theories that are used in the analysis. The statistical methods employed include generalized least squares (GLS), fixed effects models, instrumental variables, and natural experiments and interrupted time series.

Chapter 7 presents the results, providing a detailed discussion on the statistical tests used and the findings derived from these analyses. This chapter highlights the statistical significance of the findings and addresses the research questions regarding the linear relationship between GT MVT and other official crime statistics. It also presents evidence of the concurrent validity of GT MVT by incorporating various crime covariates into the models, offering a robust examination of the relationships explored.

Chapter 8 reflects on the contributions of this dissertation, including its impact on current methodology, theory, and policy implications. It discusses how digital trace data, like Google Trends, can enhance our understanding of crime statistics and the association between real-life crime events and people's search behaviors. This chapter discusses the linear relationship between GT MVT and other official crime statistics, and the concurrent validity of GT MVT with other crime covariates. It also explores how factors like population size and the availability of publicly accessible data associates with crime estimates. Furthermore, this chapter discusses the potential for extending GT MVT to include crimes that are more likely to be underreported, presenting the rationale behind this approach. Finally, this chapter discusses the limitations of the dissertation and highlights its contributions to criminological research.


\FloatBarrier

<!-- Chapter 2 -->
\newpage
\fancyhead[L]{Chapter 2: Traditional Crime Statistics, Their Applications and Limitations}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}

\chapter{TRADITIONAL CRIME STATISTICS, THEIR APPLICATIONS AND LIMITATIONS}
\begin{quote}
\emph{The bliss associated with ignorance of UCR error may fade as we learn more about the nature of UCR data quality, but the quality of scientific research will increase.}\end{quote}[@loftin2010use]

# Traditional Crime Statistics and Their Applications
Crime statistics are a vital resource for understanding the effects of crime on society. It is used to guide policy decisions, evaluate the efficacy of law enforcement, and provide insights into the causes and effects of crime. Traditional sources of crime data, such as the Uniform Crime Reports (UCR), the National Incident-Based Reporting System (NIBRS), and the National Crime Victimization Survey (NCVS) have been used to analyze crime for decades. These sources have contributed essential data on crime trends, but they are not without their limitations. This chapter will examine the traditional sources of crime statistics, their applications in criminal justice, and their acknowledged limitations.

## Uniform Crime Reports (UCR)
The FBI's Uniform Crime Reporting Program, established in 1930, is the oldest system in the U.S. for recording crimes known to the police. The Department of Justice collects and publishes \href{https://ucr.fbi.gov/crime-in-the-u.s}{\emph{Crime in the United States}} [@investigation2020crime] on an annual basis, with 97% of the population represented by over 18,000 agencies contributing data to the program voluntarily with the support of federal funding in training [@investigation2020crime]. Before the National Crime Survey was created in the early 1970s, the UCR data was the only available tool for assessing crime and identifying crime patterns and trends [@mcdowall2021uniform; @stogner2016uniform; @levinson2002encyclopedia; @fox2006encyclopedia]. UCR data has been used to measure the prevalence of crime in the U.S., and it is widely utilized by researchers, law enforcement, news media, and policy makers [@us_department_of_justice_uniform_2004; @fbi_ucr_desc_2023].

The UCR Program categorized offenses into two groups: Part I crimes and Part II crimes. Part I crimes were considered the most serious and had generally consistent definitions across different states. Part I crimes included homicide, rape, robbery, aggravated assault, burglary, larceny-theft, motor vehicle theft, arson and human trafficking. While encompassing more crime types, Part II crimes were considered less serious than Part I crimes. Notably, the data for Part II only reflected arrest counts, while both arrest and offense counts were recorded for Part I crimes [@ucr2013manualhierarchyrules; @mcdowall2021uniform; @stogner2016uniform]. Part II crimes include fraud, forgery and counterfeiting, embezzlement, buying, receiving, and possessing stolen property, vandalism, carrying and possessing weapons, prostitution, drug abuse violations, gambling, driving under influence, disorderly conducts, and other misdemeanors [@ucr2013manualhierarchyrules]. UCR serves as a significant data source for hate crime statistics as well. While the definition of hate crime has undergone expansion since the FBI began recording it in 1990 [@nolan2002hate; @holder2022convergence], the rate of reporting and recording hate crimes remains relatively low when compared to other forms of criminal activity. UCR data from 1990 to 2018 reveals that 97% of agencies reported no hate crime incidents. However, this statistic likely reflects limitations in reporting practices, not necessarily the absence of such crimes. [@rapepolicehidekaplan2021ucrbook].

For the same incident with multiple types of offenses, UCR only record the most serious type of crime. This is also known as the “hierarchy rule” [@ucr2013manualhierarchyrules; @levinson2002encyclopedia]. Hierarchy rule ranks each offense of Part I in order of severity and decides which crime to record. The ranks are as follows: homicide, rape, robbery, aggravated assault, burglary, larceny theft and motor vehicle theft. The hierarchy rule excludes justifiable homicide (e.g., in a justifiable homicide case involving a robbery, the robbery will be recorded instead of homicide), motor vehicle theft (e.g., even if property was also stolen along with the car, the case will still be recorded as motor vehicle theft), arson, and human trafficking [both arson and human trafficking were recorded separately from other Part I crime if the occurred in the same incident, @ucr2013manualhierarchyrules]. The system excluded crimes that were not the most serious under the hierarchy rule. 

Furthermore, the Supplementary Homicide Reports (SHR), which is another dataset voluntarily reported under the Uniform Crime Reporting (UCR) program, were employed to document and disseminate homicide statistics. Studies found that homicide and motor vehicle theft were the two crimes with the highest validity and reliability in terms reporting [@o1980empirical; @lauritsen2016choice;@GIBSON2008247;@ansari2015convergence]. 

In the 1980s, the FBI and the Bureau of Justice Statistics commissioned a team to reevaluate the Uniform Crime Reporting Program. This resulted in the recommendation of the reform of UCR Program toward a more incident-based crime recording system, which outlines the demographics of offenders and victims as well as other details of a given incident.  In 1991, South Carolina was the first state to transmit crime statistics to the FBI in the National Incident-Based Reporting System (NIBRS) format [@barnett2007introduction; @addington2021national]. The reform suggestions became today's NIBRS. The FBI has prioritized NIBRS implementation over the UCR, and in January 1, 2021, the national UCR Program switched to NIBRS-only data collection and stopped publishing the annual data of UCR. The FBI and BJS also provide resources available to assist agencies in addressing the cost and perception of the transition of crime statistics [@fbi_nibrs_desc_2023]. We will discuss NIBRS program as another category of crime statistics as well as the use of its data in criminological research in the following sections. 

## Examples of Applying UCR in Criminal Justice Research
The use of UCR data in criminal justice is extensive. Its statistics have been employed to assess the impact of increasing police numbers on crime reduction [@levitt1998relationship; @lin2009more], as well as the cost/benefit analysis of recruiting police personnel and crime reduction [@KOVANDZIC200265; @kaplan2019more; @evans2007cops]. UCR was also applied to evaluate the effect of concealed carry laws on homicide and violent crimes [@lott1997crime; @hamill2019state], and compared with the trends of NCVS [@biderman2012understanding; @schwartz2009trends; @steffensmeier2006gender]. It has also been used to evaluate the impact of local or national criminal justice programs on crime rates/recidivism rates, such as the State Criminal Alien Assistance Program (SCAAP) on crime rates [@lilley2009crime], and the effectiveness of the national program Project Safe Neighborhoods (PSN) in reducing violent crimes [@mcgarrell2010project]. The study conducted by @addington2006using utilized the homicide data from the Supplementary Homicide Reports (SHR) in order to examine the clearance rate. The hate crime data from the UCR has also been utilized to assess the increase in incidents of Anti-Asian hate crime during the COVID-19 pandemic [@gover2020anti]. The UCR arrest data was utilized to examine the patterns pertaining to the arrest of female offenders from 1960 to 1990. Steffensmeier's study [-@steffensmeier1993national] revealed that there is a correlation between the arrest of female offenders and the general increase in property crime.



## National Incident-Based Reporting System (NIBRS)


National Incident-Based Reporting System (NIBRS) is an incident-based crime reporting system, in contrast to UCR’s summary reporting system based on "hierarchy rules," which only records the most serious crime in an incident involving multiple offenses. In order to enhance the accuracy and usefulness of crime data, the Criminal Justice Information Services Advisory Policy Board recommended, and the FBI subsequently approved, a nationwide transition from the UCR Summary Reporting System (SRS) to the National Incident-Based Reporting System (NIBRS) by January 1, 2021 [@fbi_nibrs_desc_2023]. The FBI is actively collaborating with the law enforcement community to facilitate and encourage participation in this transition, with public support from key organizations such as the International Association of Chiefs of Police, the Major Cities Chiefs Association, the Major County Sheriffs of America, and the National Sheriffs' Association, signaling a collective commitment to improving crime reporting and analysis [@lantz2022more].

NIBRS records eight Part I crimes and 38 other types of offenses. For the numbers of arrests, NIBRS records 49 types of offenses other than the eight Part I crimes [@rantala2000effects]. The detailed information NIBRS provided are based on the following six categories within a single incident recording: Administrative (incident date/hour, cleared exception...etc.), Offense (type of force/weapon involved, method of entry, location type...etc.), Victim (type of injury, residence status, race...etc.), Property (value of property, number of recovered/stolen motor vehicles, suspected drug type...etc.), Offender(race, sex, age...etc.), and Arrestee [race, age, date...etc. For more information, see @akiyama1999methods]. Each NIBRS incident report includes 53 data points across six categories [@akiyama1999methods; @addington2021national].

While NIBRS currently encompasses a smaller share of the population and agencies compared to UCR's peak (69% in 2022 versus UCR's 97%) [@marshall_2023], research suggests a high degree of comparability between NIBRS and UCR data. When comparing data from the same year, NIBRS and UCR rates show minimal differences. Notably, murder rates are identical, while rape, robbery, and aggravated assault rates are around 1% higher in NIBRS. Burglary rates are slightly lower in NIBRS (0.5%), while larceny and motor vehicle theft rates are consistently higher (3.4% and 4.5%, respectively) [@rantala2000effects]. When it comes to arrest data, NIBRS and UCR data continue to correspond closely, and differences between demographic measures are negligible [@pattavina2017assessing]. Overall, NIBRS shifts away from the aggregated recording system and provides further descriptive indicators of each criminal incident known to the police. It provides more ways of purposeful comparisons across space and time. It also serves to professionalize policing and enables more deliberate and responsive allocations of program, policy, and resources [@strom2017future]. This nationwide detailed incident-based collection of crimes known to the police, can complement NCVS and eventually replace the UCR's summary reporting system. [@strom2017future]

## Examples of Applying NIBRS in Criminal Justice Research
The NIBRS offers a wealth of data, including details on the demographics of both criminals and victims, making it a highly useful tool for academics. Roberts and their colleagues tested the clearance rate after a police lethal violence incident and found a negative relationship between the clearance rate and the police lethal violence incidents [@roberts2022clearing]. Similarly, @BLOCK2022101987 assessed the "Cinderella effect" (stepparents are more likely than biological parents to injure/harm/kill their children) using the information from the relationship between victims and perpetrators. They found that unmarried partners, rather than stepparents, are more likely than biological parents to seriously injure the child. @fone2023you used incident data gathered surrounding football games to assess the impact of alcohol sales on crime. The findings demonstrated that the existence of alcohol sales did not result in an increase in alcohol-related crime. @paintsil2022does tested the devaluation thesis (crimes against minorities are less likely to be cleared than crimes against White victims) using NIBRS data, and they applied victims' race to both lethal and non-fatal crime occurrences. Although the results were mixed, they demonstrated how NIBRS data may be used to test criminological theories. The comprehensive incident-level information from NIBRS allows for the testing of a variety of things that were not possible with the UCR data.

These innovative NIBRS data applications bring new insights/evidences into criminal justice studies, and the usage of NIBRS data should be broadened for future studies, with an emphasis on more specific information on crime/space/time/victims/offenders. There is little question that NIBRS data has the potential to significantly contribute to criminology and criminal justice research. However, the NIBRS data remains undervalued and underutilized, which is likely to change as more agencies participate in NIBRS reporting in the years to come [@lantz2022more]. 

## 911 Calls for Service (CFS 911)
Calls for service (CFS 911) data is collected by local authorities when a citizen dials 911 or a non-emergency number, such as 311, to request police assistance. The 911 system was first used in the United States in Alabama in 1968, and it quickly became a nationwide universal emergency number due to its ease of remembering and dialing [@vera2019calls; @McEwen2006encyclopedia]. The Federal Communications Center designated 311 as a non-emergency phone number in 1997 in order to divert the overload of non-emergency 911 calls. In addition to 911 and 311, several agencies provided local numbers or online reporting systems (typically for non-emergency reports), which were also recorded in the CFS data. The data also includes occurrences from officers receiving citizen reports on patrol [@vera2019calls; @McEwen2006encyclopedia]. In 2016, mobile devices accounted for making about 80% of 911 calls. The increasing use of cell phones not only increased the number of 911 calls, but also made it more difficult to pinpoint the caller's location because, unlike residential phones, cell phones did not provide an accurate address through the PSAP (Public Safety Answering Point) system [@vera2019calls]. By the end of the 20th century, the 911 service was available to 93% of the population and 96% of the geographic areas in the United States [@911_origin_history].

Importantly, the discretion of dispatchers has a significant impact on the outcomes of an incident. It is the dispatcher's interpretation of the situation that determines whether or not the call is added to the database, if an officer needs to be dispatched to the scene, and if the incident is deemed an emergency or not [@simpson2021calling; @McEwen2006encyclopedia; @nj911calls; @madison_police_2023; @alamedaca911calls]. Calls are usually classed by category and priority by the police agency. Crimes in progress, catastrophic traffic accidents, and other types of calls that require police to attend as soon as possible are examples of emergency calls [@simpson2021calling; @McEwen2006encyclopedia]. According to a study in Philadelphia in 2019, over one-third of CFS (1.07 million dispatched out of 3.3 million calls) resulted in the dispatch of the police [@simpson2021calling]. The same study in Philadelphia in 2019 revealed that 39.9% of inquiries were for criminal activity, 23.0% were for quality (e.g., loud music, illegal parking), 11.3% were for traffic (e.g., accidents), 10.2% were for community (e.g., missing person), 7.6% were proactive (investigation), and 7.4% were for medical/public health for the calls which police officers were dispatched to [@ratcliffe2021policing]. 

@lum2022can conducted an extensive analysis of a large dataset comprising millions of computer-aided dispatch (CAD) calls from nine different agencies in the United States. The distribution of call categories revealed that, on average, 16.8% of calls pertained to traffic-related issues. Inquiries relating to criminal activities accounted for the majority, including 45.6% of calls. Within this category, suspicious activities constituted 12.8%, property-related matters accounted for 10.2%, incidents involving violence represented 6.4%, and disorders contributed 16.2%. Follow-ups and service requests made up 11.2% of calls, while administrative, agency, and non-crime matters comprised 8%. Finally, medical/mental health calls constituted 2.7% of the total call volume. Studies have also shown the dispatcher's discretion may be affected by the nature of the emergency, the demeanor of the dispatchers, the caller's and dispatcher's communication, and the dispatchers' knowledge [@simpson2021calling]. 

## Examples of Applying 911 Calls for Service in Criminal Justice Research
CFS data has been utilized in a variety of ways, from time-series analysis to city-to-city comparisons, due to the fact that it can register real-time incident information, including time and geo-location. This has made it an ideal resource for community policing and evaluation research. For example, studies by @gillooly2022lights and @gillooly2020911 highlight how a call-taker's interpretation of an incident, even for identical details, can significantly influence police officers' initial perception of the scene. This underscores the critical role of call-taker training in community policing, as their descriptions directly impact officer responses and ultimately, community interactions. Another study conducted in Los Angeles by @brantingham2021public showed an increase in CFS calls in the week following a homicide, which was seen as an opportunity for the police to build trust with the public. Additionally, CFS data has been found to have a considerable impact on variations in misdemeanor enforcement [@glazener2020understanding], setting the boundaries of police beat zones [@jones2019data], and understanding why individuals living in disadvantaged, high-crime areas contact the police more frequently despite having a restrictive view of police legitimacy [@st2020social]. CFS data was also utilized to investigate caller behavior, call type patterns across time, and factors influencing response time [@vera2019calls].


The information contained in the data from CFS, such as litters, noise complaints, and so on, is an essential source of "physical disorder," which can be used to evaluate the criminology theories, such as broken windows and rational choice theory. A study looked at the potential of 311 Boston data to develop broken windows theory metrics, such as physical disorder. It illustrated how non-researchers' almost costless measurements from "big data" may be used in econometrics by examining their validity and reliability [@o2015ecometrics]. @wheeler2018effect discovered that 311 CFS are a reliable indicator of physical disorder, hence partially confirming the broken windows theory. @piza2014punishment examined the association between deterrent and CCTV installation in Newark, NJ, using CFS 911 data. They discovered that CCTV enhanced the likelihood of punishment certainty.

Because of its utility in daily time-series analysis, studies have frequently utilized CFS data to evaluate changes in calls before and after the COVID-19 lockdown. According to research in this area, misdemeanors declined significantly following the COVID-19 lockdown compared to the same period in 2019 [@bullinger2021covid]. @wolff2022violence discovered that an increase in COVID-19 cases was negatively associated with both gunshot and assault incidences across New York City's boroughs. Finally, the most significant increase in crime during the "stay-at-home" order has been in domestic violence calls [@nix2021immediate; @boman2020has; @bullinger2021covid].

## National Crime Victimization Survey (NCVS)
The National Crime Victimization Survey (NCVS) is a nationally representative survey of self-reported victimization conducted by the Bureau of Justice Statistics (BJS). It was launched in 1973 as the National Crime Survey (NCS) [@powers2016ncvs; @langton2017second]. NCVS adopts a stratified, multistage, clustered sampling strategy [@powers2016ncvs; @rennison2021national]. It interviews household members aged twelve and over every six months for three years [@powers2016ncvs; @rennison2021national]. The total number of sampled housing units and individuals was 145,000 people and 65,000 housing units in 1973. The personal response rate was 95% and the household response rate was 96% [@rennison2021national]. In 2021, the national representative sample was 238,000 people and 150,000 households. The personal response rate and the household response rate in 2021 were 81.6% and 67.2%, respectively [@ncvs_2021_bjs]. 

The NCVS gathers data on both property and violent victimization, such as sexual assault, robbery, aggravated assault, simple assault, burglary, auto theft, and general larceny. The NCVS also collects data on criminal acts, including unsuccessful attempts, and the circumstances surrounding them. It covers the specifics such as the date and place of the offense, the results of the incident, the features of the perpetrator, the relationship between the victim and the criminal, whether or not the incident was reported to the police, and any reasons why it was not reported [@powers2016ncvs].

The NCVS has undergone several redesigns aimed at enhancing its precision and reliability [@powers2016ncvs; @langton2017second]. The redesigns typically entailed modifying the questionnaire and incorporating a longitudinal component to enable comparisons over a period of time. Before 2006, the survey was mainly conducted by Paper and Pencil Interviewing (PAPI) and Computer-Assisted Telephone Interviewing (CATI). The PAPI method is a data gathering approach wherein an interviewer engages with the respondent either face-to-face or through telephone, posing questions and documenting the answers on a physical questionnaire. The National Crime Survey (NCS) utilized a hybrid methodology involving both face-to-face and telephonic interviews to conduct PAPI. CATI was introduced into NCVS during the mid-1980s [@lynch2006understanding]. This approach was deemed cost-effective and yielded much greater rates of reported victimization as compared to traditional interview methods without CATI  [@lynch2006understanding; @hubble1995national; @rosenthal1993results; @ncvs_2006_michael].

## Examples of Applying NCVS in Criminal Justice Research
NCVS has illuminated the "dark figure" of crime and explored the reasons behind variations in crime reporting, primarily through comparisons with UCR data. For example, @pezzella2019dark used the hate-crime data from NCVS and UCR and discovered that the absence of confidence toward law enforcement can explain the variation in crime reporting behavior. Other studies that utilized NCVS data to compare the long-term trends with UCR support this notion. @lauritsen2016choice found that the type and temporal fluctuations for robbery, burglary, and motor vehicle theft of UCR and NCVS mostly remain comparable, but not for rape, aggravated assault, and a summary measure of severe violence. NCVS trends in serious violence correlate more strongly with homicide data than UCR trends, suggesting that the NCVS is a more accurate indicator of long-term trends in violent crimes.  @baumer2010reporting used both NCVS and UCR data to evaluate the association between crime reporting behavior and police notification. They discovered that the fluctuation of crime rates in UCR could be the result of a number of societal factors (for example, an increase in reporting for sex offenses and family violence), and researchers should be cautious when applying non-lethal violence and property crime data in their research. Another study by @bachman2000comparison utilized NCVS data to compare to data from the Centers for Disease Control and Prevention and the National Institute of Justice-sponsored National Violence Against Women Survey. They concluded that women were more likely to be raped and physically assaulted by someone they knew and had an emotional link with, refuting the common belief that strangers presented the greatest risk of violence to women. Finally, @schwartz2009trends and @steffensmeier2006gender conducted a comparative analysis of NCVS data and UCR data in order to assess the gender of offenders. The researchers discovered that the UCR data indicated a notable rise in the number of female offenders of assault between 1980 and 2003. However, the NCVS data did not yield comparable findings. This discovery indicates that the documented rise in female arrests cannot be attributed to an escalation in female violence, but rather stems from a change in policy that has led to a greater probability of law enforcement apprehending females for criminal assault.

Overall, the NCVS is a valuable source of data on crime victimization in the United States, providing detailed information on both property and violent victimization among a large sample of population. It also serves as a benchmark by which to validate the UCR data. The survey has undergone several redesigns to improve its accuracy and capture more detailed information, and is an important tool for understanding the prevalence of crime in the United States. It too, however, suffers from a number of limitations to be discussed in the section below. 

Table \ref{tab:table0} expands on the diverse applications of traditional crime statistics showcased earlier. These examples provide a broad overview of how criminologists have leveraged these data in their research, as well as how such data, units of analysis, and crime types have been used in past criminological studies.

```{r table0, results='asis', message=FALSE, warning = FALSE, echo = FALSE}
# Read the xlsx file
crime_literature <- read_excel("table_of_studies_using_traditional_crime_statistics.xlsx")

crime_literature_t <- knitr::kable(crime_literature, format = "latex", booktabs = T, row.names = F, linesep = "", caption = "Criminological Research with Traditional Crime Statistics: A Glimpse") %>%
  kable_styling(latex_options = c("hold_position", "striped"), font_size = 7, full_width = F) %>%
  column_spec(1, width = "3cm") %>%
  column_spec(2, width = "1.5cm") %>%
  column_spec(3, width = "3.5cm") %>%
  column_spec(4, width = "5.5cm")

print(crime_literature_t, scalebox = 0.6, type="latex", booktabs = TRUE, longtable = TRUE, linesep = "", table.placement = "!htb")

```


# The Limitations of Traditional Crime Statistics
While readily accessible, traditional crime statistics like UCR, NCVS, NIBRS, and CFS 911 grapple with limitations. UCR, NIBRS and CFS 911 primarily suffer from underreporting and discrepancies in documentation, often due to inconsistencies in state definitions of crime categories. Similarly, the lack of a standardized definition for both UCR/NIBRS and CFS 911 data presents misclassification challenges across states and cities. NCVS, impacted by redesigns, funding fluctuations, and limited sample size, struggles with longitudinal and sub-national analyses. The subsequent paragraphs provide a detailed examination of these constraints.

## The Dark Figure of Crime and Underreporting Issues for UCR/NIBRS/CFS 911
The term "dark figure of crime" refers to crimes that are not reported to police and, hence, do not appear in official statistics. The dark figure of crime has long been an unresolved challenge in determining the "true" volume of crime. Therefore, there is a fundamental problem with drawing crime statistics from the UCR, NIBRS, or CFS 911 data, which reflect incidents that are merely "reported to the police, and recorded by the police." In consequence, the crime statistics acquired from UCR/NIBRS/CFS 911 may be a visible tip of the iceberg of crime [@biderman1967exploring]. Furthermore, it should be noted that police statistics are inherently influenced by the normative perspectives and operational procedures of the institution. As a result, their relevance to social policy is inherently limited and may be distorted [@biderman1967exploring]. Therefore, researchers should exercise caution when obtaining official crime statistics from police for their research, because biased data, no matter how good the approach, might lead to skewed results. Using publicly available data without first understanding the limitations of official crime statistics may lead to incorrect conclusions for researchers and policymakers. 

To understand the emergence of the "dark figure of crime", we must first understand why victims do not always disclose the information to the police. Research has shown that victims are less likely to report to the police when the loss is little, or when they feel ashamed, or they are not satisfied with their previous reporting experiences and the police's investigative efforts, or the police has low accountability in the served community [@black1970production; @xie2006prior]. Additionally, factors such as race, wealth, education, crime seriousness, physical injuries, the offender using a weapon or not, having insurance or not, feeling obligated to report, police efficacy, attitudes toward police, perceived threat from the incident, and past victimization experience have been found to significantly affect victims’ decisions to reach out to the police [@loftin2010use; @bachman1998factors; @avakame1999did; @skogan1984reportingtopoliceworld; @laub1981ecological; @hindelang1974public; @xie2019crime; @skogan1999measuring; @macdonald2001revisiting; @goldberg1980does; @maguire1997crime; @pepper2010measurement; @macdonald2000impact; @tarling2010reporting; @weatherburn2011uses]. Furthermore, certain groups of crime victims, such as illegal immigrants [@gutierrez2017silence], sex workers [@mcbride2020underreporting], and drug traffickers [@topalli2002drug], may be reluctant to call law enforcement to report victimization. 

The willingness of victims to report crimes significantly impacts reporting rates, which fluctuate between offense types. As a consequence, the UCR data reveals a much smaller "dark figure" (unreported crimes) for readily reported offenses, like homicide and vehicle theft compared to underreported ones, like rape. According to the NCVS [@bjs2022ncvs_2019], in 2019, 79.5% motor vehicle victims reported the crime to law enforcement, but only 48.5% of burglary victims, 33.9% of rape victims, and 26.8% of theft victims reported. Studies found that, for certain crimes typically reported to the police, such as motor vehicle theft and homicide, it is better to use UCR, whereas for other crimes, such as rape, a crime that victims are often unwilling to report to the police, NCVS is a better crime estimation than official UCR statistics [@lauritsen2016choice]. Consequently, the politicians and reporters who solely rely on UCR statistics to argue that there has been a decline in rape cases since the 1970s lack empirical evidence [@lonsway2012justice]. Using only crime statistics from police for studies on crime types with significant dark figure may miss a large number of unreported victimization. 

The clarity and accuracy of crime data hinges on not just objective facts but also on subjective perceptions, particularly when defining the offense. Take homicide for instance. Its definition, for both civilians and law enforcement, is relatively clear-cut - a deceased individual is hard to miss and the gravity of the situation prompts immediate reporting. This tangible evidence minimizes the risk of losing reports in the paperwork shuffle. In contrast, defining more ambiguous crimes, like rape and severe assault becomes a murkier affair, impacting data integrity. This argument is supported by studies like @gove1985uniform, who analyzed UCR data and found that definitions for homicide, motor vehicle theft, robbery, and burglary resonate well with both citizens and law enforcement personnel, suggesting consistent application and reliable data collection for these specific offenses. Similarly, @fox2000homicide argued that homicide data is recognized as that "no other crime is measured as accurately and precisely" (p.1). Therefore, an ambiguous or unclear definition of a crime can lead to long-term underreporting within the system. Evidence has shown that UCR may have underestimated rape incidents by more than 40% prior to 2012 by comparing the statistics before and after the 2012 change to the definition of a rape offense^[In 2012, the FBI modified the definition within the UCR system, expanding the scope of sexual assault to encompass all genders. The revised definition now reads as follows: "The penetration, no matter how slight, of the vagina or anus with any body part or object, or oral penetration by a sex organ of another person, without the consent of the victim." This adjustment aims to ensure the inclusivity of any gender as victims or offenders in the national reporting system, moving away from a previous focus solely on instances of women being assaulted by men [@fbi2012rape].] [@biere2015]. 

## Police Recording Problems

\begin{quote}
\emph{If predictive policing systems are informed by such data, they cannot escape the legacies of the unlawful or biased policing practices or biased policing practices that they are built on. Nor do current claims by predictive policing vendors provide sufficient assurances that their systems adequately mitigate or segregate this data.}\end{quote}[@richardson2019dirty]

The other issue of data of UCR/NIBRS/CFS 911 is that, even when the victim has reported the crime, the police crime recording behavior itself is questionable. For example, UCR only records the most serious forms of crimes and excludes lesser offenses. This complicates the calculation of the under-recording when using UCR data. More than that, researchers found that the changing in police practices of recording crimes and the growth of police force will increase/decrease the crime recorded by police [@rosenfeld2007transfer; @o1996police]. Calculation and recording errors from police agencies represent other problems in producing accurate crime estimates. For example, in one instance, a single incident in which four people had been murdered appeared twice in computerized FBI homicide records. It is because the officers from two agencies--sheriff and state police--investigated the crime, and each agency filed a report with the FBI [@maxfield2017research]. 

The "statistical conflict of interest" between police departments and crime statistics is another serious issue. Numerous studies have shown that police are capable of manipulating crime data. As noted, "(Police) use these reports in order to advertise their freedom from crime as compared with other municipalities" [see @united1931report; as cited in @katzenbach1967challenge]. This tendency of "downgrading" or "no crimes by the police"  has yet been fully uncovered. It sometimes arises from organizational or political pressure outside the police department and sometimes from the desire of the police to appear to be doing a good job of keeping the crime rate down [@levinson2002encyclopedia; @eterno2016police]. @seidman1974getting uncovered an intriguing occurrence, which was that a newly appointed police chief had threatened to replace commanders who were unable to reduce the criminal activity in their area - and as a result, there was an abrupt drop in larceny cases. In the case of large cities, not to report crime accurately also penalizes those administrations and police departments that are honest with their citizens by causing them to suffer unjust comparisons with other cities [see @united1931report; as cited in @katzenbach1967challenge]. The research conducted by @thomas2021crime focused on the analysis of the CompStat data within the New York Police Department. Their findings revealed compelling evidence suggesting that the officers had engaged in misrepresentation of serious crime rates through the practice of downcoding. @rapepolicehidekaplan2021ucrbook compared the rape crime rate before and after the definition changed in 2013 and found evidence that police might be manipulating the rape count data. As a result, the purposeful "unrecording" by police corrupted the crime data for use in comparing between areas. 

Furthermore, because UCR, SHR, and NIBRS are voluntary programs, the missing values from the non-reporting of agencies are inevitably significant [@addington2004effect; @loftin2010use; @maltz2002note]. To mitigate the impact of the missing values from the inconsistent reporting, the UCR utilizes imputation as a technique for incorporating data that is missing. For instance, in cases when a particular agency's report solely encompasses homicide numbers for the initial six months, the UCR will employ the mean value of the first six months to estimate the corresponding values for the subsequent six months. In the event that the agency fails to report or provides fewer than three months of data, the FBI will group each reporting agency based on its metropolitan area, known as the Metropolitan Statistical Area (MSA). The FBI would then assign the estimated yearly crime value from the corresponding stratum to the agency with missing data [@lynch2008missing]. Consequently, the reliability and validity of UCR data was significantly undermined due to the use of inconsistent reporting and different imputation techniques by multiple agencies [@delang2022tackling; @loftin2010use; @maltz2002note]. 

@o1996police contended that the aforementioned recording errors exhibit characteristics of being systemic, random, or continuous in the crime data across all law enforcement agencies. Hence, by comparing these statistics across different regions, the measurement inaccuracies can be mitigated, thereby serving as a relative indication of crime. This argument, although plausible, lacks practicality. As previously discussed, the occurrence of recording errors exhibits variation among different agencies. The aforementioned lack of consistency in reporting by various organizations raises concerns about the suitability of utilizing UCR data for cross-regional or longitudinal comparisons [@loftin2010use; @maltz2002note].

## Other Critiques of UCR/NIBRS Data
The official crime statistics from UCR/NIBRS may be viewed as a convenient and comprehensive indicator of crime reporting and police activities, but their utility as an indicator of "true crime" is limited. As previously mentioned, not every crime is reported to the police, and the reasons for underreporting can differ by location, season, or year. In this section, I will discuss the methodological limitations from the data collected for UCR and NIBRS.

Apart from the challenges of underreporting and under recording, there are additional criticisms of crime statistics obtained from UCR, inherent in the data collection process. The hierarchy rules employed by UCR, as discussed by @stogner2016uniform and @levinson2002encyclopedia, contribute to an under-recording problem by restricting documentation to a single crime per event, even in cases of multiple offenses. Surprisingly, the effectiveness of these hierarchy rules has not been thoroughly assessed, and there is a notable absence of literature on the enforcement of the hierarchy rules, the extent of adherence by agencies, and the number of cases potentially missed under this rule. This lack of scrutiny raises concerns about the potential for further inaccuracies in crime statistics due to the unknown impact of the hierarchy rule.

Other than the hierarchy rule, many types of crime that are more prevalent today than ever before, such as fraud, internet scams, hate crimes, domestic violence, or crimes against children or the elderly, are not recorded in UCR [@bibel2015considerations]. The UCR's very limited crime category has also been criticized for only recording "street crimes", while excluding “suite” (or white-collar and business-related) offenses [@levinson2002encyclopedia]. It has been criticized as a "blue-collar police" agency primarily focuses on lowering crime rates by depending on UCR is utilizing a tool to target the lowest socioeconomic class [@nalla1994white]. In addition, it is important to note that the UCR lacks comprehensive victim/offender demographic data, such as age, gender, race, economic status, educational status, and the victimization/offending history of individuals involved, with the exception of homicide cases. Moreover, the UCR does not offer insights into the underlying motivations behind criminal acts or the societal factors that contribute to criminal behavior. These aspects are instead covered by the NCVS and NIBRS [@bibel2015considerations]. 

Crime statistics are typically expressed as crime rates, providing a measure of the relative severity of a specific crime per 100,000 population. This approach proves advantageous when assessing the areas with serious crime problems per 100,000 inhabitants comparing with other areas. Nevertheless, researchers need to exercise caution, especially when dealing with rare crime incidents. In the case of rare crimes like homicide, a single case in small counties or low-population areas can lead to substantial fluctuation in homicide rate. The sensitivity of these rates to small population may distort the overall comprehension of crime patterns [@finch2021assessing; @fox2004missing; @maltz2002note; @pizarro2013assessment; @pridemore2005cautionary; @wadsworth2008missing]. Another challenge associated with county-level data is the tendency of small law enforcement agencies to either partially report or refrain from reporting data to the FBI. The non-reporting or partial reporting by these smaller agencies introduces reporting errors, particularly in smaller counties, thereby impacting the accuracy and completeness of the crime statistics [@delang2022tackling; @loftin2010use; @maltz2002note].

A detailed reporting system like NIBRS is not flawless, either. NIBRS has long been criticized for its lack of generalizability, despite the increasing number of reporting agencies in recent years, following the FBI's discontinuation of the UCR. While less than 40% of law enforcement agencies participated in UCR reporting in NIBRS in 2014, this number rose to 60% in 2021 and 69% in 2022, as reported by @addington2021national and @marshall_2023, respectively. Citing discrepancies between reported agency numbers and represented populations, @asher_2023 raised concerns about potential bias in reporting, suggesting that data discrepancies might reflect a larger participation rate among larger agencies compared to smaller ones. The methodology and scrutinized process will need to be examined as NIBRS now becomes the replacement for UCR.

Another issue to be concerned about is the NIBRS data quality. The FBI conducts data audits every three years, selecting only a small number of incidents (typically 300-500) within a state. With such a small sample size, there may be fears that overall coding errors will go undetected [@bibel2015considerations]. Integrating data from several states to a federal level, as well as managing the definition of so many different detailed offenses, are also a challenge. The ability to compare states using NIBRS data could be jeopardized if there was not a unified look-up table for the coding scheme of crimes [@bibel2015considerations]. 

## Other Critiques of 911 Calls for Service Data
911 calls for service (CFS 911) data shares the same drawbacks as UCR and NIBRS, as they still rely on citizens' willingness to report crimes to law enforcement agencies and to be recorded by the agencies. Additionally, the decentralized system makes it difficult to compare data across regions. Despite the richness of 911 data available from individual jurisdictions, it is difficult to compare the data due to the fact that computer-assisted dispatch (CAD) codes and information vary from one city to another, limiting their applicability [@vera2019calls]. Therefore, it is best to use CFS 911 data to compare each city's own data, or trends from previous years. Besides, the CFS data is subject to the discretion of the dispatcher on whether to record the case, and can be influenced by potentially misleading information from the callers [@klinger1997measurement], phantom calls (automatic dialing, redialing, misdials, hang-ups), pranks, and non-emergency calls [@asu911misuse]. Also, it is not unusual of misclassification of cases. According to a study by @ratcliffe2021policing, even with the assistance of a computer-aided dispatch (CAD) system, around 20% of initial dispatches of CFS 911 cases are reclassified after police respond to the incident. Likewise, research by @simpson2021re analyzed over half a million 911 calls in Chandler, Arizona (2013-2019) and found an 85% accuracy rate in call type classification.

Errors in CFS data on crime counts at the neighborhood level vary systematically with space. Dispatch data is likely to underestimate the total number of crimes reported to the police in locations where citizens perceive officers respond slowly, are scared of crime, and experience more victimization. The fact that overall dispatch data errors are associated with such neighborhood characteristics has significant implications for research on the macro-level determinants of crime. Studies that employ CFS data to evaluate crime may produce misleading conclusions regarding the relationship between crime and other neighborhood factors. When the measurement errors in CFS crime counts are related to other neighborhood factors, the correlation between crime and the properties associated with it are contaminated. This is due to measurement error in the CFS data, which accounts for some of the observed correlation between neighborhood property and CFS-measured crime counts [@klinger1997measurement].

## Critiques of the NCVS
The NCVS was designed to provide a more precise understanding of criminal incidents than the UCR, while also attempting to address the problem of unreported crime in the UCR. However, the NCVS has several substantial drawbacks inherent in its survey methodology. These include its limited funding, which has resulted in a reduced sample size. Studies has shown that smaller states that had a restricted NCVS sample faced a disadvantage in comparison to larger states [@fay2015developmental; @shook2015assessing]. The potential consequences of this could include a decline in the generalizability of the findings, as well as limitations on the feasibility of extrapolating the results to sub-national levels, particularly in the case of rare occurrences, such as crime [@fay2015developmental; @shook2015assessing]. Research has indicated that the NCVS estimation of violent crime rates for states with a population exceeding 1 million may be considered reasonable [@fay2015developmental]. However, this does not hold true for states with a population below 1 million, and it does not extend to other types of crimes for the states with over 1 million population. Furthermore, it is worth noting that this limitation restricted the estimates from NCVS to be applied in smaller geographic units, such as Core Based Statistical Areas (CBSA) or counties. Additionally, errors can occur due to misunderstandings of incidents, the lack of knowledge of legal terms, forgetfulness regarding dates of crimes, and even fabrication [@levine1976potential; @groves2008surveying]. Furthermore, the sensitive nature of the questions asked and privacy concerns associated with self-report surveys can lead to inaccurate estimates of criminal victimization, particularly when such instances are rare among the general population [@groves2008surveying; @berg2016telling]. 

The NCVS's methodological design has significantly undercounted rape and intimate violence, and the published data used by the media and policymakers may mislead the discussion of public policy [@bachman1994measurement; @koss1996measurement]. Each instance of redesign has a significant influence on the whole dataset, necessitating caution when comparing data over an extended period [@bachman1994measurement; @kindermann1997effects]. The estimated violent crime rate was 49% higher and the estimated rape rate was 157% higher than the estimates prior to the modification of the methodology on violent crime estimate in 1992 [@powers2016ncvs; @rennison2021national; @kindermann1997effects]. Similarly, the violent victimization rate in rural areas increased by 62% following the 2006 modification, compared to 2005 data [@ncvs_2006_michael]. Therefore, comparing long-term trends can be challenging due to the major methodological redesigns of the NCVS in 1992 and 2006 [@langton2017second].



# Chapter Summary
This chapter explored the data structures, methodology, and constraints associated with crime statistics obtained from the UCR, NIBRS, CFS 911, and NCVS. We are becoming increasingly aware of how these data should and should not be used in criminological research. Nevertheless, numerous studies have used these data sources to compare crime data across different regions and time periods without adequately addressing the acknowledged limitations of each data source. Table \ref{tab:table201} summarizes the strengths and limitations of each major crime statistics source. While the UCR boasts the longest national trends and widespread coverage, its reliance on citizen reporting introduces underreporting biases. Similarly, NIBRS, though providing detailed incident data, suffers from limited adoption by agencies and similar reporting issues. The CFS 911 offers rich local insights, but its decentralized nature and potential caller bias limit comparability. Finally, the NCVS, unique in its independence from police reporting, tackles underreporting but is constrained by smaller sample sizes and methodological changes. Each source presents distinct advantages and challenges, stressing the crucial need for a nuanced and multi-source approach to inform meaningful crime analyses.

```{r table201, results='asis', message=FALSE, warning = FALSE, echo = FALSE}
# Read the xlsx file
crime_weak_strength <- read_excel("table_of_strengths_and_limitations_of_traditional_crime_statistics.xlsx")

crime_weak_strength_t <- knitr::kable(crime_weak_strength, format = "latex", booktabs = T, row.names = F, linesep = "", caption = "Summary of the Strengths and Limitations of Traditional Crime Statistics") %>%
  kable_styling(latex_options = c("hold_position", "striped"), font_size = 8, full_width = F) %>%
  column_spec(1, width = "2.5cm") %>%
  column_spec(2, width = "1.5cm") %>%
  column_spec(3, width = "1.5cm") %>%
  column_spec(4, width = "4cm") %>%
  column_spec(5, width = "5cm")

print(crime_weak_strength_t, scalebox = 0.6, type="latex", booktabs = TRUE, longtable = TRUE, linesep = "", table.placement = "!t")

```

Despite the distinct likelihood that various crime types are susceptible to differing levels of under-recording, under-reporting, and methodological challenges, certain studies persist in categorizing and amalgamating diverse crimes under broad classifications such as "property crimes" and "violent crimes." This practice compounds errors within the crime data and essentially neglects the critical issue of measurement error in crime statistics. In the following chapter, I will discuss more into the challenges associated with measurement error and explore statistical techniques aimed at mitigating its impact on data.
\FloatBarrier

<!-- Chapter 3 -->
\newpage
\fancyhead[L]{Chapter 3: The Measurement Error of Crime Data}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}

\chapter{THE MEASUREMENT ERROR OF CRIME DATA}
# Measurement and Measurement Error
Measurements, numerical values assigned to aspects of objects or phenomena, quantify attributes like temperature or speed. They facilitate comparisons and are vital in academic research, supporting data collection and analysis. Measurements extend beyond quantitative data to research, serving as tools for evaluating interventions, policy implementation, and constructing frameworks for quantitative analysis. They enable hypothesis development, model construction, and exploration of variable correlations, generating insights and informing decisions based on quantitative facts. In "Measurement Errors and Uncertainties: Theory and Practice," @rabinovich2006measurement defined measurement as: 
\begin{quote}
\emph{The value of physical quantity is the product of a number and a unit adapted of these quantities. It is found as the result of a measurement. The definitions presented above underscore three features of measurement:\\
(1) The result of a measurement must always be a concrete denominated number expressed in sanctioned units of measurements. The purpose of measurement is essentially to represent a property of an object by a number. \\
(2) A measurement is always performed with the help of some measuring instrument; measurement is impossible without measuring instruments. \\
(3) Measurement is always an experimental procedure.\\
The \textbf{true value of a measurand} is the value of the measured physical quantity, which, being know, would ideally reflect, both qualitatively and quantitatively, the corresponding property of the object.}
\end{quote}

Measurement error, also referred to as measurement bias, is a result of the inaccuracy associated with measuring a true value. It is a significant source of uncertainty in data collection and reporting due to the instruments or human behavior in the experiment procedure. The need to carefully consider measurement error arises from its potential to significantly affect the accuracy of research findings, as well as its implications for the validity, reliability, and the overall integrity of a study. Even minor deviations in the measurement can significantly distort the outcomes, resulting in erroneous interpretations and conclusions, and leading to misguided paths for future investigations.  The presence of measurement errors in a given experimental procedure can impede the reliability of replicating the experiment under identical conditions. In particular, it can distort the true meaning of information and create misunderstandings or misinterpretations. The ability to accurately measure outcomes is critical in any research endeavor as it provides the foundation for determining the validity of the results. 

Random error and systematic error are two distinct types of measurement error that can affect the validity and reliability of data [@viswanathan_measurement_2005; @BAGOZZI1998393]. Random error is an unpredictable error that is caused by chance, such as measurement errors due to human error, instrumentation errors, or environmental factors. It is typically random in nature and can be reduced by taking multiple measurements and averaging them. Systematic error, on the other hand, is an error that is caused by a consistent factor, such as a malfunctioning instrument or a bias in the data collection process/procedures. Systematic errors cannot be reduced by taking multiple measurements, therefore, these must be identified and corrected before valid results can be obtained [@hutcheon2010random]. Using a bathroom scale as an example, systematic error would occur if the scale consistently gave a weight that was 10 pounds heavier than it should be. Random error, on the other hand, would manifest as slight variations in the weight readings, which would average out across repeated measurements. As such, if one were to take the average of the readings, the systematic mistake would not be eliminated, but it could be adjusted for by subtracting 10 pounds from the person's usual weight [@rosnow1991if]. 

The measures in social science are often more problematic because we do not necessarily know the true existence of the property, hence, there is no universal benchmark against which we can compare our measurements. This lack of a standard reference point can lead to inaccuracies or inconsistencies in statistical models and results. Therefore, researchers usually take steps to ensure that the measured data and the statistical estimates are as accurate and reliable as possible. This includes using reliable instruments and methods, taking multiple measurements of the same quantity, training data collectors, and double-checking data for accuracy [@althubaiti2016information]. To assess the reliability and validity of a measurement, researchers can use a multitrait-multimethod (MTMM) correlation matrix [@demerouti2003convergent], confirmatory factor analysis, and multiple regression to examine common factors [@BAGOZZI1998393; @cole1987utility]. Even then, it is crucial to interpret the results cautiously, bearing in mind the potential presence of undetected systematic errors. Much like the bathroom scale example, acknowledging and adjusting for these systematic errors can improve the validity of the measurements and ultimately enhance the accuracy of research findings in social sciences.

## The Classic Measurement Error Models

The goal of the discussion of measurement error in Ordinary Least Squares (OLS) regression model is to formulate methodologies that, under appropriate conditions, can alleviate the impact of potential errors in variable measurements, thus enhancing the precision of study findings and analyses. In order to enhance the understanding of the challenges that researchers encounter when the true property become unobservable or difficult to measure, let us examine a fundamental OLS regression model. In this particular model, one obtains an unbiased estimation of a pre-determined variable $y^*$ with respect to a unbiased regressor. However, in practical situations, the true values of $y^*$ or $x^*$, representing their exact property, are typically unobservable or not visible. To represent the unobservable values with measurement errors, I substitute the values denoted as $y$ and $x$. Similarly, these substitutions also retain their inherent errors. For instance, consider using a police reporting record to represent the actual occurrence of a crime or employing victimization surveys to gauge the true fluctuations in crime rates. 

Considering an OLS regression model [@pepper2010measurement; @rabinovich2006measurement], $\beta$ is an unbiased estimate of the true incidence of the regressor $x^*$ on the true incidence of $y^*$. $\alpha$ is the unbiased intercept and $\varepsilon$ is the error term that is independent of $x^*$ (i.e., $E[\varepsilon|x = 0]$). However, in a real world, the true incidences of $y^*$ or $x^*$ are frequently unobservable, and are represented by error-contaminated measured data, $y$ and $x$, with error terms $\nu$ and $\mu$, respectively.
\begin{equation}\label{myeqn1}
y^* = \alpha + \beta x^* + \varepsilon
\end{equation}
\begin{equation}\label{myeqn2}
y = y^* + \nu 
\end{equation}
\begin{equation}\label{myeqn3}
x = x^* + \mu 
\end{equation}

Now, let us engage in a discussion regarding situations in which there is measurement error present in the explanatory variable in the additive model [@pudney2000relationship; @GIBSON2008247]. In the context of measurement error in the explanatory variable, $x$, the parameter $\beta$ exhibits bias. This is because the actual value, represented as $x^{*}$, is expressed as $x = x^{*} + \mu$. For example, if my aim is to measure the true prevalence of firearms in a specific area, the survey method may contain errors, rather than providing the accurate count of firearms present. Likewise, the measurement of the violent crime rate from police records also fails to accurately represent the actual prevalence of violent crime in the area. Nevertheless, within the framework of the classical measurement model, under specific assumptions, it is possible to mitigate the influence of measurement error in the regressor. Additionally, it is feasible to compute either the upper or lower bound/limit of the estimated coefficient $\beta$ under when certain assumptions hold. 

The first assumption posits that the error term in the OLS regression model, $\varepsilon$, should exhibit no correlation with the true value of the regressor, $x^*$.  The second assumption posits that the expected values of both $\mu$ and $\nu$ are assumed to be zero. This implies that if the data points in $\mu$ and $\nu$ are averaged, the resulting mean value for both $\mu$ and $\nu$ would be zero. The third assumption posits that the covariances of $x^*$ and $\nu$, $\mu$ and $\nu$, and $\varepsilon$ and $\nu$ are all expected to be zero. The fourth assumption posits that the covariance of $x^*$ and $\mu$ is expected to be zero. The fifth assumption postulates that the covariance between $\varepsilon$ and $\mu$ is anticipated to be zero. I will discuss further the ramifications if these assumptions are violated, and explore strategies for mitigating their impact in the subsequent discussion. The assumptions can be expressed in a following formats [@pepper2010measurement]:

[A1] $E[\varepsilon | x^*] = 0$

[A2] $E[\mu] = E[\nu] = 0$

[A3] $\sigma_{x^*,\nu} = \sigma_{\mu, \nu} = \sigma_{\varepsilon, \nu} = 0$

[A4] $\sigma_{x^*,\mu} = 0$

[A5] $\sigma_{\varepsilon,\mu} = 0$

Under assumptions A1 to A5, the slope $\hat{\beta}$ does not affected by the measurement error $\nu$ from the outcome variable, and the presence of the measurement error $\mu$ of the regressor $x$ caused "attenuation bias^[Attenuation bias, alternatively referred to as regression dilution or measurement error bias, is a statistical phenomenon that arises when there exists error or imprecision in the measurement of independent variables within a regression analysis. The presence of measurement error has the potential to introduce bias and underestimate the associations between variables. Attenuation bias, in essence, tends to result in estimated coefficients in a regression analysis that are relatively lower (approaching zero) compared to their actual values [@carroll1995measurement].]" of $\beta$, which reduces the strength of the relationship between $x^*$ and $y^*$. In the earlier example involving firearm prevalence as the explanatory variable and violent crime rate as the outcome variable, the true relationship ($\beta$) between firearm prevalence ($x^*$) and violent crime rate ($y^*$) can only be underestimated due to the errors in firearm prevalence ($\mu$). 

We can use the estimate of the probability limit^[The notion of the probability limit relates to the properties demonstrated by these estimators as the size of the sample increases indefinitely. Under certain conditions, the ordinary least squares (OLS) estimators are regarded as "consistent." This implies that as the sample size expands to infinitely , the estimated beta coefficients have a high likelihood of approaching the true population parameters. Moreover, a crucial attribute to take into account is the concept of "asymptotic normality." This suggests that under certain assumptions, as the size of the sample increases, the distribution of the estimators obtained through OLS approaches a normal distribution that exhibits a bell-shaped curve [@judge1991theory].] to explain the "attenuation bias." The probability limit of OLS slope $\hat{\beta}$ is proportional to the true value $\beta$ in the following way [@pepper2010measurement; @wooldridge2010econometric; @millimet2022accounting]:
\begin{equation}
plim_{N\rightarrow\infty}\hat{\beta}_{y,x} = \beta\frac{\sigma^2_{x^*}}{\sigma^2_{x^*}+\sigma^2_{\mu}} \label{myeqn5}
\end{equation}

\noindent In other words, as the variance of $\mu$ increases, the probability limit of $\beta$ decreases. The degree of attenuation bias is directly related to the magnitude of the variance of $\mu$, the error in the regressor $x$. In the example involving firearm prevalence and violent crime, the greater the variance of error $\mu$ present in $x$, the more probable it is that the estimated relationship between them, denoted as $\hat{\beta}$, will be reduced.

Now, I have established that measurement error in the regressor, $x$, will lead to "attenuation bias," resulting in an underestimation of the true value of $\beta$, provided that the assumptions are met. However, what happens when there is an error in the outcome variable? In the context of crime statistics, the crime rate is typically regarded as the outcome variable in the OLS model. Let's now embark on a discussion concerning the situation in which there is a measurement error present in the outcome variable in the classic measurement error model. Considering the linear regression model which the measurement error only exists in the outcome variable by inserting \eqref{myeqn2} in \eqref{myeqn1}, we will have $y^* + \nu = \alpha + \beta x^* + \varepsilon$, which can be rearranged as:
\begin{equation}
y^* = (\alpha - \nu)  + \beta x^* + \varepsilon \label{myeqn4}
\end{equation}

\noindent In \eqref{myeqn4}, the estimate of $\beta$ is unbiased with the error from the outcome variable $y = y^* + \nu$, because the the  error $\nu$ affects just the constant $\alpha - \nu$. That is to say, if the error is introduced into the violent crime rate ($y$) in an additive manner ($y = y^* + \nu$), it will not impact the true estimate ($\beta$) of the violent crime rate ($y^*$) and firearm prevalence ($x^*$), affecting only the intercept ($\alpha - \nu$). 

## Endogeneity and Instrumental Variable in OLS
In the OLS model, assumption A1 posits that there is no correlation between the independent variable $x$ and the error term $\varepsilon$. However, assumption A1 may not always hold true, which leads to the problem of endogeneity. Endogeneity pertains to the state where there is correlation between an independent variable and the error term. In contrast, an exogenous variable is defined as an independent variable that exhibits no correlation with the error term. Endogeneity represents a substantial violation of the fundamental assumptions underlying the OLS regression model. The existence of endogeneity within the regressors gives rise to a range of challenges, including selection bias, simultaneity, and measurement error in the independent variable [@bushway2010instrumental]. Various design or statistical methods, such as experimental research designs, instrumental variable (IV) methods, or fixed-effects models, are utilized in order to address the issue of correlation between the error term and the regressor of interest. 

The instrumental variable (IV) is introduced into a regression model to serve as a substitute for a potentially endogenous independent variable^[While there can be multiple instrumental variables in one regression model, this requires more stringent conditions to be met [@mogstad2021causal; @angrist1995identification]. Additionally, the discussion of multiple instrumental variables typically pertains to binary treatment conditions (always-taker, never-taker, complier, defier) for the identification of local average treatment effects (LATEs). In this dissertation, the IV (the car price, which we will discuss later) is not binary, which adds complexity. Therefore, we consider only one instrumental variable ($z$) and one independent variable ($x$) in our IV regression models [@angrist1995two].]. It helps in estimating the causal effect of the endogenous variable on the dependent variable [@judge1991theory]. The IV method has gained significant usage across various disciplines, particularly in econometrics and statistics for causal inference in observational studies and randomized control trials [@angrist1996identification; @stock2002survey; @heckman2005structural; @murnane2010methods; @engle1983exogeneity]. In order for an instrumental variable to be considered valid, it must satisfy two essential criteria: first, the instrumental variable $z$ exhibits correlation with the endogenous variable $x$. Furthermore, it is necessary that the instrumental variable $z$ exhibits no direct association with the dependent variable $y$. That is, the effect of $z$ on $y$ is transmitted through $x$. When the independent variable $x$ is subject to endogeneity, it might be influenced by factors not included in the model or might be correlated with the error term. Thus, $z$ is used as a proxy to explain the variations in $x$ [@angrist2001instrumental; @shea1997instrument; @angrist1996identification; @angrist1996children]. For instance, consider the sales of gun safes ($z$), which can function as an instrumental variable for gauging the prevalence of firearms ($x$). Importantly, the sales of gun safes may serve as an indication of firearm prevalence but they are not expected to directly impact the rate of violent crime ($y$). This framework facilitates a more precise examination of the correlation between the prevalence of firearms and violent crime.

The instrumental variable (IV) method is broadly utilized across diverse sectors within criminal justice research, extending from policing, court, correction, to crime statistical inquiries. Rigorous research has elucidated the positive connection between the recruitment of more police officers and the subsequent decrease in crime rates. An instance of IV application is found in correlating the number of firefighters to the number of police officers, without associating it to the crime rate [@levitt2002using]. Other studies discussed the relationship between the number of police and crime rates used other instrumental variables such as: the size of Community Oriented Policing Services (COPS) grants [@evans2007cops]; federal law enforcement grants [@worrall2010police], and the pre-post status of receiving funding for police hiring [@lin2009more].

The IV method has also been applied in various studies to examine the correlation between crime rates and the size of the prison population. Notably, @levitt1996effect employed this method by using prison overcrowding litigation in a state as an instrumental variable to probe the causal impact of the size of the prison population on crime rates. In a similar analysis, @listokin2003does used the abortion rate of the 1970s as an instrumental variable to determine the effect of crime on the prison population. Another application of the IV method is in analyzing the link between the length of sentencing and unemployment. In this context, @kling2006incarceration used the random assignment of judges as an instrumental variable to explore the effect of sentencing length on employment outcomes. Additionally, the random assignment of judges has similarly been used as an instrumental variable to investigate the correlation between pre-trial detention and conviction rates [@dobbie2018effects]. A study by @leigh2004instrumental leveraged regional cigarette price variations to explore the smoking-health connection through an IV approach. Finally, the IV method has been deployed in examining the correlation between violent crime rates and housing prices, utilizing homicide as the instrumental variable. As homicide rates are widely regarded as the most accurate crime statistic [@ennis1967criminal;@fox2000homicide], the authors contended that homicide rates can effectively address potential measurement error in violent crime rates [@tita2006crime]. 

Note that the concept of an instrumental variable is different from a control variable in a regression model. First, there are certain criteria for selecting an instrumental variable, such as it only affecting the dependent variable 
$y$ through $x$. Furthermore, to estimate the slope $\beta$ of $x$ on $y$, an instrumental variable approach uses two-stage least squares (2SLS). This method involves first regressing $x$ on the instrumental variable to obtain predicted values of 
$x$, and then using these predicted values in the second regression model to estimate the slope $\beta$. A control variable, on the other hand, is included in the same regression model to account for potential confounding factors that might affect the relationship between the independent variable $x$ and the dependent variable 
$y$, helping to isolate the effect of $x$ on $y$.

The IV method is characterized by a framework that comprises two equations. Firstly, an estimation is performed in order to determine the relationship between the instrumental variable $z$ and the endogenous variable $x$. Following this, the projected values of variable $x$ derived from this equation, $\hat{x}$, are then substituted for the actual observed values of $x$ in the main regression equation. The relationship between the independent variable $x$ and the instrumental variable $z$ can be written in the following formats:
\begin{equation}
\hat{x} = \iota + \gamma z + \delta \label{myeqn6}
\end{equation}

\noindent To estimate the exogenous effect of $x$ on $y$, we can plug in the values of $\hat{x}$ into \eqref{myeqn1}. It is written in the following format:
\begin{equation}
y = \alpha  + \beta \hat{x} + \varepsilon \label{myeqn7}
\end{equation}

While the IV method does facilitate the assessment of the causal relationship between variable $x$ and the dependent variable $y$, its effectiveness in addressing endogeneity concerns heavily depends on the meticulous selection of a suitable instrument. This chosen instrument should demonstrate a correlation with the endogenous variable while maintaining its independence from the outcome of interest. It's crucial to exercise caution when employing the IV method, given that the choice of instrument and the underlying assumptions can be challenging to meet unequivocally [@deaton2010instruments; @bound1995problems]. These four assumptions must be met when choosing the IV. First, the relationship between the IV ($z$) and $x$ and the relationship between $z$ and $y$ don't share unmeasured common causes. Second, there is no direct or indirect effect of $z$ on $y$ not through $x$. Third, $z$ must have an effect on $x$. Finally, random events, independent of the experimenter, determine the impact of $z$ on $x$. When direct randomization or natural events are unavailable, the only recourse for IV selection often lies in rigorous theoretical reasoning. This requires meticulous attention to potential biases [@martens2006instrumental]. Determining whether the impact of $z$ on $y$ occurs solely through $x$ is challenging to establish statistically. This claim relies heavily on the researcher's understanding of the underlying causal mechanisms involved. Strong theoretical foundation and justification are crucial for supporting such an assumption [@bushway2010instrumental].

In the discussion of measurement error, the IV method can also be served as a solution to the measurement error in $x$ for the estimation of $\beta$ in an OLS regression model when several conditions are met [@blalock1969multiple; @carroll1994measurement]. These conditions necessitate that the instrumental variable $z$ exhibits covariance with the endogenous variable $x$, indicating that the covariance of $z$ and $x$ is not equal to zero. Furthermore, it is essential that the covariance of $z$ and the error terms $\mu$ and $\nu$ is zero, ensuring that $z$ is uncorrelated with any measurement errors associated with variables $x$ and $y$. Additionally, the covariance of $z$ and the error term $\varepsilon$ must also be zero. These conditions can be expressed as follows:

[A6] $\sigma_{z, x^*} \neq 0$ 

[A7] $\sigma_{z, \mu} = \sigma_{z, \nu} = 0$

[A8] $\sigma_{z, \varepsilon} = 0$ 

\noindent In this case, the IV estimator is consistent for the $\beta$ in \eqref{myeqn7}, and the probability limit of $\beta$ is close to the real estimate of $\beta$:
\begin{equation}
plim_{N\rightarrow\infty}\hat{\beta}^{IV}_{y,x(z)} = \beta \label{myeqn8}
\end{equation}


In crime data, however, relying on the classical assumptions is not always feasible. Because crime data is notorious for underreporting, any reporting errors will always result in an underestimation of the real quantity of crime counts, and the lower the reporting rate, the greater the number of reporting errors in the data. Furthermore, the factors influencing underreporting may be correlated with the other crime covariates, $x^*$. As a result, $E[\mu] \neq 0$, $E[\nu] \neq 0$, $\sigma_{x^*,\nu} \neq 0$ and $\sigma_{y^*,\nu} \neq 0$, and the classic assumptions A2 and A3 is violated. 

Other than the IV method, @pina2022impact and @pina2023exploring argued that applying a multiplicative model ($y = y^*U$), rather than an additive one ($y = y^* + \nu$) could effectively handle the measurement error. This is particularly true in the context of crime statistics because reporting errors in crime statistics are often represented in a multiplicative form rather than an additive one. Consider, for example, the statistics of motor vehicle theft reported to the police. If only 70% of such thefts are reported, the recorded volume represents only 70% of the actual incidents of motor vehicle theft. This could be mathematically represented as $y = 0.7y^*$, where $y$ is the reported crime volume and $y*$ represents the actual crime volume. When the reporting rate reaches 100%, we can anticipate an unbiased estimate of motor vehicle theft, and the measurement accurately reflects the true extent of motor vehicle theft, simplifying to $y = y^*1$. Furthermore, the multiplicative model can also better cope with the right-skewed problem that exists in the majority of crime data by taking the log-transformed formation ($log(y) = log(y^*) + log(U)$). In the subsequent section named "Reporting Errors in Crime Data and The Multiplicative Error Model," I will conduct a more comprehensive exploration of this topic and investigate the effects of the multiplicative error model on crime statistics. Before that, in the next paragraph, my focus will specifically be on the measurement error often observed in crime statistics.

<!--

In addition to the IV approach, panel data structure is employed in measurement error models. Panel data models are a form of longitudinal data analysis where multiple units are observed over time. These models are helpful in studying the impact of variables that change over time. Panel data structure allows for the control of observed and unobserved individual characteristics that could possibly be correlated with the observed variables and lead to biased estimates. 

Under the classical assumptions A1-A5, the panel structure of data can eliminate the undesired time-constant terms with $\beta$ estimate. With an accurate measurement of $x_i^*$ available, OLS performed on $\Delta{y_i^*}$ and $\Delta{x_i^*}$ would yield a consistent estimate of $\beta$ [@pepper2010measurement]:
\begin{equation}
\Delta{y_i^*} = \Delta{x_i^*\beta}  + \Delta{\varepsilon_i}  \label{myeqn9}
\end{equation}
-->

# Measurement Errors in Crime Statistics

## The "Proxy" of Crime Data
Based on the provided definition of "measurement" in the introductory paragraph of this chapter, it becomes evident that the crime statistics frequently utilized in social science research, namely UCR, NIBRS, or CFS, do not conform to the precise scientific criteria for being considered "measurements." The original intent behind UCR, NIBRS, and CFS was not to serve as components of an experimental methodology. Rather, these statistics primarily consist of recorded data pertaining to crimes, incidents, and arrests reported to law enforcement agencies. However, due to the absence of alternative measurement methods for crime statistics, the officially collected crime data has been utilized as a "proxy" for true incidence of crime. For research purposes, although they may not be considered scientific measures, it is still possible to employ measurement error methods to mitigate the impact of errors present in these crime statistics.

Crime data, whether sourced from self-reported surveys, such as the NCVS, or from official law enforcement records, are susceptible to systematic recording and reporting errors [@pepper2010measurement], as extensively discussed in numerous criminological studies [@loftin2010use; @biderman1967exploring; @lauritsen2016choice; @black1970production; @skogan1984reportingtopoliceworld; @skogan1999measuring; @macdonald2001revisiting; @pepper2010measurement; @lohr2019measuring; @comer2023reported]. This error, or deviation in accuracy of measurement, is a crucial element that can significantly distort the findings in these criminological studies, leading to both under- or over-estimation of the true extent of crime [@myers1980crimes; @kovandzic2006effect; @SKOGAN197517; @sherman1979measuringhomicide; @light_comparing_2020; @nolan2011estimating; @pina2023exploring]. 

In relation to reporting systems, such as UCR/NIBRS/CFS/NCVS, diverse sources can contribute to measurement error - from factors causing underreporting [@xie2006prior; @avakame1999did; @skogan1984reportingtopoliceworld; @laub1981ecological; @hindelang1974public; @xie2019crime; @skogan1999measuring; @macdonald2001revisiting; @goldberg1980does; @maguire1997crime; @rosenfeld2007transfer; @o1996police; @pepper2010measurement] to misclassifications [@sparrow2015measuring; @sherman2011police; @nolan2011estimating], changes in police force sizes [@levitt1998relationship; @skogan1999measuring], human error in data collection and reporting, and inconsistencies in data collection practices across different law enforcement agencies [@loftin2010use]. Due to the significant limitations of these data sources, their measurement error is also highly problematic [@seidman1974getting; @rapepolicehidekaplan2021ucrbook].

While official crime data like UCR, NIBRS, and CFS 911 offer valuable insights, they are not direct measures of the elusive reality of crime [@pepper2010measurement]. Treating them as perfect proxies can obscure their limitations as non-scientific measurements [@maltz2003measurement]. In statistical analysis, overlooking this can lead to biased results. The impact of these errors depends on how crime data is used. Using crime data as an independent variable, systematic and random errors might just cause attenuation bias (reduced effect size). Using crime data as a dependent variable, they might only affect the constant term without skewing the coefficient of interest, $\beta$, assuming OLS assumptions A1-A5 hold. However, a frequent issue is correspondence error, where factors influencing the error term, reporting errors are correlated with actual crime rates, violating assumption A3 (no correlation between error term and independent variable, $\sigma_{x^*, \nu} = 0$). For example, declining trust in law enforcement ($x$) could decrease reporting behavior ($\nu$) while crime rates ($y$) rise. Similarly, socioeconomic status ($x$) often included as a covariate on crime rates ($y$), can affect both crime rates and reporting decisions ($\nu$). Consequently, criminological studies relying solely on official crime data may be built on shaky ground due to unaddressed measurement errors.

## Variation in Measurement Error Across Different Locations, Crime Types and Time-Period

Differences in measurement error across diverse locations, types of crime, and time periods lead to the breach of assumption A2, which asserts that $E[\mu] = E[\nu] = 0$. First, the variation in measurement error across diverse locations comes from the variations in reporting behavior among agencies. These variations include sporadic reporting, partial reporting, and low reporting rates, which can be attributed to the voluntary nature of the reporting system. @skogan1974validity pointed out that when we consider underreporting to be a consistent bias (measurement error) across places or time periods, official crime statistics can be useful to be compared as relative crime rates between areas. However, the reporting quality of UCR/NIBRS varies from year to year and region to region, and the effect of underreporting may be more severe in certain areas than others. According to @rapepolicehidekaplan2021ucrbook, zero-count statistics in crime data often reflect reporting issues rather than true crime absence. This pattern is observed in various contexts, such as gun violence in New York City, rape statistics in Danville, California, and annual homicides in Miami, Florida. Similarly, using a crime category with low reporting, such as rape, for long-term crime trends and comparing fluctuations can be problematic. It is also problematic when a nationwide comparison utilizes estimates that suffer from low response rates from agencies, such as NIBRS [@addington2008assessing]. We might also be more cautious after seeing official data indicating a sudden increase in the number of rapes in one jurisdiction. This could be due to an increase in sexual assaults, a greater propensity for victims to report crimes, or an increase in the number of police officers/staff involved in crime recording [@skogan1984reportingtopoliceworld; @fox2006encyclopedia]. As a result, national comparisons of official crime data are hampered by variations in estimation bias across regions and over time [@stogner2016uniform].

Comparing crime rates across different geographic areas poses a considerable challenge. Similarly, comparing different types of crime statistics within one single geographic area also presents complexity in data. Domestic violence, for example, surged significantly during the COVID-19 lockdown, whereas other types of crime declined [@ashby2020initial]. @ruback2018hate highlights regional disparities in UCR data, showing a general underestimation of hate crime rates by approximately 1.6 times, and by 2.5 times in rural areas. As a result, several researchers proposed that the variation in crime rates/counts cannot be compared from one site to the next; and the variation in crime rates/counts in one category within the same location cannot be compared to other categories of crime rates/counts [@kaplan2019more; @ruback2018hate; @GIBSON2008247]. 

A more pragmatic approach to incorporating crime statistics into research, with the aim of mitigating measurement error and bias, would involve the inclusion of crime data from diverse and various sources. Furthermore, it is crucial to investigate one type of crime within a specific geographic area at a time. By limiting the scope of comparison to a single crime category, it becomes possible to examine the temporal variations of that particular crime within a given location. Given the substantial extent of measurement error in crime statistics, it is suggested to employ the aforementioned techniques to minimize the impact of measurement error. These techniques include the use of instrumental variable methods, multiplicative model, and the application of panel structure data [@pina2023exploring]. 

## Inappropriately Applying Aggregated Crime Data

\begin{quote}
\emph{All crimes are not created equal. Counting them as if they are fosters distortion of risk assessments, resource allocation, and accountability.}\end{quote}[@sherman2016cambridge]

Though both official and survey-based crime data are inherently imperfect due to measurement errors and varying reporting rates, aggregating them into broader categories like "violent crime rates" or "property crime rates" further amplify these limitations and raise concerns for their use in evidence-based research. These aggregates, like "violent crime" encompassing homicide, rape, robbery, and assault, or "property crime" that include motor vehicle theft, larceny, and burglary, mix diverse reporting biases and make them impractical for drawing precise conclusions. While specific crime types are often the focus of individual studies, aggregated data still finds use in criminology due to data limitations or the need for larger samples (see Table \ref{tab:table0}). However, the drawbacks are substantial, and a cautious interpretation with a nuanced understanding of these limitations is crucial.

Beyond muddied reporting, the dominance of minor offenses in aggregated crime counts further undermines their accuracy. For example, in 2019, there were a total of 1,203,808 violent offenses reported in UCR [@investigation2020crime]. Among these incidents, 821,182 cases were classified as assaults, accounting for approximately 68% of all violent crimes. Analyzing this data prompts us to question the extent to which an overall increase/decrease in violent crime percentages truly captures the complexity of the situation. It becomes evident that such an increase/decrease may primarily reflect changes in the incidence of assault, given its substantial contribution to the overall variation in violent offenses. As a result, combining all crime counts into a single aggregated statistic such as "violent crime", "property crime", even "overall crime rate", has long been believed to be misleading [@sherman2016cambridge]. 

<!--
Moreover, a significant issue arises when crime statistics are inappropriately paired together, such as when "violent crime rates" are mistakenly used as a proxy measure for unrelated factors like "firearm usage" [@mcgarrell2010project]. As a result, the already problematic aggregated crime rates becomes another proxy for another property, thereby introducing additional layers of intricacy to the understanding of the data, presenting challenges and the difficulty in analyzing and understanding crime statistics. This appears as though we are amalgamating the reporting errors from various forms of 'proxies' for criminal activities to create an aggregated crime estimate. Subsequently, we utilize this intricate estimate as a different 'proxy' for another property. Without addressing the constraints of measurement error, the research results might be incomplete or potentially skewed.-->

## Reporting Errors in Crime Data and The Multiplicative Error Model

In this section, I will examine a measurement error model that is specifically applicable in the context of crime statistics, aiming to address the substantial issue of underreporting data. Denote a reporting indicator $U$, where $U\in(1, 0)$. I establish the reporting indicator $U = 1$ if the individuals report the victimization truthfully, and $U = 0$ if they do not [@breunig2020nonclassical; @d2010new; @breunig2021nonparametric; @GIBSON2008247; @pudney2000relationship]. It is typically written as follows:  
\begin{equation}
y = y^*U \label{myeqn9}
\end{equation}

<!--Because measurement errors in reporting are a prevalent issue in crime statistics, it is useful to differentiate reporting error from systematic error in crime measurement. Note that the term "reporting error" refers to the situation in which the victim fails to report the incident to the police/survey. I only investigate the reporting behavior of "true" victims and ignore the false positive case of non-victims reporting, as this is minor in comparison to non-reporting errors. Furthermore, police recording behavior, instrument and imputation errors, and recording errors are classified as systematic errors and are outside the scope of this discussion. -->

\noindent Same in \eqref{myeqn1} and \eqref{myeqn2}, $y^*$ is the true value of a category of crime generally unobservable, and $y$ is the volume of truthfully crime reported. When $U = 1$, $y^*$ is observable, and when $U = 0$, $y^*$ is unobservable. Denote $\hat{U} = E[U|y^{*}]$, is the mean probability of reporting rates. Noted that from \eqref{myeqn9}, U is the reporting rate of the true volume of $y^*$. When $\hat{U} = 1$, the expected value of $y$, $\hat{y}=y^*$, indicating that $\hat{y}$ is a 100% reported measurement of the true value of crime $y^*$ [@pepper2010measurement]. 

In a general form of outcome variable from \eqref{myeqn2}, $\nu$ is the systematic error including the reporting error. With the multiplicative error model, the systematic error is ignored. Thus, by utilizing a log-based generalized linear model, \eqref{myeqn9} itself could be used as an outcome variable, and can normalize right-skewed crime rates [@millimet2022accounting; @sutherland2013collective; @whitworth2012inequality], or utilize Poisson or negative binomial models [@osborn1998distribution; @sampson1997neighborhoods; @pina2022impact]. The trade-off is that the multiplicative error model assumes that the systematic error other than the reporting rate should be consistent across time and space, which differs from the addictive model of \eqref{myeqn2}. In a log-based multiplicative measurement error model, the multiplicative errors affecting crime rates will be transformed into additive errors [@pina2022impact]. Many scholars have recognized that using the multiplicative model can reduce the impact of measurement error in crime statistics [@pina2022impact; @pepper2010measurement; @millimet2022accounting; @pudney2000relationship]. The log-transferred multiplicative error model which finally transforms into an additive model can be written as follows: 
\begin{equation}
log(y) = log(y{^*}U) = log(y{^*}) + log(U) \label{myeqn10}
\end{equation}

It is worth to noting that in the multiplicative model, the extent of the bias will be proportionate to $U$ [@GIBSON2008247; @pina2022impact], which can be understood by rearranging $y{^*}U = \alpha + \beta x^* + \varepsilon$ into this:
\begin{equation}
y{^*} = \frac{\alpha + \beta x^* + \varepsilon}{U} \label{myeqn11}
\end{equation}


\noindent In the log-transformed multiplicative measurement model, \eqref{myeqn10} can be inserted into \eqref{myeqn1}, which is $log(y) = \alpha + \beta x^* + \varepsilon$ [@millimet2022accounting]. In this way, although the outcome variable is suffered by the reporting error $U$, it will reduce the impact of the error $U$ on the estimate of the coefficient $\beta$, and it can be further rearranged as this [@pina2022impact; @millimet2022accounting]:
\begin{equation}
log(y{^*}) = (\alpha - log(U)) + \beta x^* + \varepsilon \label{myeqn12}
\end{equation}

## Methods for Enhancing the Accuracy of Crime Statistics in the Face of Measurement Errors

Ensuring the reliability and accuracy of crime statistics is a paramount concern in the field of criminal justice research. To address the challenges posed by measurement errors and enhance the trustworthiness of crime data, several methodological approaches and strategies can be employed. The first is the use of various sources of estimates for the same type of crime since "no single estimate is sufficient to provide the full picture about reliability, and that different types of estimates should be used together" [@bruton2000reliability; @sherman2016cambridge]. Employing different types of estimates in conjunction allow for a more robust and nuanced understanding of crime patterns. 

Deciphering the intricate influences on crime statistics requires acknowledging their dynamic nature and moving beyond static data. Panel data analysis emerges as a powerful tool, dissecting the intertwined web of time and specific factors. By embracing temporal fluctuations and wielding this robust technique, researchers unlock more accurate and nuanced relationships, surpassing the limitations of single-point snapshots. This toolbox encompasses diverse methodologies like natural experiments [@loeffler2013does; @dunning2012natural; @craig2017natural], interrupted time series analysis [@steinbach2015effect; @lu2021cannabis], and fixed-effect models [@fergusson2000alcohol; @bushway1999assessing; @sharkey2012effect], empowering researchers to delve deeper and uncover the true contours of crime's ever-shifting landscape.

Third, the instrumental variable method can be used to mitigate the impact of measurement error on the explanatory variables. As previously discussed, IV method plays a pivotal role in overcoming the obstacles presented by measurement errors, ultimately reinforcing the trustworthiness of the analysis. Through its application, researchers can rectify measurement errors within the explanatory variables, thereby elevating the reliability and validity of their research outcomes [@hu2008instrumental; @carroll2006measurement]. 

Fourthly, the employment of multiplicative models represents an effective approach in tackling reporting errors present in crime statistics, especially when focusing on outcome variables. As previously discussed, when adopting a log-transformed model, multiplicative models not only possess the capability to address reporting inaccuracies but also offer a solution to address issues related to right-skewed distributions [@pina2023exploring; @pina2022impact].

Fifth, when evaluating the validity or reliability of an innovative crime measurement approach, it is advisable to employ a more reliable category of crime statistics as a "proxy." Examining specific crime types, such as homicide [@o1996police; @lauritsen2016choice] or motor vehicle theft [@GIBSON2008247; @ansari2015convergence], can be particularly beneficial, especially when assessing the new measurements for violent crime and property crime separately.

Finally, considering the rarity of crimes and victimizations compared to other types of data, it is crucial to utilize data collected from large samples or, at the very least, aggregate data from smaller samples into larger geographic areas. While there is no specific threshold for the sample size in victimization surveys, @skogan1981issues argued that, for a 10,000 household survey, it may be adequate for estimating common property crimes but not for victims of personal crimes (page. 2). @rapepolicehidekaplan2021ucrbook contended that when extracting data from official crime statistics such as UCR or NIBRS, research based on crime statistics from areas with populations exceeding 100,000 people is more likely to yield reliable results than data from towns with populations less than 10,000 people. However, noted that bigger isn't always better, as @rand2005bigger highlights with a comparison of NCVS (80,000 samples) and national violence against women survey (NVAWS, 16,000 samples). Methodological differences, like age range and reference period, can significantly impact results. 

<!--
Finally, the use of a weighted crime index can help to portray the severity of crimes. A weighted crime index is a method that provides a nuanced view of crime severity, based on factors like societal impact and victim harm. This approach allows researchers to consider the varying degrees of harm and societal consequences of different crimes, enabling informed decision-making and resource allocation. This tool is valuable for academic research and practical applications in criminal justice [@sweeten2012scaling; @sellin1964measurement; @sherman2016cambridge]. -->


# Chapter Summary

This chapter delves into the critical issue of measurement error, dissecting its core concepts and its impact on crime statistics. The discussion begins by exploring the classic measurement error models and their components, such as systematic and random errors, providing tools to navigate and cope with their presence. The specific challenges posed by measurement error in crime statistics is also discussed, highlighting the issue of how significant reporting errors can inflate errors in linear regression models. To address this, the adoption of a multiplicative model within the OLS framework is a better approach. Finally, the chapter concludes by presenting a comprehensive overview of various strategies that can be employed to mitigate the influence of measurement error on crime data, empowering researchers to draw more accurate and meaningful conclusions.

In the following chapter, I will discuss how big data can be utilized in social science research. Further, I will discuss the usage of user-generated data collected from the internet, specifically through Google Trends. This contemporary method offers a novel perspective on how we can gather and interpret crime-related information in the digital age. Drawing on existing research and case studies, I will illustrate how Google Trends data can be incorporated into social science research, offering a dynamic and responsive tool for researchers, policymakers, and law enforcement agencies.

\FloatBarrier

<!-- Chapter 4 -->
\newpage
\fancyhead[L]{Chapter 4: Google Trends and Its Applications}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}

\chapter{GOOGLE TRENDS AND ITS APPLICATIONS}

# Big Data and Social Science Studies

Big data is characterized by various aspects, as highlighted by @kitchin2013big : It encompasses colossal datasets, elaborates high velocities involving near real-time generation and processing, and demonstrates variety through structured, semi-structured, and unstructured data comprising text, images, and sensor data [@salganik2019bit; @ginsberg2009detecting]. Moreover, its exhaustive scope aims to capture entire populations or systems, making it invaluable in fields like epidemiology. Big data interconnects data elements, facilitating the analysis of social network, graph analysis, and the understanding of complex systems. It is also flexible and adaptable across many fields, from scientific research to business analytics, due to its flexibility. As a result, big data enables the analysis of a vast body of related works, drawing insights from hundreds of thousands of media resources [@manovich2011trending]. 

The widespread presence of the internet enables the utilization of big data in the field of social science. Research has indicated that information obtained from the internet and content generated by users is easier to access and more cost-effective in comparison to traditional social surveys [@salganik2019bit]. For example, a study integrated in-depth call records from approximately 1.5 million individuals with survey responses from around 1,000 participants to gauge the regional wealth distribution in Rwanda [@blumenstock2015predicting]. Remarkably, their estimates closely aligned with those derived from the Demographic and Health Survey, which is considered the benchmark in developing nations. However, the method they employed was approximately 10 times quicker and substantially more cost-effective [@salganik2019bit].

In recent years, the exponential growth of the internet usage has introduced an unprecedented opportunity for social science researchers to harness a vast source of big data. Examples of big data include what are commonly referred to as digital trace data or user-generated content. Digital trace data keeps track of our online actions and engagements. @howison2011validity identified three basic characteristics of digital trace data, which include: 1) found data: the digital trace data emerges as a by-product of activities, as opposed to being meticulously designed data collection processes for research purposes; 2) event-based data: digital trace data necessitates a departure from the traditional survey approach, wherein the researcher, instead of merely deriving measures from answers to questions, is required to actively establish the connections between records of events and the variables of interest; and 3) as the event occurs over a period of time, it generates longitudinal data.    

Similarly, @salganik2019bit noted that there are ten characteristics of "big data." They are 1) Big: Big data usually contains an immense amount of data points, millions or billions of words. 2) Always-on: big data is constantly collected, providing researchers with longitudinal data. 3) Nonreactive: data collection occurs when users' data is unintentionally gathered, minimizing the likelihood of influencing or altering their behavior. 4) Incomplete: While big data is extensive in size, it could still not contain the information essential for the purpose of research. It is crucial to recognize that big data serves a purpose beyond research. 5) Inaccessible: researchers face challenges in accessing raw data retrieved by companies and governments due to privacy concerns. Additionally, the data may be restricted to its original purpose, such as enhancing services, making it inaccessible for other research purposes. 6) Non-representative: big data may not be representative of a broader population, limiting its generalizability; however, it is valuable for making comparisons within the sampled population. 7) Drifting: the population utilizing the system, the system itself, and the ways in which the system is employed are dynamic and subject to continual change. These fluctuations complicate the capacity of big data to track long-term changes over time. 8) Algorithmically confounded: The intricately designed algorithm behind the system is at times utilized to enhance profits for enterprises through increased advertisement clicks or proliferations. This system design, consequently, holds the potential to influence the collection of big data and impacts the study of social science using big data. For example, consider Facebook, where the design is geared towards fostering more connections among individuals. The recorded counts of the number of Facebook friends and the ease of making friends online behaviors may be the byproducts of the system. Consequently, when researching friendship networks on Facebook, the observed patterns may be influenced by the algorithm embedded within the Facebook platform itself. 9) Dirty: big data can be embedded with noise and become useless, as it is automatically collected, potentially containing vast amounts of irrelevant information. 10) Sensitive: the big data itself may contain confidential information that poses a potential threat to privacy. When published, it could lead to emotional or financial harm. 

As we spend more time online, these databases concealed within web services and applications continuously amass details regarding our digital behaviors. This process results in a substantial aspect of our lives being quantified to an unprecedented degree and deposited into extensive online repositories [@gonzalez2013big; @george2014big]. Such user-generated content provides an unfiltered glimpse into people's thoughts, behaviors, and preferences. Social scientists can leverage this data to study various phenomena, from sentiment analysis to political behavior, public opinion shifts, disease outbreaks, economic indicators and consumer trends. It allows for the exploration of the digital footprints people leave behind unintentionally while seeking information online [@askitas2015internet]. Researchers can tap into these data sources to track, analyze, and predict changes as they happen. Furthermore, the ability to connect digital trace data to existing datasets enhances the depth and context of current measurements, enriching the understanding of the evolving socio-cultural landscape [@salganik2019bit]. These traces can be used to gain insights into real-world social dynamics, offering a unique perspective that complements traditional survey-based research [@salganik2019bit]. 

The utilization of big data in social science research elicits dialogues regarding the data's reliability and validity. Some scholars, optimistic about the data-driven research, postulate that this methodology could signal the "end of theory" [@anderson2008end; @steadman_2013]. They argue that the notion behind the data mining process relies on its predictive capacity rather than the understanding of why a particular dataset aligns with a certain pattern. The idea that "prediction trumps explanation" encapsulates this viewpoint, suggesting the primary importance of forecasting over the need for explicative reasoning [@anderson2008end;@steadman_2013]. An exemplary manifestation of this argument can be observed in a practical context, such as Amazon's customer behavior prediction. This e-commerce giant can prognosticate the potential products a customer might purchase based on their previously acquired items. In this scenario, deciphering the link between these product preferences becomes secondary to the ability to predict consumption patterns. The consequential prediction, irrespective of the specific reasoning behind related products, serves as its own narrative, as big data automatically discovers insights regardless of its complexity [@siegel2013predictive; see also: @kitchin2014big].

The other school of thought is more cautious about the use of big data and questions the notion of the "end of theory." Like Durkheim's famous argument that society is more than the sum of its parts, the cyber community is no exception [@durkheim2016elementary; see also: @gonzalez2013big]. While it may appear simple to find links between large datasets and specific social phenomena or traditional data points, it's important to keep in mind that big data is not immune to systematic error, random error, and sampling bias, as certain users in the flow of communication might be overestimated [@kitchin2014big; @kitchin2013big; @gonzalez2013big]. 
One significant risk associated with digital trace data is the temptation to produce largely data-driven studies instead of testing carefully developed hypotheses rooted in current theoretical debates. The sheer volume and accessibility of digital data can lead researchers to conduct exploratory analyses without a clear theoretical framework. This approach can result in research that lacks a strong foundation in established theories, potentially leading to unwarranted or inconclusive findings [@jungherr2018normalizing]. Moreover, the correlations between variables can occasionally be random and subject to manipulation by a researcher [@kitchin2014big; @kitchin2013big; @silver2012signal]. Consequently, recognizing patterns within sizable, intricate datasets gathered through complex algorithms is one aspect, but explaining these identified relationships requires a deep understanding of the context and a solid foundation in robust social theory, which is another challenge altogether [@kitchin2013big]. After all, the web service providers do not save the data trace for research. 

As web service providers may withhold or modify certain data or information, the data available for analysis might not be a complete representation of the digital environment, and the algorithms that shape how data is presented and stored can be complex and obscure, which can possibly create systematic bias. This lack of transparency can raise doubts about the quality and reliability of the data [@jungherr2018normalizing]. Therefore, digital trace data may only reflect a skewed user base, limiting the generalizability of findings. Users of online platforms and services do not represent the entire population, and their behaviors and preferences may differ from those who are not engaged in the digital sphere. This user bias can result in findings that do not accurately represent broader societal trends [@jungherr2018normalizing; @salganik2019bit]. For example, if a study recruits on a specific social media site, the results can only reflect the preferences or opinions of individuals who use that platform and cannot be extended to the entire community. Furthermore, the utilization of platform data may be susceptible to algorithm or technology modifications. It will inevitably contain bias that the researcher cannot control [@stier2020integrating; @marsden1990network]. When evaluating findings, researchers must be cautious since data gaps and algorithmic biases can affect the veracity of their results. 

Understanding the limitations of big data is equally crucial, as it might primarily highlight spurious relations and trends rather than substantial features [@brooks_2013]. Simply relying on statistical associations between digital trace data and social phenomena can lead to misleading or irrelevant findings. Emphasizing causal mechanisms and measurement validation is essential [@salganik2019bit]. To address these limitations, the field of computational social science complements the descriptive nature of big data with inferential statistics, seeking associations and causality [@grimmer2015we]. This approach safeguards the rigor and depth of social science research while harnessing the potential of big data [@kitchin2014big]. 

To draw scientifically sound conclusions from big data, it is necessary to combine evidence from diverse, independent, and unrelated sources. By embracing various resources one can reduce the influence of solitary or biased data sets, this integration ensures the robustness of the conclusions [@george2014big; @jungherr2018normalizing; @askitas2015internet]. This approach enables researchers to design tailor-made research strategies, employ triangulation methods, and eliminate spurious relationships. Connecting diverse data sources allows for a comprehensive examination of complex social and political phenomena, ensuring that findings are not merely products of statistical correlations but are grounded in meaningful causal explanations [@salganik2019bit]. For example, to explore the association between COVID-19 lock down and domestic violence trends, @berniell2021covid collected Google search data from eleven countries, and compared this with the Google mobility data. Similarly, @anderberg2022quantifying utilized Google search data to scrutinize its utility in understanding the correlation between COVID-19 lockdown and instances of domestic violence. In addition to this data, the researchers incorporated 911 calls for service data, and information from domestic violence hotlines, as well as examined datasets from both Los Angeles and England. Their findings revealed that Google search data exhibited a closer alignment with helpline calls than with officially recorded police crimes. These multi-faceted methods allow for a more nuanced understanding of the complex interplay between societal restrictions, mobility patterns, and the prevalence of domestic violence. The use of diverse and independent data sources strengthens the scientific rigor of the study, enabling researchers to draw more reliable and comprehensive conclusions regarding the impact of COVID-19 lockdown on this critical social issue.


As a result, a comprehensive strategy is critical to ensure the responsible and ethical use of digital trace data in social science research. This includes addressing data privacy concerns, quality control, and transparency in the methodologies employed. Furthermore, the establishment of a mechanism that outlines how digital trace data can be harnessed to enrich real-world data estimation reinforces the credibility and reliability of the findings, enabling researchers to make informed and accurate inferences [@salganik2019bit].


In conclusion, while the allure of big data is undeniable, researchers should not be solely captivated by its novelty. As noted by @salganik2019bit, "Big data sources tend to have a number of characteristics in common; some are generally good for social research and some are generally bad" (Ten common characteristics of big data section, para. 1). A conscious effort to scrutinize potential biasing effects of data collection and the reliability of quantitative methods is paramount. It is also important to acknowledge that big data will never serve as a substitute for traditional data from social science. However, they do offer an alternative source for gauging the public's incentives and their responses to the decisions and actions of their representatives [@grimmer2015we; @gonzalez2013big]. It is essential to exercise caution in the utilization of big data by comprehending its limitations and supplementing these limitations with other approaches. While researchers must exercise caution and transparency, embracing the messiness of digital trace data alongside the precision of traditional measures can lead to a more comprehensive understanding of the complex social world we seek to study. By thoughtfully addressing these potential challenges and constraints, researchers can leverage the potential of digital trace data to enhance and augment their social science investigations. Through this endeavor, the exploration of digital trace data can act as a driving force behind the development of robust research methods in the social sciences, underscoring the significance of meticulous analysis and responsible interpretation within a progressively data-abundant environment [@salganik2019bit; @jungherr2018normalizing]. 


# Introduction of Google Trends

Google Trends (GT)^[The Google Trends website can be accessed at https://trends.google.com/trends/?geo=US] serves as a type of digital trace and user-generated data, capturing user search queries within the Google Search Engine. Its effectiveness is intricately linked to its dominance in the web search engine market, and the search population closely mirrors the wider population in the United States. According to a study conducted by the Pew Research Center [-@pew2019], internet usage is prevalent among over 89% of adults in the United States. Furthermore, the demographic composition of internet users exhibits a notable resemblance to the broader population in terms of such factors as ethnicity, wealth, gender, and age [@pew2019; @stubbs2018searching]. Remarkably, among those who utilize the internet, which comprises 91% of individuals, search engines serve as a primary tool for acquiring information. Among these search engines, Google clearly takes the lead as the most favored option, drawing the preference of a significant majority, encompassing 87.7% of users [@pew2012; @statcounter2021]. As outlined by @rowlands2008google, Google's influence extends beyond the student demographic and encompasses parents and educators, signaling a transformation in information-seeking behavior across all age cohorts. The term "Google Generation" was introduced to describe this particular phenomenon. With its extensive usage, Google Search accumulates a significant volume of data, rendering it a valuable resource for researchers in diverse academic disciplines. 

Google Trends has been increasingly acknowledged as a valuable resource for the collection and analysis of data in the realm of social research. This tool offers significant insights into the interests, attitudes, and behaviors of communities, allowing users to analyze the popularity of different search terms in Google search within specific time frames. To ensure privacy protection, Google Trends deliberately excludes any personal information or identifying details, opting instead to aggregate search data into more extensive geographic units, encompassing from global countries to individual states, counties, Designated Market Areas (DMA), and cities. Moreover, Google Trends extends its utility to users by providing tools for visualizing information associated with specific search terms. This visualization functionality spans a wide range of time frames, including hours, days, weeks, months, and years, enabling comprehensive temporal analysis. Additionally, Google Trends facilitates geographic comparisons, illuminating the dynamic variations in search interest across different regions. For instance, as depicted in Figure \ref{figure1}(a), the analysis of the search term "lobster" from 2010 to 2023 highlights that the greatest search interest originated from Maine. This observation suggests that tourists visiting Maine are particularly intrigued by searching for information about lobsters. Notably, the peaks in search activity are concentrated predominantly in February and July, indicating that these months are the most popular seasons in Maine for tourists seeking information about lobsters. Similarly, in Figure \ref{figure1}(b), when examining the search term "air conditioner" from 2010 to 2023, a seasonal pattern emerges with peaks in July in the United States. Louisiana and New York State stand out with the highest search interest during these periods. Consequently, for particular topics, it becomes evident that popularity experiences notable spikes in specific states or regions at precise junctures in time. This offers a nuanced comprehension of the fluctuating patterns of public interest on certain topics across diverse geographic levels 


\begin{figure}[tb]
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=\linewidth]{F1_lobster_gt.png}
    \caption{Google Trends result of ``lobster''.}
    \caption*{Google Trends result of the term ``lobster'' from year 2010 to 2023 in the United States by state}
    \label{figure 1.1}
  \end{subfigure}%
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=\linewidth]{F2_air_conditioner_gt.png}
    \caption{Google Trends result of ``air conditioner''}
    \caption*{Google Trends result of the term ``air conditioner'' from year 2010 to 2023 in the United States by state}
    \label{figure 1.2}
  \end{subfigure}
  \caption{Examples of Google Trends}
  \label{figure1}
\end{figure}

To initiate an inquiry in Google Trends, individuals keen on gauging public interest in a specific topic can either input a particular "term" or make use of the "topic" search function within the Google Trends platform. Notably, a distinction exists between a "term" search and a "topic" search. A "term" search exclusively retrieves data directly related to the specified "term," whereas a "topic" search combines a group of "terms" relevant to that subject, including searches in various languages conveying the same concept as the "topic" [@trens_help_topic_2023]. This approach aims to offer a more precise representation of the primary searches conducted within a specific time frame and geographic region. Additionally, while "topic" searches only allow one word or concept, users can use the "+" or "-" operators to include or exclude specific "terms" in the query on Google Trends [@trends_help_search_tips2023]. This enables users to search various terms simultaneously to retrieve data related to a bundle of terms. Take the interest of "gun", for example. If our sole objective is to gauge the overall popularity of the subject "gun" in the U.S., a direct search for the "topic" of "gun" on Google Trends suffices. However, if our aim is to pinpoint the state where people express the highest interest in purchasing guns, we should focus on the specific "term search" used by potential gun buyers in their online searches, such as "gun sales," "gun store near me," and "where to buy gun", "gun safe", "gun maintenance". By adding various terms, we could better capture the estimates of the search results in Google Trends by increasing the searching population. Figure \ref{figure2}(a) shows the difference of a topic search "Gun" and a combined term search (where buy gun+gun sale+gun store near me+gun shop near me); whereas Figure \ref{figure2}(b) shows the difference of a combined term search (where buy gun+gun sale+gun store near me+gun shop near me) and each single term separately. 

\begin{figure}[tb]
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=\linewidth]{F4_gun_topic_vs_combining_terms.png}
    \caption{Google Trends result of ``topic'' (Gun) v.s. ``combining terms'' (gun sales) from 2010 to 2023}
    \caption*{The blue color indicates topic ``Gun'', and the red color is for the combining terms ``where buy gun+gun sale+gun store near me+gun shop near me''. Overall, the volume of topic search is higher than combined terms. The sudden drop for the topic of ``Gun'' from November 2016 to November 2019 shows the limitation of the big data and their characteristic of incomplete and inaccessible (Salganik, 2019).}
    \label{figure 2.1}
  \end{subfigure}%
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=\linewidth]{F3_gun_sale_combining terms.png}
    \caption{Google Trends result of ``single terms'' v.s. ``combining terms'' from 2010 to 2023}
    \caption*{The blue, red, yellow, and green color are the search results in Google Trends of terms ``where buy gun'', ``gun sale'', ``gun store near me'', and ``gun shop near me'', respectively. The purple color is the search result of combing all aforementioned four terms together, which overall has a higher volume than single terms}
    \label{figure 2.2}
  \end{subfigure}
  \caption{The Differences Among ``topics'', ``terms'', and ``combining terms'' in Google Trends}
  \label{figure2}
\end{figure}

Following this, the collected data is subjected to a process of standardization and is subsequently represented on a numerical scale that spans from 0 to 100. Within this scale, a numerical value of 100 signifies the utmost degree of popularity attributed to a particular term, whereas a value of 0 denotes an absence of adequate data pertaining to the said term. Note that the scale employed in this context does not accurately represent the precise search volume. Instead, it serves as an indicator of the degree of interest in the keyword relative to the highest data point depicted on the chart, considering the chosen region and timeframe [@trends_help_2023; @rogers_trends_2016]. Utilizing the aforementioned combined terms (where to buy gun + gun sale + gun shop near me + gun store near me) as an example, the results from 2010 to 2023 across DMA in Google Trends are illustrated in Figure \ref{tab:table1}.

```{r table1, results='asis', message=FALSE, warning = FALSE, echo = FALSE}
# Read the CSV file
data <- read.csv("combining_terms_example.csv")
data <- na.omit(data)

middle_rows <- data.frame(Column1 = rep("...", 1), Column2 = rep("...", 1))
names(middle_rows) <- names(data)

gt_example = knitr::kable(rbind(head(data, 10), middle_rows, tail(data, 10)), format = "latex", booktabs = T, row.names = F, linesep = "", caption = "The Highest Ten and Lowest Ten Results of Combined Search Terms (where buy gun + gun sale + gun shop near me + gun store near me) from 2010 to 2023 across DMA in the United States") %>%
  kable_styling(latex_options = "hold_position") %>%
  footnote(number = c("Values equal to `NA' was dropped in the table", "`...' = omitted rows" ), footnote_as_chunk = T)

print(gt_example, scalebox = 0.6, type="latex", table.placement = "!htb")

```

# The Applications of Google Trends in Social Science Research

The application of Google Trends has been extensively investigated by social scientists in order to comprehend how technological advancements can contribute to the acquisition of informational data for social science research. Scholars have employed Google Trends as a valuable tool to acquire significant insights into various aspects of societal constructs, concepts, or measurements. @choi2012predicting posit that empirical findings indicate the potential of Google Trends data to provide real-time and high-frequency information on diverse subjects, including automobile sales, unemployment claims, travel planning, and consumer confidence. 

The application of Google Trends has been identified in various fields within the public health, with a particular emphasis on disease surveillance. The scholars employed Google Trends as a means of monitoring the occurrence of influenza outbreaks among a particular population. Through their investigation, they found that Google search queries have the potential to accurately assess the current level of weekly/monthly influenza activity regionally and globally. For example, @ginsberg2009detecting found that Google Trends is useful to monitor weekly flu levels in the United States, and other studies found similar or the same results [@pelat2009more; @rovetta2021reliability; @kang2013using; @carneiro2009google; @teng2017dynamic; @rangarajan2019forecasting; @preis2014adaptive; @gianfredi2018monitoring; @gluskin2014evaluation; @eysenbach2006infodemiology]. This approach is also beneficial for the estimation of disease prevalence in cases involving sensitive subjects. The internet offers enhanced anonymity, as demonstrated in the case study on AIDS by @mavragani2018forecasting, which can significantly contribute to addressing this issue. 

Google Trends data has been extensively utilized in the examination of its potential in understanding economic behavior. The results of studies of economic behavior demonstrate that Google Trends data has the potential to function as a predictive instrument for various economic indicators, including the economic decision-making behavior in diverse markets [@kristoufek2013can], trading behavior in financial markets [@vosen2011forecasting; @preis2013quantifying; @choi2012predicting], estimating the volumes of visitors/tourists [@clark2019bringing; @mavragani2018assessing; @park2017short; @yu2019online; @bangwayo2015can; @yang2015forecasting], the numbers of hotel registrations [@pan2012forecasting; @rivera2016dynamic], private consumptions [@vosen2012monthly; @vosen2011forecasting; @boone2018can; @silva2019googling], emerging markets in automobile sales [@carriere2013nowcasting], forecasting unemployment insurance claims [@aaronson2022forecasting; @mulero2021forecasting; @vicente2015forecasting; @NACCARATO2018114], and public interest in insurance after major earthquakes [@gizzi2020time].


In addition to the existing research on the disease surveillance and economic applications of Google Trends, social scientist from various fields continue to investigate the utilization of this tool. Their findings indicate that the use of Google Trends shows potential in accurately estimating the distribution of religious affiliations within a given population [@scheitle2011google; @adamczyk2021understanding; @adamczyk2019using; @adamczyk2022using],  the effects of COVID-19 pandemic and the implementation of social distancing on public mental health concerns and information-seeking behavior [@knipe2020mapping; @bento2020evidence; @brodeur2021covid; @walker2020use], public attention/awareness on local natural disaster, such as flood or drought [@yeo2019public; @kam2019monitoring], public interest on conservation topics [@nghiem2016analysis], and asylum-related migration flows [@carammia2022forecasting; @wanner2021well; @bohme2020searching].

In addition to its applications in the fields of economics and public health, Google Trends serves as a valuable tool for understanding public sentiment and exploring various social phenomena. Researchers can employ this tool to evaluate public interest and sentiment pertaining to diverse subjects, including political affairs and social phenomena; the election results prediction [@lui2011predictability]; social distress [@knipe2021google], and the rise of right-wing extremism [@dean2016right]. Therefore, Google Trends data holds considerable value in assessing the impact of public discourse on societal dynamics and can provide valuable contextual information for social science research [@jun2018ten; @xu2023collective]. Furthermore, Google Trends enables the analysis of emerging social phenomena by tracking search queries related to new trends and topics, thus providing a real-time measure of societal interests and concerns. As a result, Google Trends can play a pivotal role in understanding a wide range of social phenomena. For instance, when examining the topic of "NBA" in Google Trends over the past twelve months [December 7th, 2022, to December 6th, 2023, see Figure \ref{figure3}(a)], "Victor Wembanyama" emerges as the top "rising" related "queries" and "topics". Similarly, opting for "top" related "topics" or "queries" instead of "rising" reveals that the foremost related "topics" is "NBA - League." As to related "queries", "NBA" ranked as first and "store NBA" securing the second position [see Figure \ref{figure3}(b)].

\begin{figure}[tb]
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=\linewidth]{F5_rising_topics.png}
    \caption{Google Trends result of related topics and queries of ``NBA'' (Rising)}
    \label{figure 3.1}
  \end{subfigure}%
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=\linewidth]{F6_top_topics.png}
    \caption{Google Trends result of related topics and queries of ``NBA'' (Top)}
    \label{figure 3.2}
  \end{subfigure}
  \caption{The Comparison of ``top'' and ``rising'' Related Topics and Related Queries of The Topic ``NBA'' in Google Trends for the Past Twelve Months (December 7th 2022 to December 6th 2023)}
  \label{figure3}
\end{figure}

Scholars have discovered that Google Trends is especially useful when there is a lack of access to vulnerable or sensitive populations. For example, @chykina2018using tried to use GT to understand hard-to-reach populations such as illegal immigrants’ concerns about the Trump Administration’s policy changes on immigration laws. They compared the volume of Google search queries that were indicative of immigrants’ fear of deportation (e.g., “will I be deported”) before and after the 2016 election of Trump. Their findings showed that several peaks of the time trends of Google search queries correspond with the times when Trump was elected, sworn in, and signed the executive order commonly referred to as the “Muslim ban.” Utilizing a sensitive topic involving the public's racial bias toward Black candidates, @stephens2014cost found that Google Trends offers capabilities for assessing such biases. These results show that Google Trends data may be superior to traditional survey methods when sensitive topics or populations are involved. Survey response accuracy is often questionable partially because respondents’ answers to a survey question are influenced by social acceptability. However, while utilizing search engines, internet users operate without the awareness that they are, to some extent, self-reporting their thoughts to others—particularly when they are not fully cognizant of how their search data is being utilized.

# The Applications of Google Trends in Criminal Justice Research

For research on crime-related issues, the application of Google Trends (GT) remains relatively under-explored. Since GT was first introduced in 2006, only a few studies have included GT data in their analyses of criminal justice issues. @gamma2016could used GT to produce time-series data on the search term “meth” and compared the GT data to methamphetamine-related activity recorded by government agencies in Switzerland, Germany, and Austria. Their results showed that the trends captured by GT paralleled official crime statistics in the three European countries. They recommended further utilization of GT data to determine whether it could be served as an instrument for predicting drug-related crimes. 

@gross2017there utilized Google AdWords, which is a similar platform to GT that provides data on search term activity, to create a measure of concern about police violence. The phrases they used were “Black Lives Matter,” “police brutality,” “police shooting,” “cop shooting,” “police shootings,” “cop shootings,” “I hate cops,” and “I hate police.” They examined the relationship between the measure of concern about police violence drawn from Google AdWords data and homicide/other violent crime rates compiled by the Major Cities Chiefs of Police Association (MCCA). Their findings suggested that concern about police violence calculated from Google search queries is positively related to rates of homicide and other violent crimes. The authors argued that their findings supported the hypothesis of a “Ferguson Effect,” whereby anti-police attitudes led police to reduce traditional policing activity, resulting in higher crime levels. 

@stubbs2018searching found that higher property crime levels were associated with a greater volume of specific Google search terms. For instance, the UCR’s MVT rates strongly correlated with the search term “car alarm system,” and the UCR’s burglary rates strongly correlated with the search term “home alarm system.” Moreover, they used these crime prevention Google search queries. @stubbs2018searching divided their crime prevention Google queries into four categories, they are 1) Target hardening: deadbolt, door locks, gate keypad, security door, and window bars; 2) Surveillance: alarm system, car alarm system, home alarm system, home security camera, home security system, motion detection camera, motion detection lights, and street lights; 3) Formal social control: how to file a police report, and report crime; 4) Informal social control: neighborhood crime, neighborhood security, neighborhood watch, neighborhood watch sign.] as factors within a situational crime prevention model. They found significant associations between Google search queries and the reduction in property crime over time

@liu2023big conducted a cross-sectional study in which they obtained crime estimate data from GT by utilizing victim-like search terms for Designated Market Areas (DMA) across the United States from 2010 to 2019. They correlated GT estimates of MVT, burglary, larceny, and rape with UCR data, and observed the concurrent validity of GT estimate with common crime covariates. They found that GT data is valid in estimating MVT, larceny, and rape. Moreover, their findings suggested that GT data exhibited greater reliability than the rape data in UCR, which echoes earlier criminological research, that  rape crime statistics in UCR are not reliable. Furthermore, they showcased the application of GT data in identifying the under-reported areas of rape crimes. 

The research undertaken by @liu2023big unveiled an innovative application of big data, promising far-reaching implications for future investigations in this evolving field. Their discovery of GT data's utility in rape estimate affirmed the previous finding of online, user-generated data for estimating the statistics related to sensitive populations, such as bias toward Black candidates, illegal immigrants, and people with AIDS [@stephens2014cost; @chykina2018using; @mavragani2018forecasting]. Their study strongly advocated for expanding research efforts by embracing a broader spectrum of data sources, including the NCVS data, and employing alternative units of analysis beyond the confines of Designated Market Areas and annual data. Additionally, the authors underscored the importance of employing a range of methodological approaches, such as those centered on establishing causal relationships and analyzing panel data. The comprehensive examination of various aspects is crucial for fully harnessing the capabilities of GT as a valuable instrument for estimating crime.


# The Limitations of Google Trends Data

The utilization of Google Trends in social science research is not without skepticism. GT data is a form of digital trace data and falls under the umbrella of big data. It shares key characteristics with other digital trace data, as outlined by @howison2011validity and @salganik2019bit. Notably, GT data is a by-product of individuals' search behavior and is not originally intended for research purposes. It is non-representative because the data was exclusively collected from individuals with internet access, potentially introducing bias that may distort inferences about the broader target population. Furthermore, the data is inherently incomplete and inaccessible in its raw form, as Google provides only already computed values. 

Adding to its complexities, GT data is algorithmically confounded and contains unknown noises. GT has gone through various methodological improvements on specific dates (1/1/2011, 1/1/2016, 1/1/2017, and 1/1/2022). However, the lack of comprehensive documentation makes it challenging to discern the nature of these changes. In some instances, these alterations may even lead to anomalous trends. For example, when examining the search trends of major cell phone and electronic brands (LG, Samsung, Apple) in Figure \ref{figure4}(a), an unusual and sudden drop is observed for LG, Samsung, and Apple (as a topic, not the technology company) from November 2016 to November 2019. A peculiar trend between searches for "Apple" (as a topic) and "Apple" (as a technology company) reveals a notable similarity before November 2016, followed by a divergence that persists after November 2016. In comparison, the term "Bible," serving as a control group, exhibits neither significant change before or after the methodological shift in January 2017 nor November 2016. This also happened on the gun topic search as shown in Figure \ref{figure2}(a). The unknown drop of data volumes in LG, Samsung, Apple and Gun searches underscores the opacity and inexplicability surrounding changes within GT data. Interestingly, this abrupt decline does not appear to influence "term" searches, as illustrated in Figure \ref{figure4}(b). Both the "gun" and "LG" term searches maintain relatively stable trends before and after November 2016. It is noteworthy that the "topic" searches for "Gun" and "LG" exhibit divergent trends in search volumes.

\begin{figure}[tb]
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=\linewidth]{F7_gt_errors_method_change1.png}
    \caption{The Unidentified Alteration in Google Trends Values Before and After the Methodological Improvement on January 1st, 2017}
    \label{figure 4.1}
  \end{subfigure}%
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=\linewidth]{F9_gt_errors_terms_not_affected.png}
    \caption{No Discernible Impact on ``Term'' Searches in Google Trends Values Before and After the Methodological Improvement on January 1st, 2017.}
    \label{figure 4.2}
  \end{subfigure}
  \caption{Comparing ``Topic'' and ``Term'' Results in Google Trends Regarding the Unidentified Alteration in Google Trends Values Before and After the Methodological Improvement on January 1st, 2017.}
  \label{figure4}
\end{figure}

As it shares the traits of digital trace and big data, Google Trends is very sensitive to queries. For instance, opting for "term" or "topic" while searching the same word in Google Trends can yield different results. Generally, topic searches tend to have higher volumes as they encompass a collection of diverse terms. Additionally, if there's a need to exclude specific terms or topics, the use of "-" allows for the exclusion of related searches, resulting in a noticeable decrease in total volume.

Consider the example of "Auto Theft", where the popularity of the video game "Grand Theft Auto (GTA)" influences the overall search volume. As shown in Table \ref{tab:table2}, a comparison between "term" searches and "topic" searches for the identical key-in words "Grand Theft Auto" from 2004 to 2023 in the United States illustrates distinct outcomes. The topic search displaying a higher volume compared to the terms search of "Grand Theft Auto." Simultaneously, Excluding the interest of GTA in the term "Auto Theft" using the minus sign "-" in the search query, reveals a decrease in overall volume in Google Trends searches.

```{r table2, results='asis', message=FALSE, warning = FALSE, echo = FALSE}
# Read the CSV file
data2 <- read.csv("T2_gt_sensitivity.csv")

middle_rows2 <- c(Col1 = rep("...", 1), Col2 = rep("...", 1), Col3 = rep("...", 1), Col4 = rep("...", 1), Col5 = rep("...", 1), Col5 = rep("...", 1))

names(middle_rows2) <- names(data2)

df <- rbind(head(data2, 12), middle_rows2, tail(data2, 12))

colnames(df) <- c("Month", "GTA (Term)", "GTA (Topic)", "Auto Theft (Term)",  "Auto Theft - GTA (Term)",  "MVT (Topic)")

gt_sensitivity = knitr::kable(df, 
                              align = "c",
                              format = "latex", 
                              booktabs = T, 
                              row.names = F, 
                              linesep = "", 
                              longtable = F,
                              caption = "Exploring Sensitivity in Google Trends Data: Comparing Cases with Varied Choices of Terms and Topics", 
                              escape = F,  
                              col.names = linebreak(
                                c("Month", "GTA\n(Term)", "GTA\n(Topic)", "Auto Theft\n(Term)",  "Auto Theft - GTA\n(Term)",  "MVT\n(Topic)"), align = "c")) %>%
  kable_styling(latex_options = "hold_position", font_size = 10) %>%
  footnote(number = c("GTA = Grand Theft Auto", "MVT = Motor Vehicle Theft", "`...' = omitted rows"), footnote_as_chunk = T, threeparttable = T)

print(gt_sensitivity, type="latex", table.placement = "!htb", scalebox = 0.8)
```

Google Trends data originates from a primarily "sample" of search queries in Google Search Engine [@trends_help_2023]. This sampling approach is essential due to the sheer volume of daily Google searches with over billions of searches in real-time. For faster response, Google Trends utilizes data samples rather than the entire dataset. This choice, however, comes at the cost of data drift over time. As the sampled data periodically changes, results can show inconsistencies. Unfortunately, Google doesn't offer clear documentation on when these sample shifts occur, making it challenging to fully account for their impact. In Table \ref{tab:table3}, the data points at 05:21 on December 5th exhibit a notable similarity to those at 08:36. Nevertheless, when observing the data for Baltimore, MD, and Tulsa, OK, the difference of the data points can be observed. Subsequent shifts in the data are observed at 12:11, with consistency maintained at 15:37, only to undergo another shift at 19:02. This example highlights the inadequacy of relying on a single sample from Google Trends to capture the dynamic shifts in data. 

Due to the presence of data shifting, it is advisable to invest time in acquiring diverse data points over a period of time in order to foster a more comprehensive understanding of search interests of the target population. This involves averaging data points acquired on different dates and times from Google Trends. This method offers several advantages. Firstly, it helps alleviate the influence of extreme values within the samples. Secondly, the methodology addresses the potential occurrence of missing values. As previously mentioned, when the search volumes in the sample are too small, Google Trends returns zeros, resulting in a missing value. However, upon querying the same term in Google Trends at different dates or times, the original missing value may unexpectedly emerge. These emerged values are valuable as they can enhance the overall sample size of the data. Taking the Google Trends rape data from @liu2023big as an example, in Table \ref{tab:table4}, the missing values are represented as "NA". When the same query (I raped+raped me+raped report police+being raped+been raped - dream - kobe - trump - porn) was accessed at different dates or times, the original missing values were retrieved (see DMA Boise ID, Davenport IA-Rock Island-Moline IL, Evansville IN, etc.). This method effectively expands the sample size of data points for rape crime estimates from Google Trends by averaging samples across various dates and times.


```{r table3, results='asis', message=FALSE, warning = FALSE, echo = FALSE}

# Read the CSV file and select and rename the last two columns (DMA, GT MVT)
day6 <- read.csv("MVT_10_19_2010-01-01 2019-12-31 tz=360 3652days USDMA0_202012052228.csv") %>%
  dplyr::select((ncol(.) - 1):ncol(.)) %>%
  setNames(c("DMA", "GT.MVT.12052228"))
day5 <- read.csv("MVT_10_19_2010-01-01 2019-12-31 tz=360 3652days USDMA0_202012051902.csv")%>%
  dplyr::select((ncol(.) - 1):ncol(.)) %>%
  setNames(c("DMA", "GT.MVT.12051902"))
day4 <- read.csv("MVT_10_19_2010-01-01 2019-12-31 tz=360 3652days USDMA0_202012051537.csv")%>%
  dplyr::select((ncol(.) - 1):ncol(.)) %>%
  setNames(c("DMA", "GT.MVT.12051537"))
day3 <- read.csv("MVT_10_19_2010-01-01 2019-12-31 tz=360 3652days USDMA0_202012051211.csv")%>%
  dplyr::select((ncol(.) - 1):ncol(.)) %>%
  setNames(c("DMA", "GT.MVT.12051211"))
day2 <- read.csv("MVT_10_19_2010-01-01 2019-12-31 tz=360 3652days USDMA0_202012050846.csv")%>%
  dplyr::select((ncol(.) - 1):ncol(.)) %>%
  setNames(c("DMA", "GT.MVT.12050836"))
day1 <- read.csv("MVT_10_19_2010-01-01 2019-12-31 tz=360 3652days USDMA0_202012050521.csv")%>%
  dplyr::select((ncol(.) - 1):ncol(.)) %>%
  setNames(c("DMA", "GT.MVT.12050521"))
#merge data
merged_data <- merge(day1, day2, by = "DMA", all = T) %>% 
  merge(day3, by = "DMA", all = T) %>% 
  merge(day4, by = "DMA", all = T) %>% 
  merge(day5, by = "DMA", all = T) %>% 
  merge(day6, by = "DMA", all = T)

middle_rows3 <- c(Col1 = rep("...", 1), Col2 = rep("...", 1), Col3 = rep("...", 1), Col4 = rep("...", 1), Col5 = rep("...", 1), Col6 = rep("...", 1), Col7 = rep("...", 1))

names(middle_rows3) <- names(merged_data)

df2 <- rbind(head(merged_data, 8), middle_rows3, tail(merged_data, 10))

#get the data into table
gt_sampele_shifting = knitr::kable(df2, 
                              align = "lcccccc",
                              format = "latex", 
                              booktabs = T, 
                              row.names = F, 
                              linesep = "", 
                              longtable = F,
                              caption = "The Dynamics of Data Points Shifted in Google Trends Results through Tri-Hourly Data Requests on December 5th, 2020 by DMA", 
                              escape = F,  
                              col.names = linebreak(
                                c("DMA", "GT MVT\n12/05/2020\n05:21", "GT MVT\n12/05/2020\n08:36", "GT MVT\n12/05/2020\n12:11",  "GT MVT\n12/05/2020\n15:37",  "GT MVT\n12/05/2020\n19:02", "GT MVT\n12/05/2020\n22:28"), align = "c")) %>%
  kable_styling(latex_options = "hold_position", font_size = 8) %>%
  footnote(number = c("DMA = Designated Market Areas","GT = Google Trends", "MVT = Motor Vehicle Theft", "`...' = omitted rows", "The search term is `car stolen+find stolen car+report police stolen car+insurance car stolen - dream - check', and the time frame is from year 2010 to 2019"), footnote_as_chunk = T, threeparttable = T)

print(gt_sampele_shifting, type="latex", table.placement = "!htb", scalebox = 0.7)

```

```{r table4, results='asis', message=FALSE, warning = FALSE, echo = FALSE}

# Read the CSV file and select and rename the last two columns (DMA, GT MVT)
r_day6 <- read.csv("Rape_10_19_2010-01-01 2019-12-31 tz=360 3652days USDMA0_202103250144.csv") %>%
  dplyr::select((ncol(.) - 1):ncol(.)) %>%
  setNames(c("DMA", "GT.Rape.03250144"))
r_day5 <- read.csv("Rape_10_19_2010-01-01 2019-12-31 tz=360 3652days USDMA0_202103242230.csv")%>%
  dplyr::select((ncol(.) - 1):ncol(.)) %>%
  setNames(c("DMA", "GT.Rape.03242230"))
r_day4 <- read.csv("Rape_10_19_2010-01-01 2019-12-31 tz=360 3652days USDMA0_202103241916.csv")%>%
  dplyr::select((ncol(.) - 1):ncol(.)) %>%
  setNames(c("DMA", "GT.Rape.03241916"))
r_day3 <- read.csv("Rape_10_19_2010-01-01 2019-12-31 tz=360 3652days USDMA0_202103200627.csv")%>%
  dplyr::select((ncol(.) - 1):ncol(.)) %>%
  setNames(c("DMA", "GT.Rape.03200627"))
r_day2 <- read.csv("Rape_10_19_2010-01-01 2019-12-31 tz=360 3652days USDMA0_202103200237.csv")%>%
  dplyr::select((ncol(.) - 1):ncol(.)) %>%
  setNames(c("DMA", "GT.Rape.03200237"))
r_day1 <- read.csv("Rape_10_19_2010-01-01 2019-12-31 tz=360 3652days USDMA0_202103192247.csv")%>%
  dplyr::select((ncol(.) - 1):ncol(.)) %>%
  setNames(c("DMA", "GT.Rape.03192247"))
#merge data
r_merged_data <- merge(r_day1, r_day2, by = "DMA", all.x = TRUE, all.y = TRUE) %>% 
  merge(r_day3, by = "DMA", all.x = TRUE, all.y = TRUE) %>% 
  merge(r_day4, by = "DMA", all.x = TRUE, all.y = TRUE) %>% 
  merge(r_day5, by = "DMA", all.x = TRUE, all.y = TRUE) %>% 
  merge(r_day6, by = "DMA", all.x = TRUE, all.y = TRUE)

# Identify rows with missing values in any column
rows_with_missing <- r_merged_data[apply(r_merged_data, 1, anyNA), "DMA"]


# Filter the rows above and below those with missing values
result_df <- r_merged_data[which(r_merged_data$DMA %in% rows_with_missing | 
                                  c(r_merged_data$DMA[-1], NA) %in% rows_with_missing |
                                  c(NA, r_merged_data$DMA[-length(r_merged_data$DMA)]) %in% rows_with_missing), ]

# Function to create new dataframe 
insertRow <- function(data, new_row, r) { 
  data_new <- rbind(data[1:r, ],             
                    new_row,                 
                    data[- (1:r), ])         
  rownames(data_new) <- 1:nrow(data_new)     
  return(data_new) 
} 

indices_to_insert = c(3, 7, 11, 15, 20, 24)

for (i in indices_to_insert) {
  result_df <- insertRow(result_df, "...", i)
}

result_df <- rbind("...", result_df, "...")

result_df[is.na(result_df)] <- "\\textbf{NA}"

# Display the rows with missing values
#get the data into table
gt_sampele_missing_values = knitr::kable(result_df, 
                              align = "lcccccc",
                              format = "latex", 
                              booktabs = T, 
                              row.names = F, 
                              linesep = "", 
                              longtable = F,
                              caption = "The Presence and Retrieval of Missing Values in Google Trends Data Across Different Dates and Time", 
                              escape = F,  
                              col.names = linebreak(
                                c("DMA", "GT Rape\n03/19/2021\n22:47", "GT Rape\n03/20/2021\n02:37", "GT Rape\n03/20/2021\n06:27",  "GT Rape\n03/24/2021\n19:16",  "GT Rape\n03/24/2020\n22:30", "GT Rape\n03/25/2021\n01:44"), align = "c")) %>%
  kable_styling(latex_options = "hold_position", font_size = 7) %>% 
  #column_spec(column = 1, width = "10em") %>%
  footnote(number = c("DMA = Designated Market Areas","GT = Google Trends", "NA = missing values", "`...' = omitted rows" ,"The search term is `I raped+raped me+raped report police+being raped+been raped - dream - kobe - trump - porn', and the time frame is from year 2010 to 2019"), footnote_as_chunk = T, threeparttable = T)

print(gt_sampele_missing_values, type="latex", table.placement = "!htb", scalebox = 0.7)

```

Finally, although it is plausible to utilize the 0–100 range to express search data, it's crucial to note that this percentage does not accurately represent the real count of search volume. Consequently, our knowledge is confined to understanding the ranking and the relative search interest across diverse geographic areas, without insight into the actual incidence of that search in a specific region. Another challenge with this 0–100 range comparison arises from the disparate numerators and denominators between traditional survey data and Google Trends data. Traditional survey data features an actual count of the topic of interest in a given area (numerator), divided by the population in that area (denominator). In contrast, Google Trends data involves the search volume of specific keywords in a given area (numerator), divided by the total search volume from the population in that area (denominator). These differences introduce a potential source of error in comparing traditional survey data with Google Trends data, particularly if the usage patterns of Google Search vary systematically across different geographic areas.

Due to the limitations mentioned above, some scholars approach Google Trends data with skepticism regarding its utility in social science. @mellon2013and compared various terms in Google Trends and found that the most notable application of Google Search data is related to economic topics, rather than issues, such as immigration or terrorism. @lazer2014parable expressed concerns regarding the possibility of 'big data hubris' and excessive dependence on Google Trends or similar tools, which may come at the cost of traditional data collection methods. According to their perspective, it is recommended to utilize Google Trends as a supplementary instrument rather than a substitute for traditional methodologies. 

# Chapter Summary

In this chapter, I have explored the prudent utilization of big data within the realm of social studies. The exploration has extended to the current advancements in harnessing Google Trends data for applications in social sciences, with a particular focus on its role within the field of criminal justice. While a limited number of studies have tapped into the reservoir of big data provided by Google Trends and similar tools, it becomes evident that the potential of these resources in criminological research remains largely untapped. Therefore, it is paramount to advocate for further research endeavors aimed at fully realizing the utility of these tools, acknowledging their inherent limitations, and paving the way for a more comprehensive and insightful approach to criminological inquiry. 

Indeed, there are definite limitations and challenges associated with the application of big data and Google Trends data in social science studies. However, the user-driven and digital trace data adopts a participant-centered approach, and its scale is so immense that it draws strength from its extensive sample size [@salganik2019bit], allowing us to zoom in on smaller population subsets and obtain infrequent data points, which is the task that is challenging for traditional surveys. Similarly, @lazer2021meaningful noted that in the twenty-first century, scholars will increasingly incorporate micro-level behavioral data to gain a deeper understanding of how interdependent structures give rise to distinct macro-level patterns in social science. This perspective acknowledges that "big" data, stemming from the digital trace of numerous individuals, can unveil a broader and more comprehensive understanding than previously attainable. In essence, "big" data provides insights analogous to a mosaic picture -- although not apparent upon close examination, a broader perspective allows us to grasp the complete picture.

Likewise, in the realm of crime statistics, assuming that victims turn to the Google Search engine for help or information online, the data derived from these individual searches becomes a valuable resource for estimating victimization rates. This approach can provide us with a clearer picture of the dark figure of crime. The study by @liu2023big highlighted the potential of utilizing Google Trends data to estimate victimization rates in the United States, shedding light on under-reported areas of crime. However, their findings heavily leaned on data from UCR, narrowing the scope to a single type of data resource and limiting generalizability. Additionally, their investigation was confined to a specific geographic unit of analysis, namely DMA. Moreover, their study was restricted to a cross-sectional analysis, limiting the depth of their exploration into the temporal fluctuations of GT data.

To explore the value of Google Trends data in criminal justice research, the next chapter introduces the specific datasets and methodologies utilized in this analysis. Expanding upon the groundwork laid by @liu2023big, this research will encompass an in-depth exploration of state-level and city-level MVT data with temporal elements. Furthermore, this research will integrate diverse data sources beyond the UCR, encompassing 911 calls for service and NCVS data. This approach will be underpinned by rigorous statistical methods. The primary aim is to evaluate the reliability and validity of Google Trends data in relation to motor vehicle theft.

<!--

"Many non-ornithologists are initially extremely skeptical when they hear about eBird for the first time. In my opinion, part of this skepticism comes from thinking about eBird in the wrong way. Many people first think “Are the eBird data perfect?”, and the answer is “absolutely not.” However, that’s not the right question. The right question is “For certain research questions, are the eBird data better than existing ornithology data?” For that question the answer is “definitely yes,” in part because for many questions of interest—such as questions about large-scale seasonal migration—there are no realistic alternatives to distributed data collection." [@salganik2019bit]

"7.2.2 Participant-centered data collection
Data collection approaches of the past, which are researcher-centered, are not going to work as well in the digital age. In the future, we will take a participant-centered approach."[@salganik2019bit]

# Chapter Summary

[However, these data streams have some critical compensating features. Sensor technologies may fill in important data gaps, giving visibility to those who would otherwise be erased from the map. Satellite imagery, for instance, has been used to build indicators of wealth and poverty in the Global South when surveys of household income and consumption do not exist53. The banal pervasiveness of modern technology means representation will in many cases be superior to traditional data-collection mechanisms—it is cheaper to own a mobile phone than a home. There are parallels here to the administrative data that W. E. B. Du Bois used to study African-American individuals in the late nineteenth and early twentieth century54. The data of an administrative state that enforced racial hierarchy were surely not neutral, yet still had critical value in providing visibility of those most precariously positioned in society.

Furthermore, large sample sizes allow us to look at the behavior of subsets of the data, for example, minorities (generally construed) and events that are statistically uncommon but consequential (for example, hate speech or misinformation)49,55,56. In these cases, sample size and our ability to zoom into smaller populations and infrequent data points matters more than the representativeness of the sample57. As Pareto observed long ago, many human behaviors are concentrated in tiny slices of the population58; however, twentieth century methods were generally poorly suited to studying that social reality. Perhaps the social theories of the twenty-first century will be able to use micro-level behavioral data to understand how structures of interdependence yield certain macro-level patterns59.]

[lazer2021meaningful]

-->
\FloatBarrier
<!-- Chapter 4 -->
\newpage
\fancyhead[L]{Chapter 5: Motor Vehicle Theft}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}

\chapter{MOTOR VEHICLE THEFT}

Before discussing the methodologies and data, it is essential to understand the rationale behind choosing motor vehicle theft (MVT) as the research focus for this dissertation, and its underlying mechanisms, as well as to review relevant studies. The following sections will delve into these aspects, presenting a comprehensive overview of current findings and theoretical frameworks that inform the study of motor vehicle theft. This foundation will set the stage for a detailed exploration of the methods employed and the new insights this research aims to provide.

# Motor Vehicle Theft -- Why Choose MVT When Studying GT?

Motor vehicle theft, also referred to as auto theft or car theft, is legally defined as the act of stealing or attempting to steal a motor vehicle. The term "vehicle" encompasses various types, including trucks, buses, motorcycles, and snowmobiles, but excludes farm or construction equipment or watercraft [@investigation2020crime]. In NIBRS, the system is largely similar to UCR, covering the theft of automobiles, trucks, buses, and snowmobiles [@nibrs_ms_definition; @nibrs_denver_definition]. In NCVS, it is defined as "the unlawful taking or attempted taking of a self-propelled road vehicle owned by another, with the intent of permanently or temporarily depriving the owner of the vehicle" [@ncvs_mvt_definition].

The impact of MVT in the United States is substantial. In 2019, the United States experienced roughly 721,885 incidents of MVT, resulting in an estimated rate of 219.9 MVT cases per 100,000 residents. The total estimated financial loss due to MVT nationwide for that year exceeded \$6 billion, with an average of \$8,886 lost per stolen vehicle [@investigation2020crime]. Furthermore, the average victim cost (\$6,800) and total cost (\$14,836) per MVT case is the highest among all property crimes [@la2011evaluating]. Out of all the vehicles stolen in 2019, automobiles constitutes 74.5% of the total [@investigation2020crime].

Given the presence of the dark figure of crime, when validating novel crime estimates from Google Trends, a reliable crime statistic with a high reporting rate (i.e., with a minimal dark figure of crime) is preferred, as low reporting error indicates low measurement error in crime statistics^[As noted in \eqref{myeqn11}, the reporting error, $U$, is proportionate to the measurement error in the data.]. The crime statistics from UCR MVT are recognized as one of the most reliable and accountable among police-reported crime statistics [@o1980empirical; @gove1985uniform; @blumstein1991trend; @hart2003reporting; @tarling2010reporting; @cohen1984discrepancies; @weatherburn2011uses]. This could be attributed to its high reporting rate in MVT. According to data from the NCVS, motor vehicle theft is the most commonly reported property crime in the United States [@bjs2022ncvs_2019]. NCVS recorded that 79.5% of motor vehicle crime victims reported the crime to the police in 2019 [@morgan2021criminal]. 

With an established and reliable crime statistic like MVT, we can use it as a benchmark to compare with a new measurement tool of estimating MVT. This involves not only directly comparing official MVT data with data from new measurement by testing their linear relationships but also evaluating their concurrent validity. Concurrent validity is crucial for validating new measurements by comparing them against established benchmarks and predictors already associated with the measure [@churchill1979paradigm; @bergkvist2007predictive; @strauss2009construct; @o1980empirical]. This method assesses how closely the relationships between new measurements and those established predictors align with those of already established measures.

In crime statistics, consider a new method developed to measure MVT rates using social media and traffic pattern data. Researchers could validate this new type of MVT data by comparing its beta coefficient predictors with those derived from established criminological theories and the most accurate existing MVT data, known for minimal underreporting. A close alignment between the predictions from the new model and the existing data would indicate strong concurrent validity, demonstrating the model's effectiveness in accurately measuring motor vehicle theft

This process is essential for determining the predictive ability of a new measurement tool and how closely it aligns with established MVT data. Moreover, the explanations predictors of MVT through criminological lens, primarily social disorganization and routine activity theory, will provide the theoretical compass by testing the concurrent validity of the new measurement tool, along with the examination of the relevant literature.

# Social Disorganization Theory and Motor Vehicle Theft
The examination of factors contributing to MVT often intersects with the theoretical framework of social disorganization theory, which is associated with the Chicago School. @shaw1942juvenile introduced the concept of community social disorder, subsequently undergoing empirical investigations. Research has highlighted that variables linked to social disorganization encompass broad socioeconomic/concentrated disadvantage [@sampson2008moving; @samspon1989community; @wilson2012truly; @dao2022crimescape], residential mobility [@lane2004social; @south2000crime; @xie2008escaping], and ethnic heterogeneity [@avison1986population; @blau1977inequality; @south1986structural]. 

The concepts of concentrated disadvantage, residential mobility, and heterogeneity are key ecological factors closely linked to the weakening of informal social control within communities. In areas of concentrated economic deprivation, where many residents face poverty, lack of employment, and limited access to resources, there is often an unstable population structure marked by rapid changes in residency. In the area of high rental turnover, frequent movement of residents in and out within a year hinders the development of stable community ties and collective efficacy—the ability of a neighborhood to regulate the behavior of individuals and groups, enforce norms, and maintain public order through cooperation [@sampson1997neighborhoods]. Consequently, the disruption of local friendships and informal social networks often results in higher crime rates. When more successful residents relocate as soon as better opportunities arise, it leaves behind a population that may include new immigrant groups often facing low wages and significant cultural adaptation challenges. These factors make it difficult for these groups to form a cohesive community identity. The frequent turnover complicates communication and hinders the consensus on community norms. Economic strain and communication barriers further limit residents' opportunities to engage in community activities and maintain strong social ties, which are crucial for promoting shared values and mutual aid [@bursik1988social;@bursik2002neighborhoods;@charis2017new; @kornhauser1978social;@thomas1920disorganization].

Following this trajectory, motor vehicle theft has been closely associated with socioeconomic disadvantage [@walsh2007community; @edmark2005unemployment; @dao2022crimescape; @suresh2013locations], residential mobility, and heterogeneity [@dao2022crimescape; @suresh2013locations; @rice2002socioecological]. @dao2022crimescape utilized 2010 data from Charlotte, NC, employing spatial association rule mining (SARM) to capture the social-spatial factors influencing MVT. They found that the disadvantage index and heterogeneity are positively associated with MVT incidents. Using data from the Indianapolis Metropolitan Police Department in 2013, @piza2018predicting found that factors of social disorganization—such as concentrated disadvantage, heterogeneity, geographic mobility, and population density—were associated with a heightened likelihood of an initiator or a near repeat event of MVT crimes.

@copes1999routine introduced the complexity of MVT as it intersects with social disorganization and routine activity theory. This study analyzed MVT data from a county in Louisiana and found that areas with a higher percentage of economically disadvantaged residents experienced higher MVT rates. Conversely, the percentage of young males—often categorized as more motivated offenders—did not demonstrate a significant correlation with MVT rates. Additionally, while population density was positively related to MVT, factors indicative of potential guardianship, such as the presence of multi-unit buildings, showed no positive correlation with MVT rates. 


<!--
unit of analysis, statistical approach, operational definition of SD. 
For example 
Copes (1999)
 found the percent of young males and the percent of multiple housing-units were significant predictor of MVT. 
Rice and Smith (2002)
 found that the number of African–Americans were significant predictors of MVT. 
Gilliam and Damphousse (2000)
 found residential instability significantly related to MVT.
-->


# Routine Activity and Motor Vehicle Theft
Routine activity theory argues that for a crime to occur, there must be a convergence of suitable targets, motivated offenders, and the absence of a capable guardians  [@bottoms2002environmental; @cohen2010social]. The lack of any one of the three elements can prevent crime. Building upon the foundations of routine activity theory, scholars have emphasized the importance of crime triangle and the "controller" who can disrupt the crime event [@tillyer2011getting; @eck1994drug; @clarke1995situational; @felson2010linking]. This concept includes the manager of the place, the handler of the offender (personnel who have social bonds with the offender and can exert informal social control), and the guardian of the victim/target, collectively known as the double triangle of crime [@eck2003police]. Figure \ref{figure_eck} outlines the visualization of the crime triangles from @eck2003police.


\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{ECK2003.png}
  \caption{Routine Activity's Crime Triangles -- Eck (2003), p.83.}
  \label{figure_eck}
\end{figure}

In MVT, suitable targets may include vehicles parked in commercial areas, public parking lots, near schools or universities, or at residential homes [@piza2018predicting]. The scholars have demonstrated significant relationships between the environmental factors and MVT rates, such as parks, university neighborhoods, hotels, motels, vegetation density, building prevalence, and commercial zones [@bjerregaard2021spatial; @lu2006spatial]. By increasing the difficulty of accessing the target, the suitability of the vehicle for theft may significantly decrease. @webb1994steering utilized data from Germany, Britain, and the United States and found that steering column locks could significantly reduce MVT rates. As to motivated offender, motivations behind MVT encompass a range of factors, including joyriding, short-term transportation needs, retention of the stolen vehicle, financial gain, or use for further criminal activities [@copes2012motor]. To avoid the available guardianship, more than 70% of MVT incidents occur during the nighttime, between 6:00 PM and 6:00 AM [@copes2012motor]. 

Subsequent studies found that the absence of capable guardians, coupled with the presence of motivated offenders, significantly escalated the probability of motor vehicle theft [@hollinger1999motor; @suresh2013locations]. @block2012estimating found that motor vehicle theft was most concentrated along the U.S.-Mexico border, with the highest rates observed in California and Arizona. @block2012characteristics found that SUVs and pick-up trucks, compared to 4-door cars, are more likely to be found in Mexico. Additionally, for every one-year increase in the age of the stolen car, it is 12% less likely to be found in Mexico. @roberts2013explaining used data collected from 310 U.S. cities to test the factors of offender pools, targets, settings, and opportunity structures in the routine activity regarding MVT. They concluded that several factors were associated with motor vehicle theft rates, which included the percentage of young males, proximity to the U.S.-Mexico border, unemployment rates, and the presence of automobile-related businesses, all of which showed a positive correlation; conversely, the families with high disposable income were found to have a negative association with MVT rates.

In the classic routine activity theory, @cohen2010social discussed how changes in the size and price of domestic electronics influenced overall burglary and theft rates in the United States. Since then, studies have continued to explore the relationship between the price/cost of consumer goods and crime. An increase in price indicates a rise in the target's value, affecting the suitability of a target; items with higher prices are often more attractive to potential offenders, making them more susceptible to theft, which in turn leads to a higher number of motivated offenders [@cohen2010social;@lee2018use; @cohen1981social;@felson1980human]. Similarly, if we expect the prices of cars to increase in a short amount of time, we would anticipate a rise in MVT crime.

Despite considerable research exploring various influences on crime rates, the specific link between car prices and MVT has not been thoroughly investigated. This notable gap in the literature is particularly significant considering the potential implications that fluctuations in vehicle prices might have on theft rates. One existing work, a dissertation by @vandaele1975economics, analyzes data from 1933 to 1972, confirming a linear relationship between car prices and MVT. 

Additionally, the empirical study by @draca2019changing explores the broader relationship between price and crime by analyzing how the prices of fuel, metal, and jewelry impact related criminal activities. On the other hand, @wellsmith2005influence examines the "life cycle" hypothesis, suggesting that theft rates for electronic devices remain low during the innovation stage and only increase as prices drop and products become more popular. Their research underscores a significant correlation between the popularity of suitable targets, market returns, and criminal decision-making, highlighting how crime rates are responsive to economic incentives and market pricing. 


# Young Males and Motor Vehicle Theft

Research consistently shows that crime rates often correlate with the percentage of young males, influenced by a variety of social and biological factors. Social disorganization theory identifies young males--often facing significant challenges like unemployment and limited educational opportunities--as "truly disadvantaged," which contributes to higher crime rates [@sampson1987urban; @du1899philadelphia; @wilson2012truly; @allan1989youth; @gould2002crime; @shaw1966jack]. This situation may lead to unsupervised youth groups on the streets, posing potential security issues for communities [@samspon1989community]. Sub-culture theory interprets their behavior as a cultural adaptation where gaining respect could involve engaging in criminal activities [@cohen1955delinquent; @anderson2019code]. From a biological perspective, adolescence is a critical period marked by puberty and hormonal changes that amplify risk-taking behaviors and susceptibility to peer influence [@offer1975teenage; @wilson1985competitiveness; @buchanan1992adolescents; @felson2002pubertal]. Life-course theory observes that criminal activities generally peak during adolescence and decline as individuals age [@farrington1986age; @moffitt2017adolescence]. Meanwhile, routine activity theory suggests that an increase in the population of young males leads to a higher number of potentially motivated offenders [@felson2010linking; @copes1999routine].

The percentage of young males has also been identified as closely associated with the MVT rate [@copes1999routine; @copes2012motor; @rice2002socioecological; @roberts2013explaining]. @piza2017place reveals that the presence of young males is frequently associated with higher MVT rates in neighborhoods containing parks, commercial zones, and areas with high disorder calls or foreclosures. Studies have further linked the age composition of households [@cohen2010social] and age-crime curves [@dixon2020age] to MVT, observing a male dominance in auto theft. This dominance is partly explained by greater barriers faced by women in accessing disposal networks for stolen vehicles, encompassing social, economic, and logistical challenges [@mullins2011establishing; @steffensmeier2015gender]. 

<!--
Research has shown that the theories of the Chicago School and environmental criminology are not exclusive but complementary, interacting to provide a fuller understanding of criminal behavior. Research by  Residential mobility and poverty also show strong associations with MVT, particularly in locales near multi-family housing, hotels, sit-down restaurants, and commercial zones. 

Another example illustrating the complexity of MVT as it intersects with social disorganization and routine activity theory is presented by @copes1999routine. This study analyzed MVT data from a county in Louisiana and found that areas with a higher percentage of economically disadvantaged residents experienced higher MVT rates. Conversely, the percentage of young males—often categorized as more motivated offenders—did not demonstrate a significant correlation with MVT rates. Additionally, while population density was positively related to MVT, factors indicative of potential guardianship, such as the presence of multi-unit buildings, showed no positive correlation with MVT rates.

The convergence of findings across these diverse contexts underscores the need to view environmental criminology and social disorganization as complementary theories. Together these theories provide a comprehensive framework for understanding the spatial dynamics of urban crime. -->


# Temperature, Precipitation and Crime Rates

Various studies have explored the impact of temperature on crime rates. Research has observed that higher temperatures often correlate with increased crime rates [@ranson2014crime; @butke2010analysis; @cohn1990weather; @field1992effect; @shen2020impacts; @hu2017impact; @mares2013climate; @anderson1987temperature; @sommer2018comparing]. @tiihonen2017association linked impulsivity and higher temperatures with increased rates of violent crime through changes in serotonergic transmission, suggesting a biological basis for heat-induced aggression. Notably, @field1992effect also highlighted this trend in both violent and property crimes, suggesting that warmer weather increased outdoor activity, thereby raising the opportunities for crime. Similarly, @wolff2022violence analyzed data from New York City and observed that temperature has a positive association with violence during COVID-19 pandemic. Interestingly, the non-linear relationships between temperature and crime rates also observed by scholars [@schinasi2017time; @gamble13temperature; @stevens2019hot; @heo2024nationwide]. The crimes mostly happen when the weather becomes "comfortable", and the relationship becomes non-significant when the temperature exceeds a certain point. That is, when the weather becomes extremely hot, the number of people outdoors decreases, and consequently, the crime rate also drops. @cohen2024understanding utilized daily weather data from Mexico to examine the relationship between temperature and crime, focusing on the availability of leisure time. Their findings indicate that the correlation between temperature and crime is stronger on weekends, when people generally have more free time. The above findings align with routine activity theory, which posits that more people outside lead to more suitable targets and more motivated offenders. 

Studies conducted in different regions such as South Korea [@heo2024nationwide], the United Kingdom [@field1992effect], Australia [@stevens2019hot], Mexico [@cohen2024understanding], and China [@shen2020impacts; @hu2017impact], as well as the United States [@anderson1987temperature; @schinasi2017time; @mares2013climate; @ranson2014crime; @mares2019climate; @gamble13temperature; @butke2010analysis; @sommer2018comparing; @thomas2023weird], have shown varying degrees of correlation between temperature and crime, supporting the generalizability of these findings across different regions. However, the effect of temperature on MVT is inconsistent; @anderson1987temperature observed that temperature is more connected with violent crime rates than property crime rates, @cohn1990weather and @mares2013climate noted that temperature does not correlate with MVT. From the literature, we understand that the connection between temperature and violent crime is significant. However, this relationship is non-linear, and is less pronounced with property crime and remains inconsistent when it comes to MVT.

Unlike the positive and significant relationship between temperature and crime rates, the connection between precipitation and crime rates is generally negative, though it can sometimes be weak or random [@cohn1990weather; @mares2019climate; @trujillo2021effect]. @iyer2014poverty found that in India, rainfall is negatively correlated with crime rates, likely because increased rainfall contributes to better harvests, thereby reducing poverty and subsequently lowering crime rates. Similarly, @yu2017relationships discovered a negative relationship between typhoons and crime rates, particularly affecting all violent crimes and MVT. @sommer2018comparing also discovered a negative relationship between rainfall and crime rates using daily data from Boston, spanning from 2012 to 2017. Their studies support routine activity theory, which suggests that extreme rainfall reduces the number of available offenders and targets outdoors, subsequently decreasing crime rates. 




# Chapter Summary

From the literature, we understand that the main explanatory variables of MVT are derived primarily from social disorganization theory and routine activity theory. These variables and their expected direction of influence on MVT are: low SES (+), high residential mobility (+), high heterogeneity (+), and percentage of young males (+). Detailed descriptions of these variables' operational definitions, units of analysis, sample sizes, data types, data sources, and statistical models can be found in Table  \ref{tab:tablech4_summary}. However, the findings are mostly limited to cross-sectional data. One study using panel data has a rather limited sample size, with only 252 county-year combinations [@edmark2005unemployment]. Two other studies using spatial-temporal analysis [@piza2018predicting; @bjerregaard2021spatial] are insightful but limited to a single city, which affects its generalizability. 




\clearpage
\newpage
\blandscape
```{r tablech4_summary, results='asis', message=FALSE, warning = FALSE, echo = FALSE}
# Read the xlsx file
mvt_theory_literature <- read_excel("table_of_studies_of_mvt.xlsx")

mvt_theory_literature_t <- knitr::kable(mvt_theory_literature, 
                                        format = "latex", booktabs = T, row.names = F,
                                        linesep = "", longtable = T,
                                        caption = "Motor Vehicle Theft and Criminology Theories Review") %>%
  kable_styling(latex_options = c("scale_down", "repeat_header"), 
                bootstrap_options = "striped",
                font_size = 6,
                full_width = T) %>%
  column_spec(1, width = "1.5cm") %>%
  column_spec(2, width = "0.9cm") %>%
  column_spec(3, width = "4.5cm") %>%
  column_spec(4, width = "1.5cm") %>%
  column_spec(5, width = "0.4cm") %>%
  column_spec(6, width = "1cm") %>%
  column_spec(7, width = "1.75cm") %>%
  column_spec(8, width = "1cm") %>%
  column_spec(9, width = "0.75cm") %>%
  column_spec(10, width = "2cm") %>%
  column_spec(11, width = "3cm") %>%
  footnote(general = c("SES = social economic status; SDT = Social Disorganization Theory; RAT = Routine Activity Theory; OLS = ordinary least squares"), footnote_as_chunk = T, threeparttable = T)



#cat("\\begin{sidewaystable}[!tp]\n")
print(mvt_theory_literature_t)
#cat("\\end{sidewaystable}\n")
```
\elandscape
\clearpage



Given its lower reporting error compared to other crime statistics, official MVT is an ideal candidate for exploring novel crime measurement strategies. The initial step involves using official MVT statistics to test MVT estimates from GT by assessing their linear relationship across various geographic levels. In the second phase, this research plans to test the concurrent validity of GT MVT against official MVT data based on a review of empirical literature. It expects to find a positive and significant correlation between GT MVT and official MVT, alongside variables from social disorganization theory such as concentrated disadvantage, residential mobility, and heterogeneity. From the perspective of routine activity theory, this research anticipates observing a positive correlation of car price and temperature with MVT, and a negative correlation between MVT and precipitation for both official MVT and GT MVT. Furthermore, both GT MVT and official MVT should show a positive correlation with the percentage of young males and the number of vehicles per household. The traditional crime statistics to be compared with GT include UCR, NCVS, NICB, NIBRS, and CFS 911 data. In the upcoming chapter, I will describe the data sources and methodology used to investigate these relationships.


\FloatBarrier

<!-- Chapter 5 -->
\newpage
\fancyhead[L]{Chapter 6: Methodology and Research Design}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}

\chapter{METHODOLOGY AND RESEARCH DESIGN}

# Current Study

In preceding chapters, this research discussed the constraints of traditional crime data, examined the concept of measurement error along with strategies to mitigate it, and explored the realms of big data and digital trace data--assessing their limitations and potential benefits for social science studies. While the findings in @liu2023big demonstrated the promising enhancement of crime statistics through big data, the exploration of their applicability across diverse geographic levels and temporal orders remains unexplored. 

To advance our comprehension of GT data's utility, this study seeks to estimate MVT at varying geographic and temporal levels and investigate its correlations with crime statistics beyond the Uniform Crime Reporting (UCR) data. In pursuit of a more exhaustive exploration of the relationship between GT data and crime statistics, this research leverages MVT from official crime statistics as a valid resource for crime measurement due to its relatively high reporting rate, second only to homicide [@ennis1967criminal; @o1980empirical; @gove1985uniform; @blumstein1991trend; @hart2003reporting; @tarling2010reporting; @cohen1984discrepancies; @weatherburn2011uses; @morgan2021criminal].

According to the author's knowledge, this study is the first to examine the relationship between GT MVT data and official MVT obtained from various sources at geographic levels, including UCR, the National Crime Victimization Survey (NCVS), the National Insurance Crime Bureau (NICB), and Calls for Service 911 (CFS 911) data. This is also the first study to assess the temporal variations of crime estimated from GT across different DMA in the United States. The analysis will encompass both state-level and metro-level (Designated Market Area, DMA) data on an annual basis as well as DMA-level data on a monthly basis. In the following sections, I will describe the research questions, unit of analysis, as well as the data and methods employed in the current study. 



# Research Questions

With the advent of technology and the widespread use of the internet, digital trance data has enabled researchers to explore new data and revisit social phenomena or address old questions when previous data is unavailable. This paradigm shift prompts us to explore the relationship between traditional official sources of crime data and the newer, more dynamic sources of the GT data. In this context, several important research questions arise. These questions involve the comparative reliability and validity of GT data and the alignment of known factors influencing MVT in both official and GT data. By addressing these questions, this research aims to enhance our understanding of how GT data can complement traditional crime statistics and contribute to the field of criminology. The research questions are as follows:

\begin{enumerate}[leftmargin=*, label=\textbf{\arabic*.}, itemindent=0em, labelwidth=\itemindent, labelsep=0em, align=left]  % Customizing the enumerate environment
  \item How does GT data compare with official sources of information in regard to MVT?
    \begin{enumerate}[label=\textbf{\arabic{enumi}.\arabic*.}]
      \item What are their linear relationships across different levels of geography and time?
      \item 1.2 What are their linear relationships after controlling for available yearly variables, such as population, average vehicle per household, and the percentage of internet subscriptions, or monthly variables such as weather (temperature and precipitation) in more conservative statistical models (GLS and fixed-effects model)? 
    \end{enumerate}

  \item Will the known associations of official MVT have the same effect on GT MVT?
    \begin{enumerate}[label=\textbf{\arabic{enumi}.\arabic*.}]
      \item Can the observed association between common covariates and official MVT be observed in the context of GT MVT as well?
      \item Will COVID-19, a natural experiment incident, have the same intervention effect on official MVT and GT MVT?
      \item Can the use of semiconductor prices as an instrumental variable provide insights into whether the monthly changes in vehicle prices have similar effects on both official MVT and GT MVT?
    \end{enumerate}
\end{enumerate}
  




# Data

This study avoids digital trace data from online platforms like Twitter and Reddit for crime analysis. Instead, it compares Google Trends data with official crime rates. This choice is driven by several key factors. Firstly, Google searches typically reflect a person's private desire for information, unlike the public nature of posts on Twitter and Reddit. This distinction minimizes the impact of any underreporting behavior for victims. Additionally, compared with Twitter or Reddit data, Google Trends has significantly wider usage across the population spectrum and provides national coverage for over 87% of internet users who use Google [@pew2012; @statcounter2021], making it more representative for capturing broader crime-related information-seeking patterns. While platforms, such as Twitter and Reddit, typically only track registered users, Google captures anonymous searches, potentially drawing insights from people who wouldn't be found on account-based platforms. This wider net could reveal the intentions, trends, and preferences that users do not wish others to perceive. By centering on Google Trends data, this study aims to exploit its capability to capture anonymous user behaviors regarding searches related to victimization. This method offers distinct advantages in scrutinizing victim-related information and its potential linkage to official crime rates.

## Google Trends Data

The data acquisition process from the Google Trends platform utilized the 'pytrends' package in Python, which functions as an API for automated data retrieval^[The data python and R code used in this dissertation can be found on \url{https://github.com/Yu-Hsuan-Liu}]. To estimate victimization using GT, I need to incorporate words commonly used by victims in their online searches. I initiated the process with a basic term such as “my car was stolen” to identify similar or frequently used keywords entered by victims on GT. The “related queries” section on Google Trends provides the top twenty-five queries associated with the entered keyword. For example, a search for “my car was stolen” yields phrases like “my car was stolen, what do I do?”, “find my car,” “report stolen car,” etc. I then combine these terms using “+” if they are relevant to a victim searching for information online, or exclude them using “-” if they are not. Finally, I used a composite of search terms 'car stolen + find stolen car + report police stolen car + insurance car stolen - dream - check' to pull GT MVT data. This particular set of terms was also employed in @liu2023big. 

Combining "related queries" into a single phrase serves two main purposes. Firstly, it ensures an adequate number of data points for analysis, as a greater number of input keywords in a phrase yields more data points from GT. However, the number of keywords in one phrase is limited to 100 characters, necessitating the selection of the most likely searches by victims, while excluding potentially misleading ones. Secondly, using related queries helps mitigate the potential bias stemming from the researcher's arbitrary selection of words. The composite of search terms was developed in 2019, and the author has been consistently using it since then. Consequently, when the author was developing this combination of search terms, there was no possibility to foresee the MVT data from 2019, 2020, 2021, or 2022, or to correlate the official MVT data for data mining purposes. This approach minimizes the potential for self-selection bias.

Data collection intervals were consistently set between five and ten minutes. For instance, the Google Trends interface restricts data extraction to a single year's estimate per session for all 50 states. Consequently, I systematically gathered data from 2011 to 2019 at five to ten-minute intervals. Once the dataset for the entire 11-year span was compiled, I programmed the API to enter 'sleep' mode, pausing for three hours to wait for a data shift in GT. Although not officially documented by GT, this interval adjustment is based on a rule of thumb from five years of experience with the GT platform and has proven effective in ensuring data reliability. The process for DMA-year level data collection is similar to that for state-year data. I collected data every five to ten minutes to compile the yearly data across approximately 100 DMAs, although the exact number of acquired DMAs varied, sometimes including as few as 85 or as many as 110 DMAs^[All the DMAs are metro areas in the United States, and there are a total of 210 of them. Google Trends returns partial data for each sample, shifting the samples approximately every three hours. For popular terms like “weather,” “NBA,” or “election,” Google Trends usually returns data for all 210 DMAs. However, for the terms used to pull MVT-related search terms, it typically returns data for approximately 100 DMAs. The DMAs included can vary depending on when the data is pulled from Google Trends. For example, pulling data at 0:00 am on 7/21 might include New York, NY, and Los Angeles, CA, but not Las Vegas, NV. Pulling data again at 4:00 am on the same day might include New York, NY, and Las Vegas, NV, but not Los Angeles, CA. By using this multiple sampling method, I can gradually gather data for DMAs not included in previous samples, increasing the number of DMAs and improving data accuracy. If some DMAs still lack data after sampling, I exclude those DMAs. 
]. For monthly data at the DMA level, I conducted the same combined search terms for multiple DMAs from year 2017 to 2022. I wait for five to ten-minute windows for each data search. Again, after compiling complete datasets for all DMAs from 2017 to 2022, the API was set to sleep for a minimum of three hours, providing a necessary break for the GT data to refresh and shift. However, accessing monthly data by DMA in Google Trends requires that the data be pulled individually for each DMA. Unlike the DMA-year GT data, which is aggregated annually across all DMAs, the DMA-month GT data is scaled from 0 to 100 for each DMA individually over all 72 months. 

 

During the data collection period from December 2023 to February 2024, I successfully gathered 42 state-year level samples spanning from 2011 to 2022, along with 36 DMA-year level samples from 2011 to 2022. Furthermore, I collected over 230 data files for DMA-month level analysis. ^[Cincinnati, OH and New Orleans, LA yielded fewer files, with 105 and 102 files respectively. This discrepancy is due to the researcher's inability to start collecting for these two DMAs until the later phase of the study, which limited the time for accumulating data from these locations. The researcher began collecting GT data from Cincinnati and New Orleans later in the research phase because the monthly 911 data for these cities became available online upon rechecking all available 911 monthly data from various cities in the United States. In an effort to include all available cities providing 911 monthly data, the data from these two cities was included in this research. The same query was used to extract data from the Google Trends website approximately every three hours for about a month. The data collection process remains consistent with the original methodology; the primary difference is the sample size. However, having sampled over 100 times from Google Trends, the reliability of this data should be sufficient to counter any potential shifts. Therefore, variations in sample size should not significantly impact the data from Google Trends. The detailed list of DMA-month level file counts is provided in Table \ref{tab:Appendix0}].

# Unit of Analysis

Since the goal of this dissertation is to test and understand the reliability and validity of a new crime estimate derived from digital trace data—Google Trends—it is crucial to explore the full potential of GT data through various units of analysis to produce more robust results. Understanding crime at the sub-national level is also crucial for grasping the "dark figure of crime," providing insights for policymakers, and informing law enforcement about disparities in crime rates and reporting patterns. This nuanced understanding aids in enhancing interventions, procedural measures, victim assistance, and perceptions of safety and police effectiveness, which is why this study applies units of analysis at both the state and metro levels [@bjs2022ncvs_2019]. 

The first level is state-yearly analysis. Contrary to the research which focuses on neighborhoods or street levels—identifying "hot spots" in blocks or stores—the state-level analysis offers a broader view of general crime trends across different years and states. This approach aligns well with macro-to meso-level sociological theories, such as social disorganization theory, and its predictors of crime, such as changes in economic status, racial composition, and residential mobility. Moreover, as previously noted, crime rates are susceptible to issues of underreporting and rarity. The studies apply social disorganization theory in at the state-level is not uncommon. @kawachi1999crime argues that relative deprivation and low social capital are linked to higher violent crime rates at the state level, reflecting broader public health research that associates these social factors with variations in community health and well-being. @fahey2017does analyzed the Global Terrorism Database, which provides country-level data on terrorist incidents, combined with social disorganization metrics based on country's instability from the Political Instability Task Force (PITF). Their research found empirical support for a link between social disorganization and political violence at country level. @fahey2017does noted that although social disorganization is typically applied at the neighborhood level, Durkheim's argument that rapid social change influences crime rates is also applicable on a larger scale. This study, while not as extensive as @fahey2017does at the country level, focuses on the state level. We believe that state-level analysis can provide valuable insights.

Moreover, at the macro-level analysis, when the population included in the estimation is larger, the accuracy of crime rates generally improves [@maltz2002note; @maltz2003measurement; @pridemore2005cautionary;  @rapepolicehidekaplan2021ucrbook; @skogan1981issues; @rand2005bigger]. For example, @porter2012social employs a multi-level analysis using a sample of 2,610 from the Panel Study of Religion and Ethnicity and 2000 US Census data, exploring interactions between neighborhood safety, social disorganization, and county crime rates. While they provided a sample size of 2,610 respondents, the aggregation of these data into census tracts or counties for their HLM model lacks reporting on the number of census tracts or counties used, as well as the number of respondents aggregated per tract or county. This omission raises questions about the validity of the analysis results if the data aggregation did not involve a sufficiently large sample, even the study provided neighborhood level analysis. 

Another consideration for using state-level analysis is that state boundaries inherently determine funding and jurisdictional decisions. Public policy and funding allocations are typically administered by federal agencies to state governments. Moreover, each state have its own jurisdiction and criminal laws. Therefore, a macro-level analysis in the United States is crucial for understanding how Google Trends data align with official crime statistics at the state level.

The second level of unit analysis is the Designated Market Area (DMA) and year^[The data from NICB and NCVS are at the MSA level, which was later cross-referenced into DMA. I will explain this process in detail in a subsequent section.]. DMA provide finer resolution than state-level analysis. To better understand the potential of Google Trends data, it is important to examine how well it aligns with official crime data at the DMA-year level, especially since there are different types of MVT data available at the metro-annual level, such as those from the National Crime Victimization Survey (NCVS) and the National Insurance Crime Bureau (NICB). Assessing how well Google Trends data aligns with such sources as the NCVS and NICB, in addition to the UCR, it is crucial for triangulating the validity and reliability of GT data. Similarly, analysis at the DMA-annual level provides insights into how well Google Trends data corresponds with macro to meso-level theories, such as social disorganization. These variables of social disorganization theory are often used to explain the variation in crime rates across different metro areas.

Criminology studies also applied social disorganization theory at the metro level in numerous studies. @crowley2009social, for example, used Census data in 1990 and 2000 and found that the influx of Latinos into rural towns had minimal negative economic impacts and did not deteriorate the quality of life or increase crime. @osgood2000social used juvenile violence data in 264 non-metropolitan counties across four states. They found support for social disorganization theory with juvenile violence linked to residential instability, family disruption, and ethnic heterogeneity. @lanier2006american investigated the homicide rates among Indigenous Peoples using county-level data, discovering that variables associated with social disorganization theory—such as economic deprivation, ethnic heterogeneity, mobility, and family disruption—could account for the variations in these rates.

Finally, the third level of unit analysis is at the DMA-month level. By breaking down the time unit from annual to monthly, this approach allows for a more nuanced understanding of Google Trends data, enabling observation of its variations before and after the COVID-19 lockdown, its relationship with the car price index, and the seasonal and situational changes in temperature and precipitation. This analysis aligns with routine activity theories, which help explain fluctuations in crime rates due to environmental factors. In their classic work, @cohen2010social presented routine activity theory as a reflection of general trends in everyday human activities and their influence on criminal opportunities. For instance, they used both macro and micro-level studies to demonstrate how human activities can shape opportunities for crime. Notable examples include national data on the amount of time people spend at home and its relationship with potential offenders. Another example is the analysis of the 1975 composition of stolen property reported in the UCR, alongside national data on personal consumer expenditures for goods, to provide evidence of "target suitability." This example illustrates how routine activity theory is applied not only in micro-level analysis but also utilizes macro-level trends to suggest that similar patterns can be observed across different contexts. 
Another macro-level study, @kennedy1990routine, applied routine activity theory at the metro level using data from the Canadian Urban Victimization Survey, focusing on census metropolitan areas (CMAs). Their findings indicate that personal crime is significantly influenced by exposure related to specific lifestyles, particularly among demographic groups such as young males, emphasizing the impact of situational and personality interactions on crime opportunities.



<!--
The absence of sub national estimates for comparing police-known crime with other sources, like the NCVS, underscores a critical gap. While the NCVS serves as a nationally representative survey aiming for capturing the "dark figure of crime", its limited sample size poses challenges for sub national comparisons, even at the state level. This challenge is resulted from expensive and time-consuming for collecting enough samples for local areas to have enough sample to generate precise and reliable estimates [@langton2017second]. For example, samples from New York City cannot represent the whole crime problems experiences across the New York State. Thus, research on the convergence between NCVS and UCR often focuses solely on comparing national crime rate trends over time, neglecting sub national comparisons and insights into how area differences impact crime patterns. Recognizing this limitation, the NCVS implemented a redesigned sampling method in 2013, aiming to bolster samples in specific areas to ensure sufficient coverage for state-level estimates [@langton2017second]. The challenge is not exclusive to the NCVS alone; it extends to rare crime incidents such as homicide, in the UCR data. Researchers have cautioned against the use of homicide statistics at the county level, especially in small counties. This caution arises from the fact that a sudden occurrence of a homicide case can lead to a significant and disproportionately high peak in the crime rate within a small, geographically defined population area [@maltz2002note; @maltz2003measurement; @pridemore2005cautionary].






Nevertheless, when it comes to the monthly data analysis, should the one-month data proves uninformative, I'll broaden the timeframe by aggregating monthly data into two, three, and six-month intervals. This methodology ensures that we pinpoint the most effective timeframe for maximizing the strength of correlation.
-->

## State Level/2011-2022/Annually

In order to develop a general understanding of the correlation between GT MVT and MVT in official crime data, the analysis will be initiated at the state level. The UCR data, sourced from the Federal Bureau of Investigation [@investigation2020crime], was compiled by @kaplan_ucr_2023 at the agency level by year from 1960 to 2023. I aggregated this data from the agency level to states using its unique identification numbers, known as Originating Agency Identifier (ORI) and its linkage to the FIPS code^[@ori_walk_2023 provided linkage of ORI and FIPS; As to the handling of missing values in the official data, please see the description in the section following "missing values."]. The Google Trends data was pulled from Google Trends website by each year across DMA [@gt_website]. The sample size of this data is 600 (50 states multiplied by 12 years). 

In addition to GT and crime data, other variables which have been identified as correlated with MVT from criminological theories are also collected. The incorporation of these additional factors aims to facilitate an examination of the concurrent validity of GT data. In the empirical analysis, social disorganization theory will be integrated, encompassing variables such as concentrated disadvantage (CD), residential mobility, heterogeneity into the statistical tests. These measures have undergone rigorous testing, establishing a robust theoretical connection with MVT [@walsh2007community; @dao2022crimescape; @suresh2013locations]. The concentrated disadvantage (CD) index is created by combining four standardized measures drawn from the ACS [@acs2022] annual data: the percentage of the population aged 16 and older who are unemployed, the percentage of families whose income is below the poverty level, the percentage of single-headed households with children aged under 18 years, and the percentage of the population aged over 25 years with less than high school education. The mobility index is created by standardizing and combining the percentage of people who move within the last year and the proportion renters. To create a heterogeneity index, I calculate the probability that two randomly selected individuals in a state are of different races [@blau1977inequality]. The formula is expressed as $1 - \Sigma_{i = 1}^k P_i^2$, where $k$ represents the total number of ethnic groups, and $P$ denotes the occupancy percentage of each ethnic group within the total population of the state. The percentage of young males, are sourced from @acs2022.

## DMA Level/2011-2015/Annually

At the Metropolitan DMA level, I used the same UCR data source as in the state-year level analysis, aggregating the MVT counts from the county-level FIPS to DMA [@kaplan_ucr_2023]. Google Trends data was similarly obtained from the Google Trends website annually across all DMA. Due to the limitation of GT data's sample size, I was able to pull meaningful data for 113 DMA, resulting in an expected sample size of 1,356 (113 DMA multiplied by 12 years).

The National Crime Victimization Survey data [@ncvs_msa_00_15] was sourced from its sub-national estimates available at ICPSR at MSA levels. I manually aligned MSAs with similarly named DMA^[For the DMA-MSA crosswalk table, please see Table \ref{tab:Appendix01}]. The timeframe of NCVS data is from years 2011 to 2015, which defines the starting year of this analysis period and the extent of publicly available NCVS MSA-level data. Consequently, the sample size from NCVS encompasses 45 DMA over five years, totaling 225 data points. 

MVT data from the National Insurance Crime Bureau (NICB) spans MSAs from 2011 to 2022 [@nicb_2023]. Unlike UCR, NIBRS, or NCVS data, NICB data serves as an entirely independent source of motor vehicle theft information, as all law enforcement agencies report MVT to National Crime Information Center (NCIC). Consequently, NICB data includes MVT incidents that are not reported to UCR or NIBRS^[@nicb_method_2017: "[NICB] Hot Spots data is derived from a mirror image of the vehicle theft file in National Crime Information Center (NCIC). All law enforcement agencies report stolen motor vehicles to NCIC. The UCR is a voluntary reporting program for law enforcement agencies and many of them do not report their crime statistics to the UCR."]. From the NICB dataset, I successfully aligned 102 MSAs with DMA^[Please refer to Table \ref{tab:Appendix01}.]. Over a 12-year period, the MVT data from NICB provided a sample size of 1,224 DMA-years.

Other variables at the DMA-year level, including concentrated disadvantage, residential mobility, heterogeneity index, and the percentage of young males, are sourced from @acs2022. These data are aggregated from the county level to the DMA level. The sample size for the ACS data is aligned with the GT data, amounting to 1,356 (113 DMA multiplied by 12 years). 

## DMA Level/2017-2022/Monthly

At the DMA-month level, GT MVT data was individually retrieved from the Google Trends website for each DMA spanning from 2017 to 2022. Due to limitations in the Google Trends interface, which only provides annual results when querying multiple DMA simultaneously, it was necessary to pull data for each DMA separately to obtain monthly estimates. Consequently, the monthly GT MVT data compilation was achieved by gathering monthly values for each DMA throughout the specified six-year period. In GT MVT at DMA-month level, a score of 100 represents the highest search volume for a month from 2017 to 2022 within a DMA, with other values scaled proportionally relative to this peak over the 72 months^[While at the state-year or DMA-year level, the data of 100 presents the highest search ratio across all states/DMA in that year.].

UCR data is aggregated from @kaplan_ucr_2023, which provides monthly MVT data by agencies. I have aggregated this data from the ORI at the agency level to DMA by month. Similarly, NIBRS monthly MVT data is compiled from @kaplan_nibrs_2023, also from the ORI at the agency level to DMA by month. 

The CFS 911 data is sourced from multiple cities, police departments, and other publicly accessible websites. The cities that provide daily official 911 dispatch statistics on motor vehicle thefts include Austin, Baltimore, Cincinnati, Detroit, Las Vegas, Los Angeles, Nashville, New Orleans, New York City, Phoenix, Portland, San Antonio, San Diego, San Francisco, and Seattle (See Table \ref{tab:Appendix1}). While the majority of cities provide data from 2015 to 2022, certain cities do not provide data until 2016 (Phoenix), 2017 (Denver and Detroit), or 2019 (Austin and Las Vegas). Thus, in order to achieve consistency, the data of CFS 911 data before to 2016 will be excluded. The time frame for the analysis spans from 2017 to 2022. I manually matched these cities with their corresponding DMA areas^[The cities' names and their correspond DMA names can be found in Table \ref{tab:Appendix1}]. 

Although the computer-aided dispatch (CAD) system already applies standards to identify duplicated calls [@cad_identify_duplicated_calls], I want to ensure there are no other duplicated calls in the CFS 911 data. Therefore, I considered calls to be duplicates if they occurred within 1 hour and had the same input (address, place, coordinates) except for time (hour, minute, seconds) and identification numbers (case id). I then removed these duplicated calls from the data^[Current literature on filtering out duplicated calls from 911 can be found in @weisburd2021police, which suggests 1.2 hours for vague addresses and 2.4 hours for GPS coordinates. However, considering that MVT calls are not as urgent as violent crime calls, so duplicated calls may not be a serious problem. Moreover, some agencies do not provide coordinates for addresses (some only provide beats corresponding to similar areas), I set the standard for the duplication time to 1 hour. Thus, calls within the same location/address within 1 hour are considered duplicates was removed. The raw counts of 911 calls and the percentage of duplicated calls by cities are shown in Table \ref{tab:Appendix1}.].

The weather data regarding temperature and precipitation were downloaded from the National Center for Environmental Information website [@GHCNDaily2012]. I aggregated daily data from each weather station based on latitude and longitude and aligned them with FIPS codes. Subsequently, the data was grouped by DMA, where precipitation totals were summed and the average temperature was calculated for each month. 

The car price consumer price index (Car CPI) was obtained from @car_price_bjs_2024 by directly entering the area code of each area into the website's address [@car_price_area_code]. From this data collection method, I was able to collect monthly data on new and used car prices from 17 regions across various metro areas, spanning 72 months, resulting in a sample size of 1,224. These data were then manually aligned with DMA bearing similar names. The names of the area of the Car CPI data and their corresponding aligned DMA are detailed in Table \ref{tab:Appendix1_1}. Additionally, the semiconductor price (Semiconductor PPI) data was sourced from @semiconductor_price_bls_2024. As this data is national for the United States and lacks regional specifics, it was used to represent the general trend in semiconductor prices across the DMA. This national data was applied across the same 17 regions that have data on Car CPI for a period of 72 months as an instrumental variable.




<!--
At city level, the monthly official MVT data will be derived from the downloaded CFS 911 from multiple city hall, police departments, and other publicly accessible websites. The cities providing daily official 911 dispatch statistics on motor vehicle thefts are New York City, San Francisco, Los Angeles, San Diego, Sacramento, Phoenix, Detroit, Las Vegas, and Seattle. The cities providing daily counts of motor vehicle theft are Denver, Philadelphia, Atlanta, Chicago, and Boston (See Table A1). The 911 dispatch data are incident logs for citizen calls, whereas the criminal records are gathered in response to reports or criminal investigations. These official daily crime statistics provide information on the crime date, time, and place, notwithstanding the diversity of their sources. Years from 2015 to 2021 constitute the bulk of the data time frame. Nevertheless, while the majority of cities provide data from 2015 to 2022, certain cities do not provide data until 2016 (Phoenix), 2017 (Denver and Detroit), or 2019 (Las Vegas). Selecting data from large U.S. cities, with a combined population exceeding 25 million (Census Bureau, 2023), enhances the generalizability of our statistical test, providing insights into diverse demographics or the crime patterns. This diverse sample, encompassing a significant portion of the urban population, increases the validity and applicability of our findings to other large city environments. This does not imply that the chosen cities can fully represent all cities in the United States. The explanatory power remains confined, particularly within the larger cities we have examined. In order to achieve consistency with the prevailing practices in urban areas and optimize the quantity of data collected, I have chosen to exclude CFS 911 data before to 2016. My research will focus on the period from 2017 to 2022, as this timeframe encompasses the availability of CFS 911 data for the majority of cities. The monthly estimates at the city level will be obtained by aggregating the daily data
-->


## Other Variables
Since GT (Google Trends) applications require internet access, I included the usage of the internet as a control variable. Higher percentage of internet usage may account for higher usage of Google Search engine, comparing with those states/DMAs with low internet access. To account for the impact of internet usage, this study will incorporate control variables such as internet usage and average vehicle per household [@roberts2013explaining] in both the state-year and DMA-year analyses. Data for internet subscription per household and average vehicle per household can be obtained from the Social Explorer website [@acs2022]. Population data are also sourced from the ACS on the Social Explorer website [@acs2022]. This population data is used to calculate the MVT rate by dividing the count of each official MVT statistic by the population at the state-year, DMA-year, or DMA-month level, and then multiplying by 100,000. Because monthly population data is not available, the monthly population data are interpolated from each year's population data.

Since monthly data for internet subscription and average vehicle per household are not available in the ACS data to serve as control variable, temperature and precipitation data will be used as control variables for the DMA-month analysis. As aforementioned, the weather data is acquired from @GHCNDaily2012. 

## Timeframe of the Data
The timeframe varies according to the available data. For instance, at the state level, UCR MVT data is available from 2005 to 2019. Similarly, NCVS data is available from 2000 to 2015. On the other hand, GT data is available from 2004 to present. To ensure accuracy and accountability, I omit GT data before 2011. This decision aligns with the improved data collection system in GT estimates since 2011 [@trends_help_2023]. Moreover, considering that internet availability exceeded 75% of the US population after 2010 [@pew2019], initiating the data retrieval from GT since 2011 can enhance the reliability of the GT data. To align with the majority of DMA and maximize the number of data points, I exclude CFS 911 data before 2017 and commence our analysis from 2017 to 2022, as this period captures the availability of CFS 911 data for most DMA. To specify, there are primarily three different sets of data, as described below.


## Normalization of Data

To match the data format retrieved from Google Trends, I scaled the official crime statistics to a range from 0 to 100. For the state-year and DMA-year levels, since Google Trends offers data for various DMA each year, I normalized the data such that the highest value for each DMA within a year is set to 100, with all other values adjusted proportionally between 0 and 100. The specifics of the normalization formula can be found in \eqref{myeqn61}:  

\begin{equation}
\text{\scriptsize Normalized Value} = \frac{(\text{\scriptsize Original Value} - \text{\scriptsize Minimum Value across all states or DMA in that year}) \times \text{\scriptsize 100}}{\text{\scriptsize Maximum Value across all states or DMA in that year} - \text{\scriptsize Minimum Value across all states or DMA in that year}} \label{myeqn61}
\end{equation}


For the DMA-month level, the official crime statistics (UCR, CFS 911, NIBRS) are also normalized over this period of 72 months. This normalization process assigns the highest volume across the 72 months of a DMA a value of 100, with all other values adjusted proportionally based on their ratio to this maximum. The normalization formula is in \eqref{myeqn62}:  

\begin{equation}
\text{\scriptsize Normalized Value} = \frac{(\text{\scriptsize Original Value} - \text{\scriptsize Minimum Value across all 72 months in that DMA}) \times \text{\scriptsize 100}}{\text{\scriptsize Maximum Value across all 72 months in that DMA} - \text{\scriptsize Minimum Value across all 72 months in that DMA}} \label{myeqn62}
\end{equation}


\noindent This methodological adjustment in data normalization is tailored to align with the data normalization structure of Google Trends.

## Log-Transformated Variables

As mentioned in the methods section regarding reducing measurement error, one effective strategy is to use log-transformed data for the dependent variable [@pina2022impact; @pina2023exploring]. This approach is particularly suitable for crime statistics, as reporting errors in these data are more aptly modeled by a multiplicative measurement error model than an additive one. Therefore, to conduct a more robust test, I will include results from log-transformed dependent variables of crime statistics in the analysis. To minimize errors associated with each transformation, I avoid combining log-transformed and normalized data (either log then normalize or normalize then log). Consequently, the test results are separately reported in both the normalized format (0-100) and the log-transformed format for the MVT dependent variables. Given that GT data is already normalized, I provided log-transformation results of GT only for consistency and comparison.

## Missing Values

When handling massive datasets, it is inevitable to encounter missing values. I compute missing values in the data by each specific geographic area and apply a linear interpolation method by timeframe (year or month) to estimate missing values based on adjacent data points. If gaps still remain after this process, I utilize back filling and forward filling techniques; back filling replaces missing entries with the subsequent known value moving backward, whereas forward filling employs the previous known value to fill forward^[For interpolation, back filling, and forward filling, I use a function called na.approx in R.]. 

Since the raw UCR data at both the state-year and DMA-year levels are organized by agency, missing values are imputed at the agency-year level. A recommended approach dealing with missing values, as suggested in @ucrbook, is to discard data points if an agency fails to report for more than three months within a year. This strategy could significantly reduce the number of missing or imputed values. Thus, excluding agencies with insufficient reporting is prudent. In this dissertation, agencies with NA values across all 12 years (2011 to 2022) are excluded because imputation is impractical with completely missing data.Additionally, agencies that report data for fewer than 6 years are also dropped because extensive missing data over half of the period can lead to inaccurate imputations. I interpolated missing values within each agency on a yearly basis before grouping the data into states or DMAs. After sorting the data by year and grouping by ORI number, I successfully filled 20,295 missing data points out of 147,498 total data points at the agency level, with the fill rate of 13.76%. The high incidence of missing values at the agency level is due to some local agencies either reporting zero incidents or not reporting at all [@ucrbook]. 

Similarly, at the DMA-month level, the missing value is also imputed at the agency-month level for UCR. The agencies which have all missing values across 72 months (2017-01 to 2022-12) are dropped. Secondly, I also dropped agencies reports less than 36 months because if the missing is massive over the half the period, the imputation may be biased. I interpolated missing values within each agency by year before grouping the data into states/DMA. At the DMA-month level, approximately 74,437 missing values have been filled from a total of 653,946 data points for UCR, resulting in a filling rate of nearly 7.15%. 

The raw NIBRS data is initially at the incident level.  To impute the missing values, I aggregated NIBRS data from incident level to the DMA-month level. Subsequently, DMAs with fewer than 36 months of data were dropped. The remaining NA values were filled using the same methods as for UCR data, which include interpolation, forward filling, and back filling. Out of 12,528 data points in NIBRS, 737 missing values were filled, resulting in a filling rate of 4.92%.

The raw NCVS data, at the MSA level, had 59 missing values, which were filled out of 225 total cases, the filling rate is 26.22%^[While there is no fixed threshold for missing values as it varies depending on the dataset, associated variables, and statistical methods used, it is generally observed that most papers tolerate a range of 5%-50% missing data, sometimes even 10%-50% [@pratama2016review]. The result from the non-missing values in 
NCVS yielded a very small sample size, which may not be statistically meaningful.]. The raw NICB data, also at the MSA level, contained no missing values. Additionally, the CFS 911 data at the DMA-month level, aggregated from daily values, also had no missing values. 

The merging of datasets prioritizes maximizing the sample size of the final dataframe, beginning with GT MVT and UCR MVT data, which have the most substantial sample sizes. I then merge other data aligns with the satisfied data points from GT MVT and UCR MVT. Therefore, in the datasets, GT MVT and UCR MVT have the same sample size. Other variables, if insufficient, are represented as NA values in the data, which were omitted when computing correlation or regression models. 

Zeros in the data were treated as NAs, indicating non-reporting by police departments. This approach ensures a smooth and linear transition between observed values, which is crucial when some agencies report data intermittently or stop reporting suddenly. For example, Los Angeles Police Department reported UCR data until 2020, did not report in 2021, and resumed in 2022. Treating the entire 2021 data for Los Angeles as zero would introduce bias into the dataset. Therefore, I replaced zeros with imputed values. The final imputed data is comparable, particularly from UCR and NIBRS at the DMA-month level. As shown in Figure \ref{fig:figure 7.3}, in many cities, the normalized values of MVT from UCR and NIBRS nearly coincide, suggesting that the imputation methods applied at two different levels—UCR and NIBRS—align well. This alignment enhances the credibility of the imputed data and validates the methods of imputing missing values.

# Methodology

## The Linear Relationship Between Official MVT and GT MVT 

To assess whether a measure serves as a proxy or not, a correlation matrix is often used. Some standards suggest that the correlation for the proxy should be larger than 0.5 [@welch1994identifying]. @carlson2012understanding recommends that convergent validity over 0.7 is desirable and, as low as it is, should not be lower than 0.5. Criminologists have used the correlation coefficient to estimate the reliability and validity of UCR and NCVS by comparing their long-term trends using cross-sectional data or comparing their trends of convergence [@ansari2015convergence; @cohen1984discrepancies; @o1980empirical; @blumstein1991trend]. The overall threshold for claiming a valid or reliable measurement is typically from 0.6 to 0.8, with the highest correlation usually found in motor vehicle theft between UCR and NCVS [@ansari2015convergence; @o1980empirical; @lauritsen2016choice; @berg2016telling; @blumstein1991trend; @ruback2001rural]. Given that crime is a rare incident and susceptible to measurement error, the exploration of convergence between NCVS and UCR typically extends beyond correlation. This often involves multi-year comparisons of crime rate trends, as seen in graphical analyses , and incorporates time-series analysis [@lauritsen2016choice; @ansari2015convergence]. Other methods involving using OLS regression and test the model fits, such as variance inflation factor (VIF) and $R^2$ [@welch1994identifying; @siegel2013new]

Assuming that victims tend to turn to Google for solutions, suggestions, or information after experiencing a MVT incident, it can be inferred that an upsurge in MVT incidents would correspond to an increase in GT MVT searches. This potential correlation presents an interesting opportunity to explore the use of Google Trends data as a complementary source of information for understanding crime patterns and trends. To test their linear relationship, this study will apply correlation and generalized least squares (GLS) regression to estimate the linear association between GT MVT and the official MVT (UCR, NCVS, CFS 911) at state, and DMA levels. 

Autocorrelation occurs when the value of a series at a particular time 
$t$ is dependent on the values at previous times ($t-1, t-2, ...$). In $X_t = \beta X_{t-1} + \varepsilon$, the value at time $t$, $X_t$, is dependent on the value from the previous time period, $X_{t-1}$. Studies have shown that MVT is susceptible to autocorrelation in time-series analysis. @block2013patterns analyzed MVT data from 2007 to 2009 in mid-size cities in the US and found that repeated MVT victimization often occurs days after the initial incidents, demonstrating a spatial-temporal relationship. @piza2018predicting used initial MVT incidents to predict subsequent MVT occurrences, supporting this assertion. Based on their conclusions, I suspect that MVT may exhibit a autocorrelation effect. In time-series analysis, autocorrelation violates the Ordinary Least Squares (OLS) assumption that errors should not be correlated (homoscedasticity). This requires methods such as GLS to address the autocorrelation and heteroscedasticity issue.

Generalized least squares (GLS) is a statistical approach employed when the assumption of homoscedasticity (constant variance) in the errors of the ordinary least squares (OLS) method is violated. This strategy is especially beneficial for addressing heteroscedasticity, which refers to the presence of changing error variance across observations, or when errors are associated across observations. For time-series data, the current error terms are usually correlated with the past error terms, this correlation violates the OLS assumption, and GLS can relax this assumption. In addressing the measurement error highlighted in chapter three, both additive and multiplicative models during our analysis when utilizing official crime statistics as the dependent variable will be employed. The additive form of the GLS regression model is:


\begin{equation}
 \Sigma^{-1/2}{Official}_{MVT} = \Sigma^{-1/2}\beta GT_{MVT} + \Sigma^{-1/2}\varepsilon \label{myeqn13}
\end{equation}

\noindent The multiplicative GLS regression model is:
\begin{equation}
\Sigma^{-1/2}log({Official}_{MVT}) = \Sigma^{-1/2} \beta GT_{MVT} + \Sigma^{-1/2}\varepsilon \label{myeqn14}
\end{equation}

\noindent In both equations, $\Sigma^{-1/2}$ is used to transform the variables and the error term to address issues such as heteroscedasticity or autocorrelation in the error terms, making the errors conform to the assumptions of homoscedasticity and independence^[Mathematically, $\Sigma$ typically represents the covariance matrix of the predictors or errors in the context of regression analysis. $\Sigma^{-1/2}$ is a matrix that, when multiplied by itself, yields the inverse of the covariance matrix, $\Sigma^{-1}$. If $\Sigma$ is the covariance matrix, then $\Sigma^{-1}$ is its matrix inverse, and $\Sigma^{-1/2}$ is a matrix such that $\Sigma^{-1/2} \times \Sigma^{-1/2} = \Sigma^{-1}$.].

Another approach to assess the validity of a new measure involves examining its concurrent validity [@churchill1979paradigm; @bergkvist2007predictive; @strauss2009construct; @o1980empirical]. In criminology, one method for this evaluation is to utilize common crime covariates and observe whether the new measure demonstrates similar magnitudes and direction to the classic crime covariates, as well as the traditional measure. For this analysis, I will employ the concentrated disadvantage index, residential mobility, heterogeneity, and the percentage of young males at both the state and DMA levels as common crime covariates. Additionally, at the DMA-monthly level, weather variables (precipitation and temperature) serve as predictor variables. The objective is to assess whether there are similarities in the OLS model for both official MVT and GT MVT. As previously mentioned in the methods, to mitigate the impact of measurement error in crime statistics, employing log-transformed crime statistics as the dependent variable can significantly mitigate the influence of reporting errors and provide a more reliable estimate of $\beta$. Consequently, I plan to estimate models using both the original data and log-transformed data to observe the difference between GT MVT and official crime statistics. Performing an analysis with both original and log-transformed data allows us to evaluate whether an additive or multiplicative model provides a better prediction and model fit of the outcome variable's variance. The general GLS regression form is:

\begin{equation}
\Sigma^{-1/2}Official_{MVT} = \Sigma^{-1/2}\beta CommonCrime_{covariates} + \Sigma^{-1/2}\varepsilon \label{myeqn15}
\end{equation}
\begin{equation}
\Sigma^{-1/2}GT_{MVT} = \Sigma^{-1/2}\beta CommonCrime_{covariates} + \Sigma^{-1/2}\varepsilon \label{myeqn16}
\end{equation}
\noindent The log-transformed model will also be used:
\begin{equation}
\Sigma^{-1/2}log(Official_{MVT}) = \Sigma^{-1/2}\beta CommonCrime_{covariates} + \Sigma^{-1/2}\varepsilon \label{myeqn17}
\end{equation}
\begin{equation}
\Sigma^{-1/2}log(GT_{MVT}) = \Sigma^{-1/2}\beta CommonCrime_{covariates} + \Sigma^{-1/2}\varepsilon \label{myeqn18}
\end{equation}

## Fixed Effects Models (State and DMA Level, Annual Data)

The fixed effects model will serve as a more conservative approach to assess the linear relationship between GT MVT and official MVT (UCR and NCVS). The fixed effect model is a valuable tool for mitigating potential confounding factors in a linear regression model, especially when dealing with panel data. It focuses on within-unit comparisons, evaluating changes in the outcome and treatment relative to their within-group means. 

This method becomes crucial to eliminate the influence of unobserved, time-invariant confounding factors that may be associated with both GT MVT and official MVT. These factors are not random and can impact the predictor or outcome variable. For instance, economic performance at the state or DMA level may be affected by internal characteristics, such as the political environment or cultural factors. By controlling for these fixed characteristics, the fixed effects model ensures that the effect of the predictors remains unbiased. This is the rationale behind the selection of a fixed effect model over a random effects model.

In criminological studies, the fixed effects model has proven insightful. For example, @rosenfeld2009crime found that communal perceptions of economic conditions significantly influenced an index of acquisitive crime, with an indirect effect on homicide through acquisitive crime. Another study by @aaltonen2016debt using a fixed effect model revealed a dynamic relationship between debt difficulties and criminal behavior. To examine the relationship between GT MVT and UCR, and GT MVT and NCVS, the general form of the fixed effects model in the current study is expressed as follows:
\begin{equation}
(Official_{MVT_{it}} - \widehat{Official_{MVT_{i}}}) = \beta (GT_{MVT_{it}} - \widehat{GT_{MVT_{i}}}) +  (\epsilon_{it} - \widehat{\epsilon_{i}}) \label{myeqn19}
\end{equation}
\noindent Where $i$ represents the unit of observation, such as states or MSAs, and $t$ represents time, indicating different time periods in the data (e.g., year). The estimate of fixed effects model is to assess the $\beta$ by using each data point (i.e., $Official_{MVT_{it}}$ or $GT_{MVT_{it}}$) to subtract the mean value of these data points across time (i.e. $\widehat{Official_{MVT_{i}}}$ or $\widehat{GT_{MVT_{i}}}$). $\widehat{Official_{MVT_{i}}}$ is the mean value of $Official_{MVT_{it}}$. It comes from formula: $\widehat{Official_{MVT_{i}}} = \frac{\sum_{i=1}^T Official_{MVT_{it}}}{T}$ where $T$ is the count number of time units. 

The fixed effect is usually written in a simplified version as illustrated below. Denote $\widetilde{Official_{MVT_{it}}} = Official_{MVT_{it}} - \widehat{Official_{MVT_{i}}}$, where $\widetilde{Official_{MVT_{it}}}$ is the vector of the difference between each data point of $Official_{MVT_{it}}$ minus its mean value $\widehat{Official_{MVT_{i}}}$. Similarly, $\widetilde{GT_{MVT_{it}}} = GT_{MVT_{it}} - \widehat{GT_{MVT_{i}}}$.  Therefore, the above form can be rewritten and simplified as:
\begin{equation}
\widetilde{Official_{MVT_{it}}}  = \beta \widetilde{GT_{MVT_{it}}} + \widetilde{\epsilon_{it}} \label{myeqn20}
\end{equation}
After examining the relationship between GT MVT and official crime statistics (UCR and NCVS), the next step is to measure and compare the concurrent validity of GT MVT and official MVT (UCR and NCVS) on the common crime covariates of motor vehicle theft. The formula of the estimate of official MVT can be written as:
\begin{equation}
\widetilde{Official_{MVT_{it}}}  = \beta \widetilde{CommonCrime_{covariates_{it}}} + \widetilde{\epsilon_{it}} \label{myeqn20_2}
\end{equation}
\noindent As for the estimate of GT MVT, it can be written as:
\begin{equation}
\widetilde{GT_{MVT_{it}}}  = \beta \widetilde{CommonCrime_{covariates_{it}}} + \widetilde{\epsilon_{it}} \label{myeqn20_3}
\end{equation}

To assess a more conservative effect of GT MVT on Official MVT controlling for other variables, we can use the following model:
\begin{equation}
\widetilde{Official_{MVT_{it}}}  = \beta \widetilde{GT_{MVT_{it}}} + \beta_{1}  \widetilde{CommonCrime_{covariates_{it}}} + \widetilde{\epsilon_{it}} \label{myeqn20_4}
\end{equation}

To summarize, this study will employ fixed-effects models in three distinct analyses. Firstly, it will compare GT MVT with Official MVT data at the state and DMA levels, using annual/monthly data to assess their linear association. Secondly, the study will explore the association between GT MVT and common crime covariates, as well as official MVT data with common crime covariates, providing insights into the similarity between GT MVT and traditional MVT in their relationships with other crime predictors. The goal is to compare and contrast the linear relationship between GT MVT and other official MVT statistics, as well as test the concurrent validity of GT MVT with other crime covariates. 

## Instrumental Variables Method (DMA Level, Monthly Data)

This study leverages the instrumental variable (IV) method to investigate whether the established relationship between car price and traditional MVT data persists when using GT MVT as the dependent variable. The objective is to determine if GT MVT exhibits the same sensitivity to car price variations as observed with MVT data derived from other official crime statistics, to test for GT's concurrent validity. To be specific, the IV ($z$) must exhibit a correlation with the independent variable ($x$) and should exclusively exert its influence on the dependent variable ($y$) through the variation of the independent variable. This principle underscores the importance of a valid instrumental variable in ensuring the identification of causal relationships [@bushway2010instrumental]. 



After the global outbreak of COVID-19, a lack of semiconductors led to a decrease in vehicle production, which drove up the price of both used and new vehicles [@krolikowski2021semiconductor; @thorbecke2021semiconductor; @frieske2022semiconductor; @wu2021analysis]. According to @cohen2010social, the incidence of crime is likely to rise when the perceived benefits of engaging in criminal activities increase. Additionally, an escalation in the price of goods has been associated with an increased likelihood of those products being targeted for theft [@sidebottom2014copper]. Hence, with the escalation in vehicle prices, the increase in MVT is expected. Most importantly, the price increase in semiconductors alone is unlikely to directly result in more motor vehicle theft. This relationship, if it exists, would likely be mediated by car price increases. Therefore, the semiconductor price will serve as an instrumental variable, specifically impacting the treatment (automobile price) without influencing the outcome (MVT)^[This choice draws inspiration from @leigh2004instrumental's study, which used cigarette prices as an IV to explore the smoking-health connection.]. When using GT MVT as the outcome variable, if the research design demonstrates that the rise in automobile prices correlates with an increase in GT MVT, it strengthens the credibility of using Google search data to approximate the prevalence of MVT. The formation of IV method is the same as \eqref{myeqn6} and \eqref{myeqn7}. They can be rewritten into^[Further solidifying this choice, examining the key IV assumptions reveals: 1) strong instrument-car price relationship (Assumption A6, $\sigma_{z, x^*} \neq 0$): the connection between semiconductor and car prices is non-zero, meaning the instrument does indeed influence the variable of interest; 2) no direct impact on MVT (Assumption A7, $\sigma_{z, \mu} = \sigma_{z, \nu} = 0$): existing evidence suggests no association between semiconductor prices and MVT rates or their error terms, ensuring the instrument's exogeneity; 3) independent of confounders (Assumption A8, $\sigma_{z, \varepsilon} = 0$): semiconductor prices aren't tied to the error term in the OLS estimation of car prices on MVT, minimizing potential biases; 4) natural event: the change in semiconductor prices is not artificially manipulated, arising from natural market forces, further strengthening its suitability as a valid IV.]:

\begin{equation}
\widehat{Automobile_{Price}} = \iota + \gamma Semiconductor_{Price} + \delta \label{myeqn21}
\end{equation}

\begin{equation}
Official_{MVT} = \alpha  + \beta \widehat{Automobile_{Price}} + \varepsilon \label{myeqn22}
\end{equation}

\begin{equation}
GT_{MVT} = \alpha  + \beta \widehat{Automobile_{Price}} + \varepsilon \label{myeqn23}
\end{equation}

\noindent In \ref{myeqn21}, $\iota$ is the constant, and $\delta$ serve as the error term in the estimate of $Semiconductor_{Price}$ on $\widehat{Automobile_{Price}}$. Similarly, in \ref{myeqn22} and \ref{myeqn23}, $\alpha$ is the constant of the estimate of $\widehat{Automobile_{Price}}$ on $Official_{MVT}$ or on $GT_{MVT}$, and $\varepsilon$ is the error term of $\widehat{Automobile_{Price}}$ on $Official_{MVT}$ or on $GT_{MVT}$. 

The IV method, as highlighted in Chapter 3, effectively reduces measurement error in the independent variable by utilizing an exogenous source of variation that influences the treatment but remains independent of the outcome. To explore the relationship between GT MVT data and official MVT statistics, I also use GT MVT as the dependent variable and official MVT as the independent variable, employing car price as the instrumental variable. This strategy assumes that official MVT data accurately represent actual MVT incidents and is the proxy of the actual MVT incidents. The hypothesis tests whether, following an MVT incident, individuals are likely to search online for related information. While an increase in car prices is expected to lead to a rise in MVT incidents, such price hikes alone are unlikely to influence the volume of related MVT online searches. Instead, it is the occurrence of an MVT that prompts victims to search for related information online. This IV method is outlined following the format established in equations \eqref{myeqn6} and \eqref{myeqn7}:

\begin{equation}
\widehat{Official_{MVT}} = \iota + \gamma Automobile_{Price} + \delta \label{myeqn211}
\end{equation}

\begin{equation}
GT_{MVT} = \alpha + \beta \widehat{Official_{MVT}} + \varepsilon \label{myeqn222}
\end{equation}

\noindent In equation \eqref{myeqn211}, $\iota$ represents the intercept, and $\delta$ serves as the error term in the prediction of $\widehat{Official_{MVT}}$ based on $Automobile_{Price}$. Similarly, in equations \eqref{myeqn222}, $\alpha$ denotes the intercept, and $\beta$ reflects the influence of $\widehat{Official_{MVT}}$ on $GT_{MVT}$, with $\varepsilon$ as the corresponding error term for these models.

## COVID-19 Lockdown and MVT – Natural Experiments & Interrupted Time Series (DMA Level, Monthly Data)

In a natural experiment, the treatment is randomized, but the randomization is not under the researcher's control. It utilizes an exogenous (outside of the system) event to measure the impact of an endogenous (inside the system) occurrence. Thus, the data collected before the incident have been considered a control group, whereas the data collected after the incident is a treatment group. Interrupted Time Series (ITS) is a straightforward construction often utilized in natural experimentation. When a sudden event occurs, ITS designs express the fundamental notion that I can compare results before and after the occurrence. For example, @penney2016chilling used natural experiment and ITS to explain the change in readers' behavior on Wikipedia websites after Snowden's revelation that the NSA is monitoring the network. This method is also used by @legewie2013terrorist to measure the change in the perception of immigrants before and after a specific terrorist occurrence. 

@clarke2010deterrence utilizes the natural experiment of Edmonton's redeploying of security staff contrasted with the subsequent increase in fines to compare the effectiveness of reduced checks versus higher fines as fare evasion deterrents. Their findings highlight the need for further cost-benefit analysis of such strategies, as the uncertain impact of increased fines on evasion rates underscores the importance of exploring alternative approaches. Other criminological studies have employed natural experiments, such as utilizing the COVID-19 lockdown as a case study. 


The COVID-19 pandemic significantly impacted crime rates across various categories. Research utilizing different datasets such as UCR, CFS, and Google search data has highlighted nuanced trends in criminal activities during this period. Notably, there was a surge in domestic violence incidents during the COVID-19 lockdown with the increases in 911 calls and Google searches in multiple countries [@anderberg2022quantifying; @berniell2021covid]. Additionally, hate crimes, particularly against Asian communities, escalated during COVID-19 [@gover2020anti]. Conversely, other types of crimes, including misdemeanors and assaults, generally declined [@bullinger2021covid]. Studies also noted that increases in COVID-19 cases are associated with decreases in gunshot and assault incidents in New York City [@wolff2022violence]. 


Several investigations have indicated that the COVID-19 lockdown is correlated with a divergence effect in motor vehicle thefts in U.S. cities [@ashby2020initial]. However, a sudden decrease of motor vehicle theft after the COVID-19 lockdown is observed in Mexico and London [@halford2020crime; @balmori2021u]. Therefore, the COVID-19 lockdown is an excellent natural incident for researchers to compare GT MVT to official resources. Theoretically, people have more opportunities to conduct Internet searches at home during the lockdown. Hence, it is more likely to cause an upward bias in GT MVT. If GT MVT displays the same sharp decline in crime as official data, this demonstrates the reliability of GT MVT. The statistical model can be written as:

\begin{equation}
Official_{MVT} = \alpha  + \beta_1 T + \beta_2 D + \beta_3 P +\varepsilon \label{myeqn24}
\end{equation}
\begin{equation}
GT_{MVT} = \alpha  + \beta_1 T + \beta_2 D + \beta_3 P +\varepsilon \label{myeqn25}
\end{equation}

The variable $T$ serves as a continuous measure denoting the passage of time, expressed in months, starting from the initiation of the observational period. This temporal progression is crucial for capturing any underlying trends or patterns over the course of the study. On the other hand, the binary variable $D$ assumes the role of indicating whether the data was aggregated before (designated by 0) or after (designated by 1) the implementation of COVID-19 lockdown measures. This binary distinction is useful in understanding and analyzing potential shifts or disruptions in the data that can be attributed to the impact of the pandemic and associated lockdown. Additionally, the continuous variable $P$ represents the temporal interval, in months, since the start of the COVID-19 lockdown. It offers a nuanced perspective by quantifying the duration from the initial implementation of lockdown measures. For instance, in Table \ref{tab:table5}, where the intervention (COVID-19 lockdown) commenced in April 2020, the value of $P$ dynamically adjusts in accordance with changes in $D$. This systematic approach enables a comprehensive exploration of how the temporal proximity to lockdown initiation correlates with the observed outcomes, providing valuable insights into the evolving dynamics during and after the implementation of COVID-19 measures.



```{r table5, results='asis', message=FALSE, warning = FALSE, echo = FALSE}
ITS_table_example <- data.frame(
  Time = seq(as.Date("2019-12-01"), as.Date("2020-09-01"), by = "month"),
  T = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
  D = c(0, 0, 0, 0, 1, 1, 1, 1, 1, 1),
  P = c(0, 0, 0, 0, 1, 2, 3, 4, 5, 6),
  GT.MVT = c(23, 32, 64, 55, 15, 23, 34, 67, 55, 89)
)


ITS_table_example <- knitr::kable(ITS_table_example, 
             align = "c",
             format = "latex", 
             booktabs = T, 
             row.names = F, 
             linesep = "", 
             longtable = F,
             caption = "An Example of Interrupted Time Series Data", 
             escape = F,  
             col.names = linebreak(
               c("Time","Months\n(T)", "COVID-19 Lockdown\n(D)", "Months After Treatment\n(P)", "GT MVT\n(Y)"), align = "c")) %>%
  kable_styling(latex_options = "hold_position", font_size = 12) #%>% 
  #column_spec(column = 1, width = "10em") %>%
  #footnote(number = c(""), footnote_as_chunk = T, threeparttable = T)

print(ITS_table_example, type="latex", table.placement = "!htb", scalebox = 0.8)
```

\FloatBarrier


# Chapter Summary

This chapter introduces the data and methodologies used in this study, which aim to explore the benefits and limitations of incorporating Google Trends data into dissertation. Employing rigorous methodologies that account for measurement errors and autocorrelation in time series data, I seek to provide a robust evaluation of the effectiveness of GT MVT data. The methods emphasize addressing measurement error and enhancing causal inference to deliver various estimates of GT and robust results for triangulation. This approach aims to enhance our understanding of crime statistics and its potential as a valuable tool for estimating victimization.

\FloatBarrier

<!-- Chapter 7 -->
\newpage
\fancyhead[L]{Chapter 7: Results}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}

\chapter{RESULTS}

# Introduction

This chapter presents the results of the analysis exploring the linear relationship between Google Trends data on motor vehicle theft and official crime statistics in the United States, covering various geographic scales and time periods. The statistical analysis, guided by research questions, investigates these relationships across different regions and over time, incorporating control variables to deepen the understanding. Additionally, I evaluate the concurrent validity of Google Trends data by conducting tests with common crime covariates to examine the relationships between official MVT data and corresponding covariates with Google Trends MVT. Moreover, the study employs the instrumental variable method to account for exogenous factors of car price on GT MVT and other official MVT estimates. The other set of instrumental variable test also controls for the exogenous factors of Official Crime Statistics on GT MVT. Furthermore, I use the starting of the COVID-19 lockdown (April, 2022) as a natural experiment to implement an interrupted time series analysis to examine changes in official crime statistics before and after COVID-19 to investigates whether similar changes are observable in Google Trends MVT. The statistical tests begin with state-level annual assessments, advance to DMA-level annual tests, and conclude with DMA-level monthly evaluations.

The methodology includes correlation analysis, Generalized Least Square (GLS) regression^[Generalized least squares (GLS) is an extension of Ordinary Least Squares (OLS) regression. It is specifically designed to address scenarios where the residuals are not independent, thus violating a fundamental assumption of OLS regression that the error terms must be uncorrelated. GLS relaxes this assumption and is frequently utilized in time-series analysis to accommodate data points that exhibit non-stationary, including those with seasonal or trending characteristics. I use nlme package and function gls, the parameters "correlation" was set to "correlation = corARMA(p = 1, q = 1, form = ~ time | geolocation)"], and fixed-effect models^[Due to the panel data format and its inherent time-series characteristics, issues such as autocorrelation and heteroskedasticity are inevitable. To address these issues, I apply robust error terms in all the fixed effect models, which serve as a more conservative method to confirm the significant relationships between the variables. The computation of robust standard errors was conducted using the coeftest package, with the method set to "arellano". This approach effectively accounts for heteroskedasticity and autoregression in time series data.] for both the yearly and monthly data in states and DMA. For the monthly data at the DMA level, the study extends to include interrupted time series ^[For the estimation of interrupted time series, I utilized the gls package in R, which accounts for autoregression and incorporates a moving average for more robust estimations. The 'method' parameter was set to "ML".] analyses, and instrumental variable tests^[The estimation of the instrumental variable models was conducted using the "ivreg" package in R. The robust error was using the "coeftest" package, with the method set to "HC0".]

This dissertation seeks to answer the following key research questions:

\begin{enumerate}[leftmargin=*, label=\textbf{\arabic*.}, itemindent=0em, labelwidth=\itemindent, labelsep=0em, align=left]  % Customizing the enumerate environment
  \item How does GT data compare with official sources of information in regards to MVT?
    \begin{enumerate}[label=\textbf{\arabic{enumi}.\arabic*.}]
      \item What are their linear relationships across different levels of geography and time?
      \item What are their linear relationships after controlling for available monthly variables such as weather (temperature and precipitation), or available yearly variables, such as disadvantage index, residential mobility, heterogeneity, percentage of young males, and the automobiles owned per household in a more conservative statistical model (fixed-effects model)?
    \end{enumerate}

  \item Will the known associations of official MVT have the same effect on GT MVT?
    \begin{enumerate}[label=\textbf{\arabic{enumi}.\arabic*.}]
      \item Can the observed association between common covariates and official MVT be observed in the context of GT MVT as well?
      \item Will COVID-19, a natural experiment incident, have the same intervention effect on official MVT and GT MVT?
      \item Can the use of semiconductor prices as an instrumental variable provide insights into whether the monthly changes in vehicle prices have similar effects on both official MVT and GT MVT?
    \end{enumerate}
\end{enumerate}

These questions are crucial for validating the effectiveness of Google Trends data in criminological research and for exploring its potential as a complementary source to traditional crime data.




```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
# Get the ACS data
acs_dma <- read.csv("C:/Users/tosea/Liu_Dissertation_GT_Code/acs_county_2011_2022.csv")
#acs_msa <- read.csv("C:/Users/tosea/Liu_Dissertation_GT_Code/acs_msa_2011_2022.csv")
acs_state <- read.csv("C:/Users/tosea/Liu_Dissertation_GT_Code/acs_state_2011_2022.csv")
```




```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
acs_dma_monthly <- acs_dma %>%
  mutate(Date = as.Date(paste0(Year, "-12-01"))) %>%
  dplyr::select(-Year) %>%
  complete(Date = seq(min(Date), max(Date), by = "1 month"), nesting(DMA)) %>%
  arrange(DMA, Date)

# Interpolating numeric columns, this will be done for each DMA separately if needed
acs_dma_monthly <- acs_dma_monthly %>%
  group_by(DMA) %>%
  mutate(across(where(is.numeric), ~ {
    # Approximate NAs first
    . <- na.approx(., na.rm = F, rule = 2)
    # Forward fill (carries the last observation forward to replace NA)
    . <- na.locf(., fromLast = TRUE, na.rm = F)
    # Backward fill (carries the next observation backward to replace NA)
    . <- na.locf(., na.rm = F)
    # Round the final values to 5 decimal places
    round(., 5)
  })) %>%
  ungroup()

# Sort the data by DMA and Date (or Year if you have that column)
acs_dma_monthly <- acs_dma_monthly %>%
  arrange(DMA, Date) %>%
  mutate(year_month = format(Date, "%Y-%m")) %>%
  dplyr::select(-Date)

write.csv(acs_dma_monthly, "acs_dma_monthly.csv", row.names = FALSE)

```


```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
# Google Trends State Annually
gt_state_annual <- read.csv("C:/Users/tosea/Liu_Dissertation_GT_Code/gt_state_mvt_2011_2022.csv")

# Google Trends DMA Annually
gt_dma_annual <- read.csv("C:/Users/tosea/Liu_Dissertation_GT_Code/gt_dma_mvt_annually_2011_2022.csv")

# Google Trends DMA Monthly
gt_dma_month <- read.csv("C:/Users/tosea/Liu_Dissertation_GT_Code/gt_dma_mvt_monthly_2017_2022.csv")


```




```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
# North Dakota, GT has no data in 2011 and 2013
# Fill by interpolation 

#gt_state_annual %>% group_by(State) %>%  filter(length(Year) < 12)

nd_missed_data <- data.frame(
  Year = c(2011, 2013),
  State = c('North Dakota', 'North Dakota')
)

nd_missed_data$Year <- as.integer(nd_missed_data$Year)

# Assuming 'gt_state_annual' is already loaded and has 'Year' and 'State' columns in the proper format
# Merge the data frames
gt_state_annual <- merge(nd_missed_data, gt_state_annual, by = c("State", "Year"), all = TRUE)

nd_data <- gt_state_annual %>% 
  filter(State == 'North Dakota')

nd_data$GT.MVT <- na.approx(nd_data$GT.MVT, na.rm = FALSE) 
nd_data$GT.MVT <- na.locf(nd_data$GT.MVT, na.rm = FALSE)
nd_data$GT.MVT <- na.locf(nd_data$GT.MVT, na.rm = FALSE, fromLast = TRUE)

#get filled data into full dataset
gt_state_annual_filtered <- gt_state_annual %>% 
  filter(State != 'North Dakota')

# Merge the updated North Dakota data back into the main data frame
gt_state_annual_updated <- rbind(gt_state_annual_filtered, nd_data)

# Optionally, you might want to reorder the data frame based on Year or State
gt_state_annual <- gt_state_annual_updated %>% 
  arrange(State, Year)

```



```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
normalize_data <- function(data, group_col, time_col, value_col) {
  summarised_data <- data %>%
    group_by(!!sym(group_col), !!sym(time_col)) %>%
    mutate(!!sym(value_col) := na.approx(!!sym(value_col), na.rm = FALSE)) %>%
    summarise(!!sym(value_col) := mean(!!sym(value_col), na.rm = TRUE), .groups = 'drop')


  # Normalize the numeric columns from 0 to 100
 normalized_date <- summarised_data %>%
    group_by(!!sym(group_col)) %>%
    mutate(!!sym(value_col) := ( !!sym(value_col) - min(!!sym(value_col)) ) * 100 / (max(!!sym(value_col)) - min(!!sym(value_col)))) %>%

  return(normalized_date)
}


gt_dma_month <- normalize_data(gt_dma_month, "DMA", "year_month", "GT.MVT")

```




```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
# Official/Traditional Crime Data

#state annually
ucr_state <- read.csv("D:/0dissertation_code_data/state_level_motor_vehicle_theft_drop_dp_with_over_six_na_then_fill.csv")

# dma annually
ucr_dma <- read.csv("D:/0dissertation_code_data/dma_level_motor_vehicle_theft_drop_dp_with_over_six_na_then_fill.csv")
ncvs_dma <- read.csv("D:/0dissertation_code_data/msa_ncvs_mvt.csv")
nicb_dma <- read.csv("D:/0dissertation_code_data/nicb_dma_year.csv")


#monthly data
nibrs_dma_month <- read.csv("D:/0dissertation_code_data/nibrs_dma_month.csv")
ucr_dma_month <- read.csv("D:/0dissertation_code_data/ucr_dma_month_dropped_na_over_36_months.csv")
cfs_911_dma_month <- read.csv("D:/0dissertation_code_data/dma_911_mvt.csv")
```



```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
ucr_dma_month_fl <- ucr_dma_month %>%
  filter(str_detect(DMA, "FL")) 

ucr_dma_month_fl$year_month <- as.Date(paste0(ucr_dma_month_fl$year_month, "-01"))


florida_missing_values <- ggplot(data = ucr_dma_month_fl, aes(x = year_month, y = UCR.MVT, group = DMA, color = DMA)) +
  geom_line() +  # This adds line connections between points
  geom_point() +  # This adds points at each data entry
  labs(title = "UCR MVT in Florida",
       x = "Year-Month",
       y = "UCR MVT Value",
       color = "DMA") +
  theme_minimal() +  # A clean theme for the plot
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  

ggsave("UCR_MVT_florida_missing.png", plot = florida_missing_values, width = 10, height = 6, dpi = 300)

```


```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
# Drop UCR Florida data as it contains too many missing values
#ucr_dma_month <- ucr_dma_month %>%
  #filter(!str_detect(DMA, "FL")) 

ncvs_dma_missing <- ncvs_dma %>%
  mutate(NCVS.MVT = ifelse(NCVS.MVT == 0, NA, NCVS.MVT))

ncvs_na_count <- sum(is.na(ncvs_dma_missing$NCVS.MVT))

# Deal with missing values in NCVS by interporation
ncvs_dma <- ncvs_dma %>%
  mutate(NCVS.MVT = ifelse(NCVS.MVT == 0, NA, NCVS.MVT)) %>%
  group_by(DMA) %>%
  mutate(NCVS.MVT = na.approx(NCVS.MVT, na.rm = FALSE, rule = 2)) %>%
  filter(!any(is.na(NCVS.MVT))) %>%
  ungroup()
```




```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
# Subset acs_dma_monthly to keep only the necessary columns before merging
acs_dma_monthly_subset <- acs_dma_monthly[, c("DMA", "year_month", "Population")]
weather_month <- read.csv("D:/0dissertation_code_data/dma_month_weather.csv")

#From 10 Celsius to Celsius TAVG is 10th of degrees C, so we need to divide it by 10 before converting it into degrees F. 
weather_month$TAVG <- (weather_month$TAVG/10 * 9/5) + 32
#get from mm to inch
weather_month$PRCP <- weather_month$PRCP/25.4
#read CPI PPI
cpi_ppi <- read.csv("C:/Users/tosea/Liu_Dissertation_GT_Code/new_and_used_car_cpi_semiconductor_ppi.csv")
```




```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
fill_missing_dates_and_values <- function(data, value_col, acs_dma_monthly_subset) {
  start_date = as.Date("2017-01-01")
  end_date = as.Date("2022-12-01")
  data <- merge(data, acs_dma_monthly_subset, by = c("DMA", "year_month"), all.x = TRUE)
  # Ensure the date column is in the correct Date format
  data$year_month <- as.Date(paste0(data$year_month, "-01"))
  
  # Now merge x with the subset of acs_dma_monthly
  data <- data %>% arrange(DMA, year_month)
  
  # Complete data with missing year-months and interpolate missing values

  completed_data <- data %>%
    group_by(DMA) %>%
    complete(year_month = seq(start_date, end_date, by = "month")) %>%
    #fill na and calculate crime rate by dividing by the population * 100000
    mutate({{value_col}} := na.approx(!!sym(value_col), na.rm = FALSE, rule = 2)) %>%
    mutate({{value_col}} := na.locf(!!sym(value_col), na.rm = FALSE)) %>%
    mutate({{value_col}} := na.locf(!!sym(value_col), na.rm = FALSE, fromLast = TRUE))%>%
    mutate({{value_col}} := round(!!sym(value_col)/Population*100000, 3)) %>%
    #normalize from 0 to 100
    #mutate(across(where(is.numeric), ~ (.- min(.)) * 100 / (max(.) - min(.)))) %>%
    ungroup()
  completed_data <- completed_data %>%  filter(!is.na(year_month))
  completed_data <- completed_data %>%  filter(!is.na(DMA) & DMA != "")
  return(completed_data %>%  dplyr::select(-Population))
}

# Fill NA
cfs_911_filled <- fill_missing_dates_and_values(cfs_911_dma_month, "CFS.911.MVT", acs_dma_monthly_subset)
nibrs_dma_month_filled <- fill_missing_dates_and_values(nibrs_dma_month, "NIBRS.MVT", acs_dma_monthly_subset)
ucr_dma_month_filled <- fill_missing_dates_and_values(ucr_dma_month, "UCR.MVT", acs_dma_monthly_subset)
```



```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
################################################
###########Deal with NAs in NIBRS###############
################################################

test_for_nibrs <- nibrs_dma_month_filled  %>%
  group_by(DMA) %>%
  # Add a new temporary column to count NAs per group
  mutate(nibrs_na_count = sum(is.na(NIBRS.MVT))) %>%
  ungroup()

#range(test_for_nibrs$nibrs_na_count)


filtered_test_for_nibrs <- test_for_nibrs %>% 
  filter(nibrs_na_count <= 36) %>%
  dplyr::select(-nibrs_na_count)

```


```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
#fill nibrs missing
filtered_test_for_nibrs_filled <- filtered_test_for_nibrs %>%
  arrange(DMA, year_month) %>%
  group_by(DMA) %>%
  # Ensure that there is at least one non-NA value before applying na.approx
  mutate(NIBRS.MVT = ifelse(is.infinite(NIBRS.MVT), NA, NIBRS.MVT)) %>%
  mutate(NIBRS.MVT = if (all(is.na(NIBRS.MVT))) NA else na.approx(NIBRS.MVT, na.rm = FALSE, rule = 2)) %>%
  mutate(NIBRS.MVT = na.locf(NIBRS.MVT, na.rm = F)) %>%
  mutate(NIBRS.MVT = na.locf(NIBRS.MVT, fromLast = T, na.rm = F)) %>%
  ungroup()

filtered_test_for_nibrs_filled %>% filter(is.na(NIBRS.MVT))
```


```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
#sum(is.na(filtered_test_for_nibrs$NIBRS.MVT))
#filled 737 missing values
#nrow(filtered_test_for_nibrs)
#total rows 12528
#sum(is.na(filtered_test_for_nibrs$NIBRS.MVT))/nrow(test_for_nibrs)
# filling rate is 4.92%


nibrs_dma_month_filled <- filtered_test_for_nibrs_filled
#######################################################
#######################################################
#######################################################
```




```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
# Fill Cpi PPi lost months
cpi_ppi$year_month <- as.Date(cpi_ppi$year_month)
cpi_ppi <- cpi_ppi[order(cpi_ppi$DMA, cpi_ppi$year_month), ]
cpi_ppi2 <- cpi_ppi %>%
  group_by(DMA) %>%
  mutate(Car.CPI = round(na.locf(na.locf(na.approx(Car.CPI, na.rm = FALSE, rule = 2), fromLast = TRUE), na.rm = FALSE), 3)) %>%
  ungroup()




gt_dma_month$year_month <- as.Date(paste0(gt_dma_month$year_month, "-01"))
acs_dma_monthly$year_month <- as.Date(paste0(acs_dma_monthly$year_month, "-01"))




#MERGE
year_month_merged_data <- merge(gt_dma_month, ucr_dma_month_filled, by = c("DMA", "year_month")) %>%
  merge(cfs_911_filled, by = c("DMA", "year_month"), all.x = TRUE) %>% 
  merge(nibrs_dma_month_filled, by = c("DMA", "year_month"), all.x = TRUE) %>% 
  merge(acs_dma_monthly, by = c("DMA", "year_month"), all.x = TRUE) %>%
  merge(weather_month , by = c("DMA", "year_month"), all.x = TRUE) %>%
  merge(cpi_ppi2, by = c("DMA", "year_month"), all.x = TRUE) 
  #drop if na in UCR (THE BASE LINE IS GT AND UCR, IF NON DATA IN UCR, NO NEED TO COMPARE)
  #filter(!is.na(UCR.MVT))
```



```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
#normalize MVT data
year_month_merged_data <- year_month_merged_data %>%
  # Group the data by DMA
  group_by(DMA) %>%
  # Apply the normalization only to numeric columns containing 'MVT' in their names, within each group
  mutate(across(
    .cols = contains("MVT") & where(is.numeric),
    .fns = ~ if_else(is.na(.), 
                     NA_real_, 
                     (. - min(., na.rm = TRUE)) * 100 / (max(., na.rm = TRUE) - min(., na.rm = TRUE))),
    .names = "{.col}_normalized"
  )) %>%
  mutate(across(.cols = contains("MVT") & where(is.numeric) &!contains("_normalized"),
                .fns = ~ (log(.+1)),
                .names = "log.{.col}")) %>%
  # Ungroup the data to prevent grouping from affecting further operations
  ungroup()
```



```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
#get the origianl 0 back into the normalized value original 0 become NA before normalization, so add the original 0 back
year_month_merged_data <- year_month_merged_data %>%
  mutate(NIBRS.MVT_normalized = if_else(is.na(NIBRS.MVT_normalized) & !is.na(NIBRS.MVT), 0, NIBRS.MVT_normalized))

year_month_merged_data <- as.data.frame(year_month_merged_data)
```



```{r, results='hide', message=FALSE, warning = FALSE, echo = FALSE}
nibrs_dma_month$year_month <- as.Date(paste0(nibrs_dma_month$year_month, "-01"))

#To retrieve the count of NA values specifically for the NIBRS data, given that no NA filling for UCR and CFS 911 data (as verified by checks showing no difference before and after filling)

#nibrs_na_count <- sum(is.na(merge(year_month_merged_data, nibrs_dma_month, by = c("DMA", "year_month"), all.x = TRUE) %>% dplyr::select(NIBRS.MVT.y)))


```





\FloatBarrier
# State-Year Analysis

Table \ref{tab:desc_state} presents descriptive statistics for various state-year level variables from 2011 to 2022. It includes data on motor vehicle theft from Google Trends and Uniform Crime Report, alongside other common crime covariates, they are concentrated disadvantage index, mobility index, and heterogeneity index, percentage of foreign born, and percentage of young males. The control variables are population(logged), average vehicle per household, and the percentage of internet subscription per household. 

In Table \ref{tab:desc_state}, GT MVT is normalized from 0 to 100 by each year across all states in that year. The average GT MVT stands at 60.79, with a standard deviation of 16.00. GT MVT (log) is logarithmically transformed GT MVT. As previously mentioned, using log-transformed crime statistics as the dependent variable can substantially reduce the impact of reporting errors in crime statistics, providing a more accurate estimate of $\beta$. The data averages to 4.09 with a smaller standard deviation of 0.27.

UCR MVT is the MVT statistics from UCR. It is the UCR theft rate (per 100,000 population) by state by year. It has a mean of 212.2 and a standard deviation of 107.06. Normalized UCR MVT (UCR MVT (0-100)) is the normalized metric scales of the UCR MVT data to a 0-100 range for comparative purposes, averaging at 34.93 with a standard deviation of 22.46. It is normalized by each year across states. UCR MVT (log) is the logarithmic transformation of UCR MVT data yields an average of 5.24 with a 0.51 standard deviation.

```{r, result = 'hide'}
acs_state_cdindex_mean <- mean(acs_state$Concentrated.Disadvantaged.Index)
```

The Concentrated Disadvantage Index (CD Index) aggregates key socioeconomic indicators: the percentage of unemployed individuals, single-parent households, families with income below poverty, and individuals over 25 with less than a high school education. These sub-category variables are first scaled individually and then summed to form the annual CD Index for each state. This process is repeated across a 12-year span^[\label{note_1}There are two reasons why the mean of the scaled data is not zero. First, computational limitations associated with numerical representation cause the actual mean of each sub-category variable in the CD Index to be close to zero, but not exactly zero, as discussed on Stack Overflow here https://stackoverflow.com/questions/40405803/mean-of-data-scaled-with-sklearn-standardscaler-is-not-zero. Second, the original calculation of ACS data includes the District of Columbia and Puerto Rico. However, due to data limitations, the final merged dataset excludes these two states. The original mean before dropping District of Columbia and Puerto Rico is `r acs_state_cdindex_mean`.]. The mean of CD Index is -0.33, with a standard deviation of 2.76. The Mobility Index is aggregated from the scaled values of the percentage of renters and the percentage of individuals who moved in from the same county, a different county, a different state, or abroad within the past year. The mean of the Mobility Index is -0.02, with a standard deviation of 1.24. The Heterogeneity Index is calculated by the probability of someone meeting another person from a different race. It has a mean of 0.45, a standard deviation of 0.16. The percentage of foreign-born individuals has a mean of 9.21, a standard deviation of 6.13. The percentage of young males has a mean of 6.97, a standard deviation of 0.47.

Population (log) is the log-transformed population count, showing an average of 15.19, a standard deviation of 1.01. The average number of vehicles per household (Average Vehicle HH) stands at 1.92, with a standard deviation of 0.28. Internet subscription rates average 81.45%, with a standard deviation of 7.42.

Table \ref{tab:desc_state} displays an overall sample size of 600 for each variable^[Fifty states, excluding Washington D.C. and Puerto Rico, over a twelve-year period (2011 to 2022).]. The selected visualization comparing GT MVT with UCR MVT (0-100) across Texas, California, New York, and Washington is presented in Figure \ref{fig:figure 7.1}. The goal of this comparison is to provide a general understanding on GT MVT trends compared to UCR MVT within each state by year. The aggregated MVT statistics across all states by year can be found in Figure \ref{fig:figure 7.1.1}. Generally, there is a parallel trend observed each year. A comprehensive visualization encompassing all 50 states can be found in Figure \ref{fig:figure A.3} in the Appendix.



```{r}
# Merge State - YEAR data
merged_state_year_data <- merge(merge(
  gt_state_annual, ucr_state, by = c("State", "Year")), 
  acs_state, by = c("State", "Year"), all.x = TRUE)

# Calculate the UCR MVT rate by dividing the UCR data by the total population
merged_state_year_data$UCR.MVT <- merged_state_year_data$UCR.MVT/merged_state_year_data$Population * 100000


# Normalize the UCR/NIBRS/NCVS/NICB Across the geographic regions
normalize_to_0_100 <- function(x) {
    # Check if all values are zero or NA
    if (all(x[!is.na(x)] == 0)) {
      return(rep(NA, length(x)))
    } else {
      # Calculate scaled values ignoring NA
      scaled <- ifelse(is.na(x), NA, (x - min(x, na.rm = TRUE)) * 100 / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
      # Replace NA values with 0 after scaling
      #scaled[is.na(scaled)] <- 0
      return(scaled)
    }
}


normalize_data_by_year <- function(data, year_col, variable_col) {
  # Function to normalize a variable to a 0-100 scale within the given dataframe
  

  # Generate the new variable column name automatically
  new_variable_col <- paste0(variable_col, "_normalized")

  # Initialize an empty list to store normalized data frames
  normalized_list <- list()

  # Get unique years in the data
  unique_years <- unique(data[[year_col]])

  # Iterate through each year
  for (year in unique_years) {
    
    # Filter data for the current year
    year_data <- dplyr::filter(data, !!rlang::sym(year_col) == year)

    # Normalize the variable and create a new column for it
    year_data <- dplyr::mutate(year_data, !!rlang::sym(new_variable_col) := normalize_to_0_100(!!rlang::sym(variable_col)))
    
    # Store the normalized data frame in the list
    normalized_list[[as.character(year)]] <- year_data
  }

  # Concatenate normalized data frames into a single data frame
  merged_data <- dplyr::bind_rows(normalized_list, .id = "Year")
  #replacing the remaining nan value as zero after normalization
  #merged_data[new_variable_col] <- lapply(merged_data[new_variable_col], function(x) {
    #x[is.na(x) | is.nan(x) | is.infinite(x)] <- 0
    #return(x)
    #})
  return(merged_data)
}

```


<!--

A test for normalization through regions (by DMA or State), but the correlation
is way lower than expected. It became 0.095 after normalization for the state level
between UCR MVT and GT MVT 

```{r test123}


normalize_data_by_region <- function(data, region_col, variable_col) {
  # Function to normalize a variable to a 0-100 scale within the given dataframe
  

  # Generate the new variable column name automatically
  new_variable_col <- paste0(variable_col, "_normalized")

  # Initialize an empty list to store normalized data frames
  normalized_list <- list()

  # Get unique years in the data
  unique_regions <- unique(data[[region_col]])

  # Iterate through each year
  for (region in unique_regions) {
    # Filter data for the current year
    region_data <- dplyr::filter(data, !!rlang::sym(region_col) == region)
    
    # Normalize the variable and create a new column for it
    region_data <- dplyr::mutate(region_data, !!rlang::sym(new_variable_col) := normalize_to_0_100(!!rlang::sym(variable_col)))
    
    # Store the normalized data frame in the list
    normalized_list[[as.character(region)]] <- region_data
  }

  # Concatenate normalized data frames into a single data frame
  merged_data <- dplyr::bind_rows(normalized_list, .id = region_col)

  # Replace 0 with NA in the new normalized column
  merged_data <- dplyr::mutate(merged_data, !!rlang::sym(new_variable_col) := dplyr::if_else(!!rlang::sym(new_variable_col) == 0, NA_real_, !!rlang::sym(new_variable_col)))

  return(merged_data)
}


```
-->


```{r}
# List of variable names to normalize

merged_state_year_data <- normalize_data_by_year(merged_state_year_data, "Year", "UCR.MVT")

# merged_state_year_data now has normalized versions of the specified variables
# +1 before log because log(0) == -inf
merged_state_year_data <- merged_state_year_data %>% 
  mutate(log.GT.MVT = log(GT.MVT+1)) %>% 
  mutate(log.UCR.MVT = log(UCR.MVT+1)) %>%
  mutate(UCR.MVT_normalized_log = log(UCR.MVT_normalized+1))

```


```{r}
#Check NA values
na_counts <- colSums(is.na(merged_state_year_data))
# This will give you the count of NA values in each column
#print(na_counts)
#drop District of Columbia because of NA value in UCR.MVT
merged_state_year_data <- merged_state_year_data %>% filter(State != "District of Columbia")


```



```{r, results = "asis", message=FALSE, warning = FALSE, echo = FALSE, header=FALSE}
# Selecting the specified variables from the dataframe
desc_variables <- c("GT.MVT", "log.GT.MVT", "UCR.MVT", "UCR.MVT_normalized",  "log.UCR.MVT",
                    "Concentrated.Disadvantaged.Index", "Mobility.Index", "Heterogeneity.Index", "Percentage.of.Foreign.Born", "Percentage.of.Young.Males",
               "Population..logged.", "Average.Vehicle.HH", "Percentage.of.Internet.Subscription.per.Household")

desc_variable_name <- c("GT MVT", "GT MVT (log)", "UCR MVT", "UCR MVT (0-100)", "UCR MVT (log)",
                        "Concentrated Disadvantage Index", "Mobility Index", "Heterogeneity Index",
                    "\\% Foreign Born", "\\% Young Males", 
                   "Population (log)", "Average Vehicle HH", "\\% Internet Subscription HH")

df1 = merged_state_year_data[desc_variables]

stargazer(df1, type = "latex", header = FALSE,  label = "tab:desc_state",
 title="Descriptive Statistics for State Variables, 2011 to 2022", digits=2, covariate.labels=desc_variable_name, notes = c("GT = Google Trends; HH = Household"), table.placement = "!htb")

```
```{r, results = "hide", message=FALSE, warning = FALSE, echo = FALSE, header=FALSE}

merged_state_year_data_mean <- merged_state_year_data %>%
  group_by(Year) %>%
  dplyr::summarize(
    Mean.GT.MVT = mean(GT.MVT, na.rm = TRUE),
    Mean.UCR.MVT = mean(UCR.MVT_normalized, na.rm = TRUE)
  )

merged_state_year_data_mean$Year <- as.numeric(merged_state_year_data_mean$Year)


pstate_year_all <- ggplot(merged_state_year_data_mean, aes(x = Year)) +
  geom_line(aes(y = Mean.GT.MVT, colour = "GT MVT"), size = 1) +
  geom_line(aes(y = Mean.UCR.MVT, colour = "UCR MVT"), size = 1) +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  scale_color_manual(values = c("GT MVT" = "#CC79A7", 
                                "UCR MVT" =  "#0072B2")) +
  labs(title = "Aggregated State-Year MVT Rates, By Year",
       x = "Year",
       y = "Normalized Value",
       colour = "Data Source") +
  theme_minimal()

ggsave("state_year_aggregated.png", pstate_year_all, width = 8, height = 6, bg = "white") 

```



\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{selected_mvt_year_state.png}
  \caption{Motor Vehicle Theft Statistics by State and Year (Selected States)}
  \label{fig:figure 7.1}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{state_year_aggregated.png}
  \caption{Aggregated Motor Vehicle Theft Statistics Across All States by Year}
  \label{fig:figure 7.1.1}
\end{figure}



## State-Year Correlation

```{r, results = "asis", message=FALSE, warning = FALSE, echo = FALSE}


corstars <-function(df, imported_row_names, method=c("pearson", "spearman"), removeTriangle=c("upper", "lower"),
                     result=c("none", "html", "latex")){
    #Compute correlation matrix
    require(Hmisc)
    x <- as.matrix(df)
    # If nan, transfer to zero
    x[is.na(x) | is.nan(x) | is.infinite(x)] <- 0

    correlation_matrix<-rcorr(x, type=method[1])
    R <- correlation_matrix$r # Matrix of correlation coeficients
    p <- correlation_matrix$P # Matrix of p-value 
    
    ## Define notions for significance levels; spacing is important.
    mystars <- ifelse(p < .01, "**", ifelse(p < .05, "*\ ", "\ \ "))
    
    ## trunctuate the correlation matrix to two decimal
    R <- format(round(cbind(rep(-1.11, ncol(x)), R), 3))[,-1]
    ## build a new matrix that includes the correlations with their apropriate stars
    Rnew <- matrix(paste(R, mystars, sep=""), ncol=ncol(x))
    diag(Rnew) <- paste(diag(R), " ", sep="")
    rownames(Rnew) <- colnames(x)
    colnames(Rnew) <- paste(colnames(x), "", sep="")
    
    ## remove upper triangle of correlation matrix
    if(removeTriangle[1]=="upper"){
      Rnew <- as.matrix(Rnew)
      Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
      Rnew <- as.data.frame(Rnew)
    }
    
    ## remove lower triangle of correlation matrix
    else if(removeTriangle[1]=="lower"){
      Rnew <- as.matrix(Rnew)
      Rnew[lower.tri(Rnew, diag = TRUE)] <- ""
      Rnew <- as.data.frame(Rnew)
    }
    
    
    len <- length(colnames(df))

    # Generate a sequence of numbers from 1 to len and format them with parentheses
    # So we can add (1), (2), ... into the correlation table
    formatted_sequence <- paste("(", seq(len), ")", sep="")
    
    Rnew[["new"]] <- formatted_sequence
    # get the (1), (2)... to the first column
    Rnew <- Rnew[,c(len+1, 1:(len))]
    #rename the columns
    colnames(Rnew) <- c( " ", formatted_sequence[-len])
    #rename the rows
    rownames(Rnew) <- imported_row_names
    ## remove last column and return the correlation matrix
    
    Rnew <- cbind(Rnew[1:length(Rnew)-1])
    if (result[1]=="none") return(Rnew)
    else{
      if(result[1]=="html") print(xtable(Rnew), type="html")
      else print(xtable(Rnew), type="latex") 
    }
} 
```





```{r state-correlation-table, results='asis', echo=FALSE, message=FALSE, warning = FALSE}

# Selecting the specified variables from the dataframe
variables_state <- c("GT.MVT", "log.GT.MVT", "UCR.MVT", "UCR.MVT_normalized",   "log.UCR.MVT")
variable_name_state <- c("GT MVT", "GT MVT (log)", "UCR MVT",  "UCR MVT (0-100)", "UCR MVT (log)")
state_cor_table <- corstars(merged_state_year_data[variables_state], variable_name_state)


#print latex table
final_state_corr_table <- kable(state_cor_table, longtable = T, booktabs = T, 
                          align = "l",
                          format = "latex", linesep = "",
    caption = 'Correlation Table of State Level MVT, 2011 - 2022') %>%
  kable_styling(latex_options="scale_down", font_size = 10,
                #position = "float_bottom", 
                bootstrap_options = "striped",
                full_width = F) %>%
  footnote("** = p < .01, * = p < .05; GT = Google Trends; MVT = Motor Vehicle Theft", footnote_as_chunk = T, threeparttable = T)

cat("\\begin{table}[!h]\n")
print(final_state_corr_table)
cat("\\end{table}\n")

```

Table \ref{tab:state-correlation-table} presents correlation coefficients among several MVT variables at the state level between 2011 and 2022. The correlation between GT MVT and UCR MVT is 0.681, which is statistically significant at the 0.01 level. This moderate positive correlation indicates that higher search volumes on Google Trends are associated with higher reported instances of vehicle theft. Furthermore, the correlation between GT MVT and the normalized UCR MVT (0-100) is even higher at 0.747, significant at the 0.01 level. This enhancement suggests that when UCR data is normalized to a 0-100 scale, mirroring the data format from Google Trends, the association strengthens. Additionally, the correlation between GT MVT and UCR MVT (log) is 0.655, also significant at the 0.01 level, supporting a consistent relationship even after log-transforming the official crime data. In @cohen2013statistical, the general guidelines for effect sizes classify approximately 0.20 as 'small,' 0.50 as 'medium,' and 0.80 as 'large.' However, within the field of personality-social measurement, the standards adjust to 0.1 for 'small,' 0.3 for 'medium,' and 0.5 for 'large.' Furthermore, when the effect size, $r$, reaches 0.2, a sample size of over 287 ensures that the power of the analysis exceeds 0.8. As the sample size increases, so does the power of the analysis [see also @bosco2015correlational].




<!--
float.env: a character string that specifies the floating environment of the resulting LaTeX table (when argument float is set to TRUE). Possible values are "table" (default), "table*" and "sidewaystable" (requires \usepackage{dcolumn} in
LaTeX preamble).


font.size: a character string that specifies the font size used in the table. The font can be one of the following: "tiny", "scriptsize", "footnotesize", "small",
"normalsize", "large", "Large", "LARGE", "huge", "Huge". If NULL (default), no particular font is imposed.
-->


```{r state-ols-table-func0, results='asis', echo=FALSE}
## In report the std.err in fixed-effect models, use coeftest vcovHC
## to account for heteroskedasticity and auto serial correlation
## "arellano" - both heteroskedasticity and serial correlation. 
## Recommended for fixed effects.
## https://www.princeton.edu/~otorres/Panel101R.pdf
## https://stackoverflow.com/questions/40136434/appending-statistics-to-coeftest-output-to-include-in-stargazer-tables

se_robust <- function(x) coeftest(x, vcovHC(x, method = "arellano"))[, "Std. Error"]

original_se <- function(x) summary(x)$coefficients[, "Std. Error"]
```

```{r}
#examples of using GLS
#https://rdrr.io/cran/nlme/man/corARMA.html
#https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/gls.html

#gls_model <- gls(UCR_MVT ~ GT_MVT, data = data,
#correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA))
```

<!--
Why p and q are set to 1:
p = 1 (Autoregressive Order):
This specifies that the model includes one lag of the dependent variable. It means the current value of the series (e.g., crime rate in a DMA for a specific month) is regressed on its immediately previous value (last month's crime rate). This is a simple model that assumes the last observation has a linear influence on the current observation. The choice of p = 1 is often a starting point in time series analysis to test for basic autoregressive effects.

q = 1 (Moving Average Order):
This indicates that the model includes one lag of the error term. It models the current value of the series as being influenced by the previous month’s forecast error. This is useful when the error terms (residuals) are correlated from one time period to the next, and a simple AR(1) model (just autoregressive without moving average) does not sufficiently capture the pattern in the residuals.

In the model correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA), this correlation structure is applied to handle potential autoregressive and moving average effects within each DMA across months:

form = ~ year_month | DMA: This specifies that the ARMA structure should be applied within each DMA, with year_month guiding the ordering of the data. This respects the time series nature within each DMA group, allowing for serial correlation that can vary by DMA.

-->


```{r state-ols-table-func01, results='asis', echo=FALSE}
## Regression Function for State-level analysis
run_regression_analysis_gt_and_official <- function(data, title, modeltype,label,
                                                    font_size, float_env = "table", table_position = "!htb") {
  # Define model formulas
  data$Year <- as.integer(data$Year)
  # Ensure 'State' is a factor
  data$State <- as.factor(data$State)
  formula1 <- UCR.MVT_normalized ~ GT.MVT
  formula2 <- log.UCR.MVT ~ GT.MVT
  formula1l <- UCR.MVT_normalized ~ log.GT.MVT
  formula2l <- log.UCR.MVT ~ log.GT.MVT
  formula11 <- UCR.MVT_normalized ~ GT.MVT + Population..logged. + Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household
  formula21 <- log.UCR.MVT ~ GT.MVT + Population..logged. + Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household
  formula11l <- UCR.MVT_normalized ~ log.GT.MVT + Population..logged. + Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household
  formula21l <- log.UCR.MVT ~ log.GT.MVT + Population..logged. + Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household

  # Choose model type based on input argument
  if (modeltype == "gls") {
    model_function <- function(
    formula, data) gls(formula, data = data,
                       correlation = corARMA(p = 1, q = 1, 
                                             form = ~ Year | State),
                       na.action = na.omit)
  } else if (modeltype == "plm") {
    model_function <- function(formula, data) plm(formula, data = data, index=c("State", "Year"), model = "within")
  } else {
    stop("Invalid model type. Use 'gls' for GLS linear models or 'plm' for panel models.")
  }

  # Apply model function to each formula
  m1 <- model_function(formula1, data = data)
  m2 <- model_function(formula2, data = data)
  m1l <- model_function(formula1l, data = data)
  m2l <- model_function(formula2l, data = data)
  m11 <- model_function(formula11, data = data)
  m21 <- model_function(formula21, data = data)
  m11l <- model_function(formula11l, data = data)
  m21l <- model_function(formula21l, data = data)
  

  
  model_list <- list(m1, m2, m1l, m2l, m11, m21, m11l, m21l)
  
  if (modeltype == "plm")  {
    se_list <- lapply(model_list, se_robust)  # Assuming 'se_robust' is defined to handle 'plm' objects
    } else {
      #se_list <- lapply(model_list,original_se)  # Replace NULL with your alternative method if needed
      }
  
  # Generate the table using stargazer
  stargazer(model_list,
            title = title,
            omit.stat = c("rsq", "ll", "ser"),
            no.space = TRUE,
            notes.append = FALSE, 
            notes = c("* p<0.05; ** p<0.01; GT = Google Trends", "MVT = Motor Vehicle Theft; HH = Household"),
            notes.align = "l",
            #se = se_list,
            dep.var.labels = rep("UCR MVT", 8),
            column.labels = c("(0-100)", "(log)", "(0-100)", "(log)",
                              "(0-100)", "(log)", "(0-100)", "(log)"),
            covariate.labels = c("GT MVT", "GT MVT (log)", "Population (log)",
                                 "Average Vehicle HH", "\\% Internet Subscription HH"),
            column.sep.width = "-2pt",
            font.size = font_size,
            digits = 2,
            df = FALSE,
            header = FALSE,
            star.char = c("*", "**"),
            star.cutoffs = c(.05, .01),
            label = label, 
            table.placement = table_position, 
            float.env = float_env,
            type = "latex")
}
```


```{r state-ols-func2, results='asis', echo=FALSE}
run_covariate_regression_analysis <- function(data, title, modeltype, label,
                                                    font_size, float_env = "table", table_position = "!htb") {
  # Define model formulas
  data$Year <- as.integer(data$Year)
  # Ensure 'State' is a factor
  data$State <- as.factor(data$State)
  # Check if the correct model type is specified and set the model function
  if (modeltype == "gls") {
    model_function <- function(formula, data) gls(formula, data = data,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | State),
                  na.action = na.omit)
  } else if (modeltype == "plm") {
    model_function <- function(formula, data)  plm(formula, data = data, index=c("State", "Year"), model = "within")
  } else {
    stop("Invalid model type specified. Use 'gls' for GLS regression or 'plm' for fixed effects models.")
  }
  
  # Define models using the selected model function
  m3 <- model_function(UCR.MVT_normalized ~ Concentrated.Disadvantaged.Index + Mobility.Index + 
            Heterogeneity.Index +  Percentage.of.Foreign.Born + Percentage.of.Young.Males +
            Population..logged. + Average.Vehicle.HH + Percentage.of.Internet.Subscription.per.Household, 
            data = data)

  m4 <- model_function(log.UCR.MVT ~ Concentrated.Disadvantaged.Index + Mobility.Index + 
            Heterogeneity.Index +  Percentage.of.Foreign.Born + Percentage.of.Young.Males +
            Population..logged. + Average.Vehicle.HH + Percentage.of.Internet.Subscription.per.Household, 
            data = data)

  m5 <- model_function(GT.MVT ~ Concentrated.Disadvantaged.Index + Mobility.Index + 
            Heterogeneity.Index +  Percentage.of.Foreign.Born + Percentage.of.Young.Males +
            Population..logged. + Average.Vehicle.HH + Percentage.of.Internet.Subscription.per.Household,
            data = data)

  m6 <- model_function(log.GT.MVT ~ Concentrated.Disadvantaged.Index + Mobility.Index + 
            Heterogeneity.Index +  Percentage.of.Foreign.Born + Percentage.of.Young.Males +
            Population..logged. + Average.Vehicle.HH + Percentage.of.Internet.Subscription.per.Household,
            data = data)

  model_list2 <- list(m3, m4, m5, m6)
  if (modeltype == "plm")  {
  se_list <- lapply(model_list2, se_robust)  # Assuming 'se_robust' is defined to handle 'plm' objects
  } else {
    #se_list <- lapply(model_list,original_se)  # Replace NULL with your alternative method if needed
    }
  # Generate the table using stargazer
  stargazer(m3, m4, m5, m6,
            title = title,
            omit.stat = c("rsq", "ll", "ser"),
            no.space = TRUE,
            notes.append = FALSE,
            notes = c("* p<0.05; ** p<0.01; GT = Google Trends", "MVT = Motor Vehicle Theft; HH = Household", "CD = Concentrated Disadvantage"),
            notes.align = "l",
            #se = se_list,
            dep.var.labels = c("UCR MVT","UCR MVT","GT MVT","GT MVT"),
            column.labels = c("(0-100)", "(log)", "", "(log)"),
            covariate.labels = c("CD Index", "Mobility Index", "Heterogeneity Index", "\\% Foreign Born",  "\\% Young Males",
                                 "Population (log)", "Average Vehicle HH", "\\% Internet Subscription HH"),
            column.sep.width = "-2pt",
            font.size = font_size,
            digits = 2,
            df = FALSE,
            header = FALSE,
            star.char = c("*", "**"),
            star.cutoffs = c(.05, .01),
            label = label, 
            table.placement = table_position, 
            float.env = float_env,
            type = "latex")
}
```



```{r}
# This is for the final forest plots
formula1 <- UCR.MVT_normalized ~ GT.MVT
formula11 <- UCR.MVT_normalized ~ GT.MVT + Population..logged. + Average.Vehicle.HH + Percentage.of.Internet.Subscription.per.Household
formula111<- UCR.MVT_normalized ~ Concentrated.Disadvantaged.Index + Mobility.Index + Heterogeneity.Index +  Percentage.of.Foreign.Born + Percentage.of.Young.Males + Population..logged. + Average.Vehicle.HH + Percentage.of.Internet.Subscription.per.Household

formula1111<- GT.MVT ~ Concentrated.Disadvantaged.Index + Mobility.Index + Heterogeneity.Index +  Percentage.of.Foreign.Born + Percentage.of.Young.Males + Population..logged. + Average.Vehicle.HH + Percentage.of.Internet.Subscription.per.Household

# GLS analysis requries year as integer and geo unit as factor (with orders)
state_for_gls <- merged_state_year_data
state_for_gls <- state_for_gls %>%
  arrange(State, Year)
# Define model formulas
state_for_gls$Year <- as.integer(state_for_gls$Year)
# Ensure 'State' is a factor
state_for_gls$State <- as.factor(state_for_gls$State)


ucr_state_gls <- gls(formula1, data = state_for_gls, 
                     correlation = corARMA(p = 1, q = 1, form = ~ Year | State),
                     na.action = na.omit)
ucr_state_gls_control <- gls(formula11, state_for_gls, 
                     correlation = corARMA(p = 1, q = 1, form = ~ Year | State),
                     na.action = na.omit)
ucr_state_gls_common <- gls(formula111, state_for_gls, 
                     correlation = corARMA(p = 1, q = 1, form = ~ Year | State),
                     na.action = na.omit)

ucr_state_plm <- plm(formula1, merged_state_year_data,index=c("State", "Year"), model = "within")

ucr_state_plm_control <- plm(formula11, merged_state_year_data,index=c("State", "Year"), model = "within")

ucr_state_plm_common <- plm(formula111, merged_state_year_data,index=c("State", "Year"), model = "within")

gt_state_gls_common <- gls(formula1111, state_for_gls, 
                     correlation = corARMA(p = 1, q = 1, form = ~ Year | State),
                     na.action = na.omit)

gt_state_plm_common <- plm(formula1111, merged_state_year_data,index=c("State", "Year"), model = "within")






```

```{r}
# This is for the final forest plots (logged)
formula1 <- log.UCR.MVT ~ GT.MVT
formula11 <- log.UCR.MVT ~ GT.MVT + Population..logged. + Average.Vehicle.HH + Percentage.of.Internet.Subscription.per.Household
formula111<- log.UCR.MVT ~ Concentrated.Disadvantaged.Index + Mobility.Index + Heterogeneity.Index +  Percentage.of.Foreign.Born + Percentage.of.Young.Males + Population..logged. + Average.Vehicle.HH + Percentage.of.Internet.Subscription.per.Household

formula1111<- log.GT.MVT ~ Concentrated.Disadvantaged.Index + Mobility.Index + Heterogeneity.Index +  Percentage.of.Foreign.Born + Percentage.of.Young.Males + Population..logged. + Average.Vehicle.HH + Percentage.of.Internet.Subscription.per.Household

ucr_state_gls_log <- gls(formula1, data = state_for_gls, 
                     correlation = corARMA(p = 1, q = 1, form = ~ Year | State),
                     na.action = na.omit)
ucr_state_gls_control_log <- gls(formula11, state_for_gls, 
                     correlation = corARMA(p = 1, q = 1, form = ~ Year | State),
                     na.action = na.omit)
ucr_state_gls_common_log <- gls(formula111, state_for_gls, 
                     correlation = corARMA(p = 1, q = 1, form = ~ Year | State),
                     na.action = na.omit)

ucr_state_plm_log <- plm(formula1, merged_state_year_data,index=c("State", "Year"), model = "within")

ucr_state_plm_control_log <- plm(formula11, merged_state_year_data,index=c("State", "Year"), model = "within")

ucr_state_plm_common_log <- plm(formula111, merged_state_year_data,index=c("State", "Year"), model = "within")

gt_state_gls_common_log <- gls(formula1111, state_for_gls, 
                     correlation = corARMA(p = 1, q = 1, form = ~ Year | State),
                     na.action = na.omit)

gt_state_plm_common_log <- plm(formula1111, merged_state_year_data,index=c("State", "Year"), model = "within")






```

## State-Year GLS Models


```{r stateolst1, results='asis', echo=FALSE, message=FALSE, warning = FALSE}
# Example usage
olsstatet1 <- run_regression_analysis_gt_and_official(data = merged_state_year_data, title = "State GLS Regression -- with UCR MVT v.s. GT MVT", modeltype = "gls", label = "tab:stateolst1", font_size = "scriptsize", table_position = "!htb")
```


Table \ref{tab:stateolst1} presents the results of GLS regression analyses exploring the relationship between GT MVT and UCR MVT, along with other control variables. Model (1) demonstrates that GT MVT is significantly and positively associated with UCR MVT, and this relationship persists even after adding control variables, as seen in model (5). Similarly, for GT MVT (log) on UCR MVT, consistent patterns are observed in models (3) and (7). However, the log-transformation of UCR MVT does not show a significant association with either GT MVT or GT MVT (log). Given that this method is a more conservative test of their linear relationship, it is expected. This test answers research questions 1.1 & 1.2 linear relationship across time and space -- at state-year level.



```{r state-ols-table2, results='asis', echo=FALSE}
# Example usage
run_covariate_regression_analysis(data = merged_state_year_data, title = "State GLS Regression -- with Common MVT Covariates", modeltype = "gls", label = "tab:stateolst2", font_size = "scriptsize", table_position = "!htb")
```

Table \ref{tab:stateolst2} presents the results from GLS regression analyses with various common crime covariates examining their influence on UCR MVT and GT MVT^[Upon examining the Variance Inflation Factor (VIF) indicators for all models applied to common crime covariates, the highest VIF is 4.38, observed in the State-year PLM model. Other VIFs, whether for fixed-effect or GLS models, are all below 4. This indicates that multicollinearity does not pose a problem for these regression models. Detailed VIF results can be found in the appendix, in Table \ref{tab:Appendixvif}.]. The analyses focus on the concurrent validity of GT MVT relative to UCR MVT, aiming to determine if GT MVT reflects similar statistical relationships with known crime-related covariates as UCR MVT does. The mobility index has significant and positive association with UCR MVT (100), GT MVT, and GT MVT (log). It can be observed in model (1), (3), and (4). From model (2) to model (4), the heterogeneity index shows a significant positive association with UCR MVT (log), GT MVT, and GT MVT (log). Additionally, the percentage of foreign-born individuals shows a significantly negative association with UCR MVT (log). The concentrated disadvantage index exerts a negative and significant impact on UCR MVT (100) and GT MVT in model (1) and (3). The control variable, average number of vehicles per household, has a significant association with UCR MVT in model (1) but shows no significant relationship in other models. 

Overall, the results in Table \ref{tab:stateolst2} indicate that the mobility index and heterogeneity index are robust predictors for both UCR MVT and GT MVT at the state-annual level. The percentage of foreign-born is negatively associated with log-transformed UCR MVT; and the concentrated disadvantage index is significantly negatively associate with UCR MVT and GT MVT. Overall, GT MVT demonstrates its concurrent validity by mirroring significant effects found in official data, I found the observed association with common crime covariates on UCR MVT can also be found on GT MVT. This finding reinforcing the utility of Google Trends as a complementary analytical tool for motor vehicle theft alongside traditional crime statistics at the state-yearly level. This test answers research question 2.1, the association of common crime covariates on official MVT and GT MVT.


## State-Year Fixed-Effect Models

```{r state-fixed-effect-table1, results='asis', echo=FALSE}
run_regression_analysis_gt_and_official(data = merged_state_year_data, title = "State Fixed Effect Model -- with UCR MVT v.s. GT MVT", modeltype = "plm", label = "tab:statfix1", font_size = "scriptsize", table_position = "!htb")
```

Table \ref{tab:statfix1} presents the results from state fixed effect models that examine the impact of GT MVT on UCR MVT, allowing us to control for time-invariant confounders. The findings consistently show a significant positive effect of both GT MVT and GT MVT (log) on UCR MVT and UCR MVT(100) across all models from (1) to (8), with and without control variables. This evidence highlights the strong predictive power of GT MVT data on UCR MVT. The significant relationships persist in the log-transformed versions and remain robust even after adjusting for other variables, such as the logged population, average number of motor vehicles per household, and the percentage of internet subscriptions per household. This consistency underscores the reliable linear relationship between GT MVT and UCR MVT at the state-year level. This test answers research question 1.1 & 1.2, the linear relationship between official MVT and GT MVT.

```{r state-fixed-effect-table2, results='asis', echo=FALSE}
run_covariate_regression_analysis(data = merged_state_year_data, title = "State Fixed Effect Model -- with Common MVT Covariates", modeltype = "plm", label = "tab:statfix2", font_size = "scriptsize", table_position = "!htb")
```


Table \ref{tab:statfix2} explores the relationship between state-level MVT within the context of common crime covariates, using a fixed-effect model. The findings indicate that the concentrated disadvantage index significantly impacts UCR MVT (log) and GT MVT in models (2) and (3), but in opposite directions—positively affecting UCR MVT (log) and negatively impacting GT MVT and GT MVT (log). Moreover, the heterogeneity index significantly and positively affects GT MVT but does not impact other dependent variables. The percentage of foreign born shows an overall negative relationship with all MVT variables, and is significant with UCR MVT (log), GT MVT, and GT MVT (log) in model (2), (3), and (4).

These mixed results in CD Index suggest a limited ability to apply crime covariates effectively when using single crime statistics as the dependent variable within one type of statistical model. To achieve more robust results, utilizing different statistical tests with multiple data sources can be beneficial. Comparing the results from Table \ref{tab:statfix2} with those from Table \ref{tab:stateolst2} may provide deeper insights into how GT MVT and UCR MVT associate with common crime covariates, which I will discuss further in the conclusion chapter. Overall, the findings here indicate a mixed capability for GT MVT to replicate the outcomes observed with UCR MVT when analyzed with common crime covariates in a fixed-effect model at the state-year level. This analysis addresses research question 2.1 concerning the association of common crime covariates with official MVT and GT MVT.

\FloatBarrier
# DMA-Year Analysis

Table \ref{tab:desc_dma_year} presents descriptive statistics for various DMA-year level variables from 2011 to 2022. It includes data on motor vehicle theft from GT, UCR, NCVS, and NICB^[For the full list of DMAs, please see \ref{tab:dmayearlist}.], alongside other common crime covariates: concentrated disadvantage index, mobility index, heterogeneity index, percentage of foreign-born, and percentage of young males. The control variables are population (logged), average vehicles per household, and the percentage of internet subscription per household.

In Table \ref{tab:desc_dma_year}, GT MVT has a sample size of 1,356, representing 113 DMA over 12 years. The data from Google Trends is inherently normalized from 0 to 100 by year across all DMAs within that year. The average GT MVT stands at 47.83, with a standard deviation of 14.77. GT MVT (log) is the logarithmically transformed GT MVT. The data averages to 3.84, with a smaller standard deviation of 0.31.

UCR MVT is the MVT statistics from UCR, representing the MVT rate (per 100,000 population) by DMA by year. It has a sample size of 1,356, corresponding to the 113 DMA with GT over 12 years. It has a mean of 220.81 and a standard deviation of 124.53. Normalized UCR MVT (UCR MVT (0-100)) scales the UCR MVT data to a 0-100 range for comparative purposes, averaging 27.43 with a standard deviation of 18.3. It is normalized by each year across DMA, ranging from 0 to 100. UCR MVT (log) is the logarithmic transformation of UCR MVT data, yielding an average of 5.25 with a 0.56 standard deviation.

NCVS MVT represents the MVT data from the National Crime Victimization Survey. It represents the MVT rate (per 100,000 population) by DMA by month from NCVS. It has a sample size of 225, corresponding to the available data for 25 DMA over 5 years, from 2011 to 2015. NCVS MVT has a mean of 2.99, a standard deviation of 2.31. Normalized NCVS MVT (NCVS MVT (0-100)) is scaled to a 0-100 range, with a mean of 25.90, a standard deviation of 23.14. NCVS MVT (log) is the log-transformed NCVS MVT, with an average of 1.24, a standard deviation of 0.53.

NICB MVT is the MVT data from the National Insurance Crime Bureau (NICB).It represents the MVT rate (per 100,000 population) by DMA by year from NICB. It has a sample size of 1,224, representing 102 DMAs with GT over 12 years. It has a mean of 277.91, a standard deviation of 157.23. Normalized NICB MVT (NICB MVT (0-100)) scales this data to a 0-100 range, with a mean of 28.68, a standard deviation of 19.43. NICB MVT (log) is the log-transformed NICB MVT, with an average of 5.47, a standard deviation of 0.59.

The other variables come from the American Community Survey and directly merge with the GT data, resulting in a sample size of 1,356 for each. The Concentrated Disadvantage Index (CD Index) aggregates from the following socioeconomic indicators: the percentage of unemployed individuals, single-parent households, families with income below poverty, and individuals over 25 with less than a high school education. These sub-category variables are first scaled individually and then summed to form the annual CD Index for each DMA. This process is repeated across a 12-year span. The mean of the CD Index is -0.03, with a standard deviation of 3.04^[Please see footnote \ref{note_1}].

The Mobility Index is aggregated from the scaled values of the percentage of renters and the percentage of individuals who moved in from the same county, a different county, a different state, or abroad within the past year. The mean of the Mobility Index is 0.15, with a standard deviation of 1.48. The Heterogeneity Index is calculated by the probability of someone meeting another person from a different race. It has a mean of 0.46, a standard deviation of 0.15. The percentage of foreign-born individuals has a mean of 9.66, a standard deviation of 7.19. The percentage of young males has a mean of 7.09, a standard deviation of 0.79.

Population (log) is the log-transformed format of population count, showing an average of 14.38, a standard deviation of 0.77. The average number of vehicles per household (Average Vehicle HH) stands at 1.89, with a standard deviation of 0.23. Internet subscription rates average 82.22, with a standard deviation of 7.48.

The selected visualization of the relationships between GT MVT, UCR MVT (0-100), NCVS MVT (0-100), and NICB MVT (0-100) across DMA are depicted in Figure \ref{fig:figure 7.2}, and the full grid of figures including 113 DMA can be found in Appendix Table \ref{fig:figure A.3}. The aggregated MVT statistics across all DMAs by year can be found in Figure \ref{fig:figure 7.2.1}. The goal of this comparison is to provide a general understanding of GT MVT trends compared to UCR MVT, NCVS, and NICB within each DMA by year. Generally, there is a parallel trend observed each year. The limited performance of NCVS compared to other crime statistics may stem from its restricted sample range from 2011 to 2015, which make the trend comparisons with NCVS data more difficult.


```{r}
merged_dma_year_data <- merge(merge(merge(merge(
  ucr_dma, gt_dma_annual, by = c("DMA", "Year")),
  nicb_dma, by = c("DMA", "Year"), all.x = TRUE), 
  acs_dma, by = c("DMA", "Year"), all.x = TRUE),
  ncvs_dma, by = c("DMA", "Year"), all.x = TRUE)
```



```{r}
#DEAL WITH SPEICAL MISSING VALUES IN Hartford & New Haven CT
#One MISSING VALUES IN year 2022, use interpolate to add back
merged_dma_year_data <- merged_dma_year_data %>%
  mutate(across(where(is.numeric) & -all_of("NCVS.MVT"), 
                ~ if_else(DMA == "Hartford & New Haven CT", na.approx(., na.rm = TRUE, rule = 2), .)))
ncvs_na_count <- ncvs_na_count + 1 
```


```{r}
# Calculate the UCR MVT rate by dividing the UCR data by the total population
merged_dma_year_data$UCR.MVT <- merged_dma_year_data$UCR.MVT/merged_dma_year_data$Population*100000



merged_dma_year_data <- normalize_data_by_year(merged_dma_year_data, "Year", "UCR.MVT")
merged_dma_year_data <- normalize_data_by_year(merged_dma_year_data, "Year", "NCVS.MVT")

#The NCVS data only in 2011 to 2015, assign NA if normalized
merged_dma_year_data <- merged_dma_year_data %>%
  mutate(NCVS.MVT_normalized = if_else(Year >= 2016, NA_real_, NCVS.MVT_normalized))

#assign all zero into NA in the normalization process
merged_dma_year_data <- merged_dma_year_data %>%
  mutate(NCVS.MVT_normalized = if_else(
    NCVS.MVT_normalized == 0, NA_real_, NCVS.MVT_normalized))

#get the origianl 0 back into the normalized value
merged_dma_year_data <- merged_dma_year_data %>%
  mutate(NCVS.MVT_normalized = if_else(is.na(NCVS.MVT_normalized) & !is.na(NCVS.MVT), 0, NCVS.MVT_normalized))


merged_dma_year_data <- normalize_data_by_year(merged_dma_year_data, "Year", "NICB.MVT")


#log data

# merged_state_year_data now has normalized versions of the specified variables
# +1 before log because log(0) == -inf
merged_dma_year_data <- merged_dma_year_data %>% 
                              mutate(log.GT.MVT = log(GT.MVT+1)) %>% 
                              mutate(log.UCR.MVT = log(UCR.MVT+1)) %>%
                              mutate(log.NCVS.MVT = log(NCVS.MVT+1)) %>%
                              mutate(log.NICB.MVT = log(NICB.MVT+1))
```



```{r}
# Filter the DMA without full years from 2011 to 2022
# Filter years of interest
years_of_interest <- 2011:2022

# Group by DMA and check if all years are unique
filtered_dma_year_data <- merged_dma_year_data %>%
  group_by(DMA) %>%
  filter(length(unique(Year)) == length(years_of_interest))

# filter out the DMA without full years
merged_dma_year_data <- filtered_dma_year_data
```


```{r, results = "asis", message=FALSE, warning = FALSE, echo = FALSE}

# Selecting the specified variables from the dataframe
dma_desc_variables <- c("GT.MVT", "log.GT.MVT", 
                        "UCR.MVT",  "UCR.MVT_normalized",  "log.UCR.MVT",
                        "NCVS.MVT", "NCVS.MVT_normalized", "log.NCVS.MVT",
                        "NICB.MVT", "NICB.MVT_normalized", "log.NICB.MVT",
                        "Concentrated.Disadvantaged.Index", "Mobility.Index",
                        "Heterogeneity.Index",
                         "Percentage.of.Foreign.Born",
                        "Percentage.of.Young.Males",
                        "Population..logged.", "Average.Vehicle.HH",
                        "Percentage.of.Internet.Subscription.per.Household")

dma_desc_variable_name <- c("GT MVT", "GT MVT (log)", 
                            "UCR MVT",  "UCR MVT (0-100)", "UCR MVT (log)", 
                            "NCVS MVT", "NCVS MVT (0-100)", "NCVS MVT (log)", 
                            "NICB MVT", "NICB MVT (0-100)", "NICB MVT (log)",  
                   "Concentrated Disadvantage Index", "Mobility Index",
                   "Heterogeneity Index",  
                   "\\% Foreign Born", "\\% Young Males", 
                   "Population (log)", "Average Vehicle HH", 
                   "\\% Internet Subscription HH")

df2 <- merged_dma_year_data[dma_desc_variables]
df2  <- as.data.frame(df2)
stargazer(df2, type = "latex", header = FALSE, 
          title="Descriptive Statistics of DMA-Yearly Data", digits=2, 
          covariate.labels=dma_desc_variable_name, notes = c("HH = Household"), label = "tab:desc_dma_year", table.placement = "!htb")

```
```{r, results = "hide", message=FALSE, warning = FALSE, echo = FALSE}
summary_by_dma <- merged_dma_year_data %>%
  group_by(DMA) %>%
  summarize_all(list(mean = ~ mean(., na.rm = TRUE)))

# Select only the columns of interest
summary_by_dma <- summary_by_dma %>%
  dplyr::select(DMA, GT.MVT_mean, UCR.MVT_normalized_mean, NCVS.MVT_normalized_mean, NICB.MVT_normalized_mean)

summary_by_year <- merged_dma_year_data %>%
  group_by(Year) %>%
  summarize_all(list(mean = ~ mean(., na.rm = TRUE)))

# Select only the columns of interest
summary_by_year <- summary_by_year %>%
  dplyr::select(Year, GT.MVT_mean, UCR.MVT_normalized_mean, NCVS.MVT_normalized_mean, NICB.MVT_normalized_mean)


```


```{r, results = "hide", message=FALSE, warning = FALSE, echo = FALSE, header=FALSE}



summary_by_year$Year <- as.numeric(summary_by_year$Year)


pdma_year_all <- ggplot(summary_by_year, aes(x = Year)) +
  geom_line(aes(y = GT.MVT_mean, colour = "GT MVT"), size = 1) +
  geom_line(aes(y = UCR.MVT_normalized_mean, colour = "UCR MVT"), size = 1) +
  geom_line(aes(y = NCVS.MVT_normalized_mean, colour = "NCVS MVT"), size = 1) +
  geom_line(aes(y = NICB.MVT_normalized_mean, colour = "NICB MVT"), size = 1) +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  scale_color_manual(values = c("GT MVT" = "#CC79A7", 
                                "UCR MVT" =  "#0072B2",
                                "NCVS MVT" = "#E69F00",
                                "NICB MVT" = "#56B4E9"),
                     breaks = c("GT MVT", "UCR MVT", 
                                "NCVS MVT", "NICB MVT")) +
  labs(title = "Aggregated DMA-Year MVT Statistics, By Year",
       x = "Year",
       y = "Normalized Value",
       colour = "Data Source") +
  theme_minimal()

ggsave("dma_year_aggregated.png", pdma_year_all, width = 8, height = 6, bg = "white") 

```

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{selected_mvt_year_dma.png}
  \caption{Motor Vehicle Theft Statistics by DMA and Year (Selected DMA)}
  \label{fig:figure 7.2}
\end{figure}


\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{dma_year_aggregated.png}
  \caption{Aggregated Motor Vehicle Theft Statistics Across All DMAs by Year}
  \label{fig:figure 7.2.1}
\end{figure}


\FloatBarrier

## DMA-Year Correlation

```{r dma-correlation-table, results='asis', echo=FALSE, message=FALSE, warning = FALSE}

# Selecting the specified variables from the dataframe
dma_variables <- c("GT.MVT", "log.GT.MVT", 
                    "UCR.MVT",  "UCR.MVT_normalized",  "log.UCR.MVT",
                    "NCVS.MVT", "NCVS.MVT_normalized", "log.NCVS.MVT",
                    "NICB.MVT", "NICB.MVT_normalized", "log.NICB.MVT")

dma_variable_name <- c("GT MVT", "GT MVT (log)", 
                      "UCR MVT",  "UCR MVT (0-100)", "UCR MVT (log)", 
                      "NCVS MVT", "NCVS MVT (0-100)", "NCVS MVT (log)", 
                      "NICB MVT", "NICB MVT (0-100)", "NICB MVT (log)")


dma_cor_table <- corstars(merged_dma_year_data[dma_variables], dma_variable_name)


#print latex table
final_dma_corr_table <- kable(dma_cor_table, longtable = T, booktabs = T, 
                          align = "l",
                          format = "latex", linesep = "",
    caption = 'Correlation Table of DMA Level MVT, 2011 - 2022\\label{dma_cor_table}') %>%
  kable_styling(latex_options="scale_down", font_size = 10,
                #position = "float_left", 
                bootstrap_options = "striped",
                full_width = F) %>%
  footnote("** = p < .01, * = p < .05; GT = Google Trends; MVT = Motor Vehicle Theft", footnote_as_chunk = T, threeparttable = T) 
#%>%  kableExtra::landscape()
cat("\\begin{sidewaystable}[!tp]\n")
print(final_dma_corr_table)
cat("\\end{sidewaystable}\n")
```



The correlation table provided in Table \ref{dma_cor_table} shows the relationships between different measures of MVT at the DMA level from 2011 to 2022. The strongest correlation exists between UCR MVT and NICB MVT, with coefficients ranging from 0.681 to 0.756, excluding the log-transformed NICB MVT (log). Notably, NICB MVT (log) exhibits a gap in correlation with other variables. Although this finding is not entirely surprising, as log-transformation generally provides a more conservative measure, it may underestimate the important relationship between variables.

GT MVT shows a moderate to strong correlation with UCR MVT, with coefficients varying from 0.576 to 0.653. Additionally, GT MVT exhibits a moderate association with NICB MVT; the correlation coefficients between these measures, across both scaled and log-transformed formats, range from 0.503 to 0.586, excluding NICB MVT (log). In contrast, the correlations between NCVS MVT, NCVS MVT (0-100) and NCVS MVT (log) with GT, UCR, and NICB data are weak, spanning from 0.088 to 0.210. This analysis confirms a positive, moderate to strong association between GT MVT, UCR MVT, and NICB MVT at the DMA-year level. This analysis addresses research question 1.1 and 1.2 regarding the linear relationships between GT MVT and other official MVT measures over time and across regions.

## DMA-Year GLS Models 

Tables \ref{tab:ols_dma_year1} and \ref{tab:ols_dma_year2} utilize GLS regression to explore the linear relationship between GT MVT and other official MVT metrics, including UCR, NCVS, and NICB, across both normalized (0-100) and log-transformed scales, with or without the control variables such as the logged population, average vehicle per household, and the percentage of internet subscription per household. In Table \ref{tab:ols_dma_year1} models (1) to (4), GT MVT demonstrates a significant positive relationship with UCR MVT across both scaled and log-transformed formats, with and without control variables. Similarly, GT MVT shows significant positive relationships with NICB MVT, across both scaled and log-transformed scales, with and without control variables, from models (9) to (12). Regarding NCVS MVT, GT MVT only demonstrates a significant and positive relationship in model (6), which features scaled NCVS with control variables.

Table \ref{tab:ols_dma_year2} presents the results using log-transformed GT MVT as the independent variable. The findings are largely consistent with those in Table \ref{tab:ols_dma_year1}, except for model (3), where there is no significant relationship between GT MVT and UCR MVT (log). However, models (1), (2), (4), and (9) through (12) show similar results to those found in Table \ref{tab:ols_dma_year1}, where GT MVT is significantly and positively associated with UCR MVT and NICB MVT. GT MVT is only significantly associated with NCVS in its scaled form when control variables are included. This GLS test further confirms the linear relationships between GT MVT and UCR MVT, as well as between GT MVT and NICB MVT, both with and without control variables at the DMA-year level. This GLS regression test addresses questions 1.1 and 1.2, the linear relationship between GT MVT and other official metrics, with/without control variables.



```{r dma-ols-table1, results='asis', echo=FALSE}

merged_dma_year_data <- as.data.frame(merged_dma_year_data)

merged_dma_year_data_for_lm <- merged_dma_year_data
merged_dma_year_data_for_lm <- merged_dma_year_data_for_lm %>% 
  arrange(DMA, Year)
merged_dma_year_data_for_lm$Year <- as.integer(merged_dma_year_data_for_lm$Year)

# Ensure 'State' is a factor
merged_dma_year_data_for_lm$DMA <- as.factor(merged_dma_year_data_for_lm$DMA)

# Define models
dma_m1 <- gls(UCR.MVT_normalized ~ GT.MVT, data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)

dma_m11 <- gls(UCR.MVT_normalized ~ GT.MVT + Population..logged. +
                Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household,
              data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
dma_m2 <- gls(log.UCR.MVT ~ GT.MVT, data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
dma_m21 <- gls(log.UCR.MVT ~ GT.MVT + Population..logged. +
                Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household,
              data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
dma_m3 <- gls(NCVS.MVT_normalized ~ GT.MVT, data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
dma_m31 <- gls(NCVS.MVT_normalized ~ GT.MVT + Population..logged. +
                Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household,
              data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
dma_m4 <- gls(log.NCVS.MVT ~ GT.MVT, data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
dma_m41 <- gls(log.NCVS.MVT ~ GT.MVT + Population..logged. +
                Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household,
              data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
dma_m5 <- gls(NICB.MVT_normalized ~ GT.MVT, data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
dma_m51 <- gls(NICB.MVT_normalized ~ GT.MVT + Population..logged. +
                Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household,
              data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)

dma_m6 <- gls(log.NICB.MVT ~ GT.MVT, data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
dma_m61 <- gls(log.NICB.MVT ~ GT.MVT + Population..logged. +
                Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household,
              data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)

dma_ols_models <- list(dma_m1,dma_m11,dma_m2,dma_m21,dma_m3,dma_m31,dma_m4,dma_m41,dma_m5,dma_m51,dma_m6, dma_m61)

stargazer(dma_ols_models,
          title = "DMA Yearly GLS Regression -- Official MVT v.s. GT MVT",
          omit.stat = c("rsq", "ll", "ser"),
          no.space = TRUE,
          table.placement = "!htb",
          notes.append = FALSE, 
          notes = c("* p<0.05; ** p<0.01; GT = Google Trends"),
          notes.align = "l",
          #se = lapply(dma_ols_models, se_robust),
          dep.var.labels=c("UCR MVT","UCR MVT","NCVS MVT","NCVS MVT","NICB MVT", "NICB MVT"),
          column.labels = c("(0-100)","(0-100)", "(log)", "(log)", 
                            "(0-100)","(0-100)", "(log)", "(log)", 
                            "(0-100)","(0-100)", "(log)", "(log)"),

          covariate.labels = c("GT MVT", 
                               "Population (log)", 
                               "Average Vehicle HH", 
                               "\\% Internet Subscription HH"),
          column.sep.width = "-2pt",
          font.size = "footnotesize",
          float.env = "sidewaystable",
          digits = 2,
          df = F,
          header=FALSE,
          star.char = c( "*", "**"),
          star.cutoffs = c(.05, .01),
          label = "tab:ols_dma_year1",
          type = "latex")
          #type = "html",
          #out = "regression_models2.html")

```


```{r dma-ols-table2, results='asis', echo=FALSE}



# Define models
ldma_m1 <- gls(UCR.MVT_normalized ~ log.GT.MVT, data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
ldma_m11 <- gls(UCR.MVT_normalized ~ log.GT.MVT + Population..logged. +
                Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household,
              data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
ldma_m2 <- gls(log.UCR.MVT ~ log.GT.MVT, data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
ldma_m21 <- gls(log.UCR.MVT ~ log.GT.MVT + Population..logged. +
                Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household,
              data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
ldma_m3 <- gls(NCVS.MVT_normalized ~ log.GT.MVT, data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
ldma_m31 <- gls(NCVS.MVT_normalized ~ log.GT.MVT + Population..logged. +
                Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household,
              data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
ldma_m4 <- gls(log.NCVS.MVT ~ log.GT.MVT, data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
ldma_m41 <- gls(log.NCVS.MVT ~ log.GT.MVT + Population..logged. +
                Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household,
              data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
ldma_m5 <- gls(NICB.MVT_normalized ~ log.GT.MVT, data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
ldma_m51 <- gls(NICB.MVT_normalized ~ log.GT.MVT + Population..logged. +
                Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household,
              data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)

ldma_m6 <- gls(log.NICB.MVT ~ log.GT.MVT, data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)
ldma_m61 <- gls(log.NICB.MVT ~ log.GT.MVT + Population..logged. +
                Average.Vehicle.HH +
                Percentage.of.Internet.Subscription.per.Household,
              data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)

dma_ols_models_log <- list(ldma_m1,ldma_m11,ldma_m2,ldma_m21,ldma_m3,ldma_m31,ldma_m4,ldma_m41,ldma_m5,ldma_m51,ldma_m6, ldma_m61)

stargazer(dma_ols_models_log,
          title = "DMA Yearly GLS Regression -- Official MVT v.s. GT MVT (log)",
          omit.stat = c("rsq", "ll", "ser"),
          no.space = TRUE,
          table.placement = "!htb",
          notes.append = FALSE, 
          notes = c("* p<0.05; ** p<0.01; GT = Google Trends"),
          notes.align = "l",
          #se = lapply(dma_ols_models_log, se_robust),
          dep.var.labels=c("UCR MVT","UCR MVT","NCVS MVT","NCVS MVT","NICB MVT", "NICB MVT"),
          column.labels = c("(0-100)","(0-100)", "(log)", "(log)", 
                            "(0-100)","(0-100)", "(log)", "(log)", 
                            "(0-100)","(0-100)", "(log)", "(log)"),

          covariate.labels = c("GT MVT (log)", 
                               "Population (log)", 
                               "Average Vehicle HH", 
                               "\\% Internet Subscription HH"),
          column.sep.width = "-2pt",
          font.size = "footnotesize",
          float.env = "sidewaystable",
          digits = 2,
          df = F,
          header=FALSE,
          star.char = c( "*", "**"),
          star.cutoffs = c(.05, .01),
          label = "tab:ols_dma_year2")
          #type = "html",
          #out = "regression_models2.html")

```


Table \ref{tab:ols_dma_year3} presents a GLS regression analysis that explores the impact of common crime covariates on various measures of MVT including GT MVT, UCR MVT, NCVS MVT, and NICB MVT, over the years 2011-2022 at the DMA-year level. The GLS regression models assess both normalized (0-100) and log-transformed scales of MVT data as dependent variables. The results concerning the influence of common crime covariates on MVT metrics are mixed. Notably, the mobility index and heterogeneity index emerge as the strongest predictors. The mobility index shows significant and positive relationships in models (1), (5), (7), and (8) with UCR MVT (0-100), NICB MVT (0-100), GT MVT, and GT MVT (log). The heterogeneity index also exhibits significant positive effects on UCR MVT in models (1) and (2), and on GT MVT in models (7) and (8), across both scaled and log-transformed formats. Conversely, the percentage of young males significantly and negatively associated with UCR MVT (0-100) and NICB MVT (0-100) in models (1) and (5). The percentage of foreign-born individuals shows a positive association with UCR MVT (0-100), NICB MVT (0-100), and GT MVT (0-100) in model (1), (5), and (7). 

Meanwhile, the concentrated disadvantage index yields mixed results, displaying positive significant relationships with UCR MVT (log) and NICB MVT (log), but is negatively and significantly associated with NICB MVT (0-100), GT MVT (0-100), and GT MVT (log) in models (5), (7), and (8). The goal of the analysis is to determine if there are similar associations between GT MVT and other official crime statistics. GT MVT has performed very well, and despite some discrepancies in the direction of the concentrated disadvantage index, it has shown convergence with other variables in the mobility index, and heterogeneity index. This test addresses research question 2.1, exploring the concurrent validity of GT MVT.


\clearpage
\newpage
```{r dma-ols-table-covariates, results='asis', echo=FALSE}

# Define models using common covariates directly in the formula
m1 <- gls(UCR.MVT_normalized ~ Concentrated.Disadvantaged.Index + Mobility.Index + 
           Heterogeneity.Index +  Percentage.of.Foreign.Born + 
           Percentage.of.Young.Males + Population..logged. + Average.Vehicle.HH +
           Percentage.of.Internet.Subscription.per.Household,
         data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)

m2 <- gls(log.UCR.MVT ~ Concentrated.Disadvantaged.Index + Mobility.Index + 
           Heterogeneity.Index +  Percentage.of.Foreign.Born + 
           Percentage.of.Young.Males + Population..logged. + Average.Vehicle.HH +
           Percentage.of.Internet.Subscription.per.Household,
         data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)

m3 <- gls(NCVS.MVT_normalized ~ Concentrated.Disadvantaged.Index + Mobility.Index + 
           Heterogeneity.Index +  Percentage.of.Foreign.Born + 
           Percentage.of.Young.Males + Population..logged. + Average.Vehicle.HH +
           Percentage.of.Internet.Subscription.per.Household,
         data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)

m4 <- gls(log.NCVS.MVT ~ Concentrated.Disadvantaged.Index + Mobility.Index + 
           Heterogeneity.Index +  Percentage.of.Foreign.Born + 
           Percentage.of.Young.Males + Population..logged. + Average.Vehicle.HH +
           Percentage.of.Internet.Subscription.per.Household,
         data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)

m5 <- gls(NICB.MVT_normalized ~ Concentrated.Disadvantaged.Index + Mobility.Index + 
           Heterogeneity.Index +  Percentage.of.Foreign.Born + 
           Percentage.of.Young.Males + Population..logged. + Average.Vehicle.HH +
           Percentage.of.Internet.Subscription.per.Household,
         data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)

m6 <- gls(log.NICB.MVT ~ Concentrated.Disadvantaged.Index + Mobility.Index + 
           Heterogeneity.Index +  Percentage.of.Foreign.Born + 
           Percentage.of.Young.Males + Population..logged. + Average.Vehicle.HH +
           Percentage.of.Internet.Subscription.per.Household,
         data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)

m7 <- gls(GT.MVT ~ Concentrated.Disadvantaged.Index + Mobility.Index + 
           Heterogeneity.Index +  Percentage.of.Foreign.Born + 
           Percentage.of.Young.Males + Population..logged. + Average.Vehicle.HH +
           Percentage.of.Internet.Subscription.per.Household,
         data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)

m8 <- gls(log.GT.MVT ~ Concentrated.Disadvantaged.Index + Mobility.Index + 
           Heterogeneity.Index +  Percentage.of.Foreign.Born + 
           Percentage.of.Young.Males + Population..logged. + Average.Vehicle.HH +
           Percentage.of.Internet.Subscription.per.Household,
         data = merged_dma_year_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ Year | DMA),
                  na.action = na.omit)

# Generate the table using stargazer

dma_ols_covariates_models <- list(m1, m2, m3, m4, m5, m6, m7, m8)

stargazer(dma_ols_covariates_models, 
          title = "DMA Yearly GLS Regression -- MVT and Common Crime Covariates",
          omit.stat = c("rsq", "ll", "ser"),
          no.space = TRUE,
          table.placement = "!htb",
          notes.append = FALSE, 
          notes = c("* p<0.05; ** p<0.01; GT = Google Trends; MVT = Motor Vehicle Theft", "HH = Household; CD = Concentrated Disadvantage"),
          notes.align = "l",
          #se = lapply(dma_ols_covariates_models, se_robust),
          dep.var.labels=c("UCR MVT","UCR MVT","NCVS MVT", "NCVS MVT","NICB MVT", "NICB MVT","GT MVT","GT MVT"),
          column.labels = c("(0-100)", "(log)", 
                            "(0-100)", "(log)", 
                            "(0-100)", "(log)",
                            "(0-100)", "(log)"),
          covariate.labels = c(
                               "CD Index", 
                               "Mobility Index",
                               "Heterogeneity Index",
                               "\\% Foreign Born", 
                               "\\% Young Males",
                               "Population (log)",
                               "Average Vehicle HH",
                               "\\% Internet Subscription HH" ),
          column.sep.width = "-2pt",
          float.env = "sidewaystable",
          font.size = "small",
          digits = 2,
          df = F,
          header=FALSE,
          star.char = c( "*", "**"),
          star.cutoffs = c(.05, .01),
          label = "tab:ols_dma_year3")
          #type = "html",
          #out = "regression_models2.html")

```

\FloatBarrier


## DMA-Year Fixed-Effect Models 

Table \ref{tab:fix_dma_year1} provides results from DMA yearly fixed effect models that explore the influence of GT MVT on established MVT metrics such as UCR, NCVS, and NICB, utilizing both normalized (0-100) and log-transformed data. GT MVT demonstrates a consistent and significant positive relationship with UCR MVT across models (1) to (4), evident in both scaled and log-transformed formats, and regardless of the presence of control variables. Similarly, GT MVT maintains a positive and significant correlation with NICB MVT across models (9) to (12), applicable to both data formats, with or without control variables. 

Comparable outcomes for GT MVT (log) are detailed in Table \ref{tab:fix_dma_year2}. These results align closely with those from Table \ref{tab:fix_dma_year1}, confirming a robust and statistically significant association between GT MVT and both UCR MVT and NICB MVT. GT MVT shows a significant and positive association with UCR MVT and NICB MVT in both scaled and log-transformed formats. This holds true across models, with and without control variables, in models (1) to (4) and from (9) to (12). This consistent evidence underscores the strong statistical relationship between GT MVT and official MVT measures, validating the reliability of these findings even under the conservative lens of log-transformed data analysis and with control variables at the DMA-year level. This analysis addresses research question 1.1 and 1.2, which examines the linear relationship between GT MVT and other official MVT metrics, with/without control variables].

```{r DMA-fixed-effect-table1, results='asis', echo=FALSE}
# Define models
dx1 <- plm(UCR.MVT_normalized ~ GT.MVT, data = merged_dma_year_data, index=c("DMA", "Year"), model="within")
dx11 <- plm(UCR.MVT_normalized ~ GT.MVT +
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household, 
           data = merged_dma_year_data, index=c("DMA", "Year"), model="within")

dx2 <- plm(log.UCR.MVT ~ GT.MVT, data = merged_dma_year_data, index=c("DMA", "Year"), model="within")
dx21 <- plm(log.UCR.MVT ~ GT.MVT +
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household,
           data = merged_dma_year_data, index=c("DMA", "Year"), model="within")

dx3 <- plm(NCVS.MVT_normalized ~ GT.MVT, data = merged_dma_year_data, index=c("DMA", "Year"), model="within")
dx31 <- plm(NCVS.MVT_normalized ~ GT.MVT+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household,
            data = merged_dma_year_data, index=c("DMA", "Year"), model="within")

dx4 <- plm(log.NCVS.MVT ~ GT.MVT, data = merged_dma_year_data, index=c("DMA", "Year"), model="within")
dx41 <- plm(log.NCVS.MVT ~ GT.MVT+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household,
            data = merged_dma_year_data, index=c("DMA", "Year"), model="within")

dx5 <- plm(NICB.MVT_normalized ~ GT.MVT, data = merged_dma_year_data, index=c("DMA", "Year"), model="within")
dx51 <- plm(NICB.MVT_normalized ~ GT.MVT+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household,
           data = merged_dma_year_data, index=c("DMA", "Year"), model="within")

dx6 <- plm(log.NICB.MVT ~ GT.MVT, data = merged_dma_year_data, index=c("DMA", "Year"), model="within")
dx61 <- plm(log.NICB.MVT ~ GT.MVT+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household,
            data = merged_dma_year_data, index=c("DMA", "Year"), model="within")

######CALCULATE THE MODEL FITS (within) and (between)#######
# Example models (you should replace these with your actual plm model objects)
#models <- list(dx1, dx11, dx2, dx21, dx3, dx31, dx4, dx41, dx5, dx51, dx6, dx61)

# Calculate R-squared values
#r_squared_overall <- sapply(models, function(x) round(r.squared(x, model = "pooled"), 2))
#r_squared_between <- sapply(models, function(x) round(r.squared(update(x, effect = "individual", model = "between")), 2))
#r_squared_within <- sapply(models, function(x) round(r.squared(update(x, effect = "individual", model = "within")), 2))

# Prepare additional lines for stargazer
#add_lines <- list(c("Overall R$^{2}$", r_squared_overall),
                  #c("Between R$^{2}$", r_squared_between),
                  #c("Within R$^{2}$", r_squared_within))


### To accounrt for auto correlation and hetrogeneity in time series data#####
############## Heteroskedasticity consistent coefficients (Arellano)##########
##"arellano" - both heteroskedasticity and serial correlation. ###############
#### Recommended for fixed effects.###########################################
### https://www.princeton.edu/~otorres/Panel101R.pdf##########################


dxs <- list(dx1, dx11, dx2, dx21, dx3, dx31, dx4, dx41, dx5, dx51, dx6, dx61)
                                                             
stargazer(dxs,
          title = "DMA Yearly Fixed Effect Models -- Official MVT v.s. GT MVT",
          omit.stat = c("rsq", "ll", "ser"),
          no.space = TRUE,
          table.placement = "!htb",
          notes.append = FALSE, 
          notes = c("* p<0.05; ** p<0.01; GT = Google Trends; HH = Household"),
          notes.align = "l",
          se = lapply(dxs, se_robust),
          dep.var.labels=c("UCR MVT","UCR MVT",
                           "NCVS MVT", "NCVS MVT",
                           "NICB MVT", "NICB MVT"),
          column.labels = c("(0-100)", "(0-100)", "(log)", "(log)",
                            "(0-100)", "(0-100)", "(log)", "(log)",
                            "(0-100)", "(0-100)", "(log)", "(log)"),
          covariate.labels = c("GT MVT", 
                               "Population (log)", 
                               "Average Vehicle HH", 
                               "\\% Internet Subscription HH"),
          column.sep.width = "-2pt",
          font.size = "footnotesize",
          float.env = "sidewaystable",
          digits = 2,
          df = F,
          header=FALSE,
          star.char = c( "*", "**"),
          star.cutoffs = c(.05, .01),
          label = "tab:fix_dma_year1", 
          #add.lines = add_lines,
          type = "latex")
          #type = "html",
          #out = "regression_models2.html")
```


```{r DMA-fixed-effect-table2, results='asis', echo=FALSE}
# Define models
ldx1 <- plm(UCR.MVT_normalized ~ log.GT.MVT, data = merged_dma_year_data, index=c("DMA", "Year"), model="within")
ldx11 <- plm(UCR.MVT_normalized ~ log.GT.MVT +
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household, 
           data = merged_dma_year_data, index=c("DMA", "Year"), model="within")

ldx2 <- plm(log.UCR.MVT ~ log.GT.MVT, data = merged_dma_year_data, index=c("DMA", "Year"), model="within")
ldx21 <- plm(log.UCR.MVT ~ log.GT.MVT +
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household,
           data = merged_dma_year_data, index=c("DMA", "Year"), model="within")

ldx3 <- plm(NCVS.MVT_normalized ~ log.GT.MVT, data = merged_dma_year_data, index=c("DMA", "Year"), model="within")
ldx31 <- plm(NCVS.MVT_normalized ~ log.GT.MVT+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household,
            data = merged_dma_year_data, index=c("DMA", "Year"), model="within")

ldx4 <- plm(log.NCVS.MVT ~ log.GT.MVT, data = merged_dma_year_data, index=c("DMA", "Year"), model="within")
ldx41 <- plm(log.NCVS.MVT ~ log.GT.MVT+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household,
            data = merged_dma_year_data, index=c("DMA", "Year"), model="within")

ldx5 <- plm(NICB.MVT_normalized ~ log.GT.MVT, data = merged_dma_year_data, index=c("DMA", "Year"), model="within")
ldx51 <- plm(NICB.MVT_normalized ~ log.GT.MVT+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household,
           data = merged_dma_year_data, index=c("DMA", "Year"), model="within")

ldx6 <- plm(log.NICB.MVT ~ log.GT.MVT, data = merged_dma_year_data, index=c("DMA", "Year"), model="within")
ldx61 <- plm(log.NICB.MVT ~ log.GT.MVT+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household,
            data = merged_dma_year_data, index=c("DMA", "Year"), model="within")

######CALCULATE THE MODEL FITS (within) and (between)#######
# Example models (you should replace these with your actual plm model objects)
#models2 <- list(ldx1, ldx11, ldx2, ldx21, ldx3, ldx31, ldx4, ldx41, ldx5, ldx51, ldx6, ldx61)

# Calculate R-squared values
#r_squared_overall2 <- sapply(models2, function(x) round(r.squared(x, model = "pooled"), 2))
#r_squared_between2 <- sapply(models2, function(x) round(r.squared(update(x, effect = "individual", model = "between")), 2))
#r_squared_within2 <- sapply(models2, function(x) round(r.squared(update(x, effect = "individual", model = "within")), 2))

# Prepare additional lines for stargazer
#add_lines2 <- list(c("Overall R$^{2}$", r_squared_overall2),
                  #c("Between R$^{2}$", r_squared_between2),
                  #c("Within R$^{2}$", r_squared_within2))

#############
dxls <- list(ldx1, ldx11, ldx2, ldx21, ldx3, ldx31, ldx4, ldx41, ldx5, ldx51, ldx6, ldx61)

stargazer(dxls ,
          title = "DMA Yearly Fixed Effect Models -- Official MVT v.s. GT MVT (log)",
          omit.stat = c("rsq", "ll", "ser"),
          no.space = TRUE,
          table.placement = "!htb",
          notes.append = FALSE, 
          notes = c("* p<0.05; ** p<0.01; GT = Google Trends; HH = Household"),
          notes.align = "l",
          se = lapply(dxls , se_robust),
          dep.var.labels=c("UCR MVT","UCR MVT",
                           "NCVS MVT", "NCVS MVT",
                           "NICB MVT", "NICB MVT"),
          column.labels = c("(0-100)", "(0-100)", "(log)", "(log)",
                            "(0-100)", "(0-100)", "(log)", "(log)",
                            "(0-100)", "(0-100)", "(log)", "(log)"),
          covariate.labels = c("GT MVT (log)", 
                               "Population (log)", 
                               "Average Vehicle HH", 
                               "\\% Internet Subscription HH"),
          column.sep.width = "-2pt",
          font.size = "footnotesize",
          float.env = "sidewaystable",
          digits = 2,
          df = F,
          header=FALSE,
          star.char = c( "*", "**"),
          star.cutoffs = c(.05, .01),
          label = "tab:fix_dma_year2", 
          #add.lines = add_lines2,
          type = "latex")
          #type = "html",
          #out = "regression_models2.html")
```

Table \ref{tab:fix_dma_year3} analyzes the concurrent validity of GT MVT within a fixed-effect model framework, incorporating common crime covariates. The dependent variables include scaled and log-transformed measures of UCR MVT, NCVS MVT, and NICB MVT. The mobility index shows a statistically positive association with UCR MVT, GT MVT, and GT MVT (log) in model (1), (7), and (8). Conversely, the concentrated disadvantage index, while positively associated with UCR MVT (log) in model (2), exhibits a negative statistical impact on both GT MVT and GT MVT (log) in model (7) and (8). This fixed-effect analysis highlights the mobility index as a consistent predictor for both UCR MVT and GT MVT, whereas the CD Index has a positive effect on UCR MVT but a negative impact on GT MVT. This divergence underscores the importance of utilizing various resources and methods to test theories, emphasizing that reliance on a single type of data source and statistical test is insufficient. The result of the concurrent validity for GT MVT at the DMA-year level is mixed, and needs further investigation. This analysis addresses research question 2.1, which explores the impact of common crime covariates on GT MVT and other MVT metrics. 


```{r DMA-fixed-effect-table3, results='asis', echo=FALSE}
# Define models using common covariates directly in the formula
x7 <- plm(UCR.MVT_normalized ~ Concentrated.Disadvantaged.Index + 
             Mobility.Index + Heterogeneity.Index + 
             Percentage.of.Foreign.Born + Percentage.of.Young.Males+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household, 
           data = merged_dma_year_data, index=c("DMA", "Year"), model="within")



x71 <- plm(log.UCR.MVT ~ Concentrated.Disadvantaged.Index + 
             Mobility.Index + Heterogeneity.Index + 
             Percentage.of.Foreign.Born + Percentage.of.Young.Males+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household, 
           data = merged_dma_year_data, index=c("DMA", "Year"), model="within")




############################################################################



x9 <- plm(NCVS.MVT_normalized ~ Concentrated.Disadvantaged.Index + 
             Mobility.Index + Heterogeneity.Index + 
             Percentage.of.Foreign.Born + Percentage.of.Young.Males+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household, 
           data = merged_dma_year_data, index=c("DMA", "Year"), model="within")


x91 <- plm(log.NCVS.MVT ~ Concentrated.Disadvantaged.Index + 
             Mobility.Index + Heterogeneity.Index + 
             Percentage.of.Foreign.Born + Percentage.of.Young.Males+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household, 
           data = merged_dma_year_data, index=c("DMA", "Year"), model="within")



##############################################################################

x11 <- plm(NICB.MVT_normalized ~ Concentrated.Disadvantaged.Index + 
             Mobility.Index + Heterogeneity.Index + 
             Percentage.of.Foreign.Born + Percentage.of.Young.Males+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household, 
           data = merged_dma_year_data, index=c("DMA", "Year"), model="within")



x111 <- plm(log.NICB.MVT ~ Concentrated.Disadvantaged.Index + 
             Mobility.Index + Heterogeneity.Index + 
             Percentage.of.Foreign.Born + Percentage.of.Young.Males+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household, 
           data = merged_dma_year_data, index=c("DMA", "Year"), model="within")



x13 <- plm(GT.MVT ~ Concentrated.Disadvantaged.Index + 
             Mobility.Index + Heterogeneity.Index + 
             Percentage.of.Foreign.Born + Percentage.of.Young.Males+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household, 
           data = merged_dma_year_data, index=c("DMA", "Year"), model="within")

x14 <- plm(log.GT.MVT ~ Concentrated.Disadvantaged.Index + 
             Mobility.Index + Heterogeneity.Index + 
             Percentage.of.Foreign.Born + Percentage.of.Young.Males+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household, 
           data = merged_dma_year_data, index=c("DMA", "Year"), model="within")

######CALCULATE THE MODEL FITS (within) and (between)#######
# Example models (you should replace these with your actual plm model objects)
#models3 <- list(x7,x8,x71,x81,x9,x10,x91,x101,x11,x12,x111,x121,x13,x14)

# Calculate R-squared values
#r_squared_overall3 <- sapply(models3, function(x) round(r.squared(x, model = "pooled"), 2))
#r_squared_between3 <- sapply(models3, function(x) round(r.squared(update(x, effect = "individual", model = "between")), 2))
#r_squared_within3 <- sapply(models3, function(x) round(r.squared(update(x, effect = "individual", model = "within")), 2))

# Prepare additional lines for stargazer
#add_lines3 <- list(c("Overall R$^{2}$", r_squared_overall3),
                  #c("Between R$^{2}$", r_squared_between3),
                  #c("Within R$^{2}$", r_squared_within3))

#############



# Generate the table using stargazer

xmodellist <- list(x7,x71,x9,x91,x11,x111,x13,x14)

stargazer(xmodellist,
          title = "DMA Yearly Fixed Effect Models -- MVT and Common Crime Covariates",
          omit.stat = c("rsq", "ll", "ser"),
          no.space = TRUE,
          table.placement = "!htb",
          notes.append = FALSE, 
          notes = c("* p<0.05; ** p<0.01; GT = Google Trends; MVT = Motor Vehicle Theft; HH = Household; CD = Concentrated Disadvantage"),
          notes.align = "l",
          se = lapply(xmodellist , se_robust),
          dep.var.labels=c("UCR MVT", "UCR MVT", 
                           "NCVS MVT", "NCVS MVT", 
                           "NICB MVT", "NICB MVT",  
                           "GT MVT", "GT MVT"),
          column.labels = c("(0-100)",  "(log)", 
                            "(0-100)",  "(log)",  
                            "(0-100)",  "(log)", 
                            " ", "(log)"),
          covariate.labels = c("CD Index", "Mobility Index",
                               "Heterogeneity Index", "\\% Foreign Born", 
                                "\\% Young Males",
                                 "Population (log)", "Average Vehicle HH", 
                               "\\% Internet Subscription HH"),
          column.sep.width = "-2pt",
          font.size = "footnotesize",
          digits = 2,
          float.env = "sidewaystable",
          df = F,
          header=FALSE,
          star.char = c( "*", "**"),
          star.cutoffs = c(.05, .01),
          label = "tab:fix_dma_year3", 
          #add.lines = add_lines2,
          type = "latex")
          #type = "html",
          #out = "regression_models2.html")


```

\FloatBarrier

# DMA-Month Analysis

Table \ref{tab:desc_dma_month} presents the data utilized in the subsequent DMA-month analysis. The data points span from 2017 to 2022 by month. As a result, we have data for 72 months across different DMA. Due to sample size limitations, I was able to pull data from 44 DMA with meaningful data from Google Trends, totaling 3,168 data points. Both GT MVT and UCR MVT have the same sample size of 3,168, ensuring consistency in the initial data merging process^[^[For the full list of DMAs, please see \ref{tab:dmamonthlist}.]]. Subsequent merges include NIBRS (N=2,592) and CFS 911 (N=1,080), followed by weather data (N=2,592), and finally, CPI and PPI data (N=1,296).

GT MVT is pulled from Google Trends for the years 2017 to 2022 across all DMA. The sample size of 3,168 represents 44 DMA over 72 months. This data is normalized from 0 to 100 by each DMA across all 72 months. The average GT MVT stands at 38.27, with a standard deviation of 22.35. GT MVT (log) is the logarithmically transformed GT MVT. The data averages to 3.44, with a smaller standard deviation of 0.80.

UCR MVT is the MVT statistics from UCR, representing the motor vehicle theft rate (per 100,000 population) by DMA by month. It has a sample size of 3,168, corresponding to the 44 DMA with GT over 72 months. It has a mean of 24.43 and a standard deviation of 14.55. Normalized UCR MVT (UCR MVT (0-100)) scales the UCR MVT data to a 0-100 range for each DMA across a 72-month span, averaging 39.36 with a standard deviation of 25.40. It is normalized by each month across DMA. UCR MVT (log) is the logarithmic transformation of UCR MVT data, yielding an average of 3.02 with a 0.77 standard deviation.

CFS 911 MVT represents the motor vehicle theft data from the Call for Service 911 records in different cities. It represents the MVT rate (per 100,000 population) by DMA by month from CFS 911 data. It has a sample size of 1,080, representing the available data for 15 DMA over 72 months, from 2017 to 2022. CFS 911 MVT has a mean of 44.33, a standard deviation of 51.87. Normalized CFS 911 MVT (CFS 911 MVT (0-100)) is scaled to a 0-100 range within each DMA across its 72 months, with a mean of 50.25, a standard deviation of 24.39. CFS 911 MVT (log) is the log-transformed CFS 911 MVT, with an average of 3.38, a standard deviation of 0.93.

NIBRS MVT is the motor vehicle theft data from the National Incident-Based Reporting System. It has a sample size of 2,592, representing 36 DMA with GT over 72 months. It has a mean of 15.37, a standard deviation of 13.16. Normalized NIBRS MVT (NIBRS MVT (0-100)) scales this data to a 0-100 range for each DMA across a 72-month span, with a mean of 37.47, a standard deviation of 30.64. NIBRS MVT (log) is the log-transformed NIBRS MVT, with an average of 2.32, a standard deviation of 1.14.

Precipitation and temperature data are also included in the table. Precipitation has a sample size of 2,592, with a mean of 162.65 inches, a standard deviation of 569.80, and ranges from 0.00 to 8,630.91 inches. Temperature also has a sample size of 2,592, with a mean of 57.92°F, a standard deviation of 16.97, and ranges from 5.88°F to 89.48°F.

Car Consumer Price Index (Car CPI) and Semiconductor Producer Price Index (Semiconductor PPI) are included as well. Data for Car CPI is available across 17 DMA, resulting in a total sample size of 1,296 (18 DMA x 72 months), with a mean of 106.36, a standard deviation of 13.92, and ranges from 86.25 to 145.31. Semiconductor PPI has a sample size of 1,296, with a mean of 55.59, a standard deviation of 0.85, and ranges from 53.80 to 57.61.

As aforementioned in the methodology section, unlike GT-DMA-Yearly or GT-State-Yearly data, which are standardized across different geographic areas within a specific year, the monthly DMA data is standardized on a monthly basis inside the same geographic location. In the following analysis, both normalized and non-normalized data formats are presented in the statistic tests. 

```{r, results = "asis", message=FALSE, warning = FALSE, echo = FALSE}
df3 = year_month_merged_data[c("GT.MVT","log.GT.MVT", 
                               "UCR.MVT","UCR.MVT_normalized","log.UCR.MVT",
                               "CFS.911.MVT", "CFS.911.MVT_normalized","log.CFS.911.MVT",
                               "NIBRS.MVT", "NIBRS.MVT_normalized","log.NIBRS.MVT",
                               "PRCP", "TAVG", "Car.CPI", "Semiconductor.PPI")]


stargazer(df3, type = "latex", header = FALSE, table.placement = "!htb",
 title="Descriptive Statistics of DMA-Monthly Data", digits=2, notes = c("HH = Household"),
 covariate.labels=c("GT MVT","GT MVT (log)",
                    "UCR MVT", "UCR MVT (0-100)", "UCR MVT (log)",
                    "CFS 911 MVT", "CFS 911 MVT (0-100)", "CFS 911 MVT (log)", 
                    "NIBRS MVT",  "NIBRS MVT (0-100)", "NIBRS MVT (log)",
                    "Precipitation (inch)", "Temperature (°F)", "Car CPI",
                    "Semiconductor PPI"),
 label = "tab:desc_dma_month")
```

```{r, results = "hide", message=FALSE, warning = FALSE, echo = FALSE}
summary_by_dma_m <- year_month_merged_data %>%
  group_by(DMA) %>%
  summarize_all(list(mean = ~ mean(., na.rm = TRUE)))

# Select only the columns of interest
summary_by_dma_m <- summary_by_dma_m %>%
  dplyr::select(DMA, GT.MVT_mean, UCR.MVT_normalized_mean, NIBRS.MVT_normalized_mean, CFS.911.MVT_normalized_mean)


summary_by_year_month_m <- year_month_merged_data %>%
  group_by(year_month) %>%
  summarize_all(list(mean = ~ mean(., na.rm = TRUE)))
summary_by_year_month_m
# Select only the columns of interest
summary_by_year_month_m <- summary_by_year_month_m %>%
  dplyr::select(year_month, GT.MVT_mean, UCR.MVT_normalized_mean, NIBRS.MVT_normalized_mean, CFS.911.MVT_normalized_mean)

```




```{r, results = "hide", message=FALSE, warning = FALSE, echo = FALSE, header=FALSE}

summary_by_year_month_m

pdma_month_all <- ggplot(summary_by_year_month_m, aes(x = year_month)) +
  geom_line(aes(y = GT.MVT_mean, colour = "GT MVT"), size = 1) +
  geom_line(aes(y = UCR.MVT_normalized_mean, colour = "UCR MVT"), size = 1) +
  geom_line(aes(y = CFS.911.MVT_normalized_mean, colour = "CFS 911 MVT"), size = 1) +
  geom_line(aes(y = NIBRS.MVT_normalized_mean, colour = "NIBRS MVT"), size = 1) +
  geom_vline(xintercept = as.numeric(as.Date("2020-04-01")), color = "firebrick", linetype = "dashed", ) +
  annotate("text", x = as.Date("2020-04-01"), y = 75, label = "COVID-19 Lockdown", hjust = 0.5, color = "firebrick", size = 3) +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  scale_color_manual(values = c("GT MVT" = "#CC79A7", 
                                "UCR MVT" =  "#0072B2",
                                "CFS 911 MVT" = "#E69F00",
                                "NIBRS MVT" = "#56B4E9"),
                     breaks = c("GT MVT", "UCR MVT", 
                                "CFS 911 MVT", "NIBRS MVT")) +
  labs(title = "Aggregated DMA-Month MVT Statistics, By Year-Month",
       x = "Year-Month",
       y = "Normalized Value",
       colour = "Data Source") +
  scale_x_date(limits = c(as.Date("2017-01-01"), 
                          as.Date("2022-12-01")), 
               date_labels = "%Y-%m", 
               date_breaks = "1 month") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 55, hjust = 1, size = 5))


ggsave("dma_month_aggregated.png", pdma_month_all, width = 10, height = 8, bg = "white") 

```


\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{selected_mvt_year_month_with_911_plot.png}
  \caption{Motor Vehicle Theft Statistics by DMA and Month (Selected DMA)}
  \label{fig:figure 7.3}
\end{figure}


\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{dma_month_aggregated.png}
  \caption{Aggregated Motor Vehicle Theft Statistics Across All DMAs by Year-Month}
  \label{fig:figure 7.3.1}
\end{figure}

Figure \ref{fig:figure 7.3} showcases selected visualizations of 0-100 scaled DMA-month data for Houston, Los Angeles, New York, and Seattle-Tacoma. The full set of scaled MVT metrics comparisons, which includes 44 DMA in this DMA-month analysis, is available in Figure \ref{fig:figure A.4}. The aggregated MVT statistics across all DMAs by year-month can be found in Figure \ref{fig:figure 7.3.1}. The figures generally show parallel trends between GT, UCR, CFS 911, and NIBRS in motor vehicle theft by month across DMA. This suggests that these MVT data sources tend to capture similar patterns of changes in motor vehicle theft rates over time on a 0-100 scale by DMA-month. Additionally, vertical dashed lines have been included to mark the starting point of the COVID-19 lockdown in April 2020. It reveals significant shifts or anomalies in the trends during these times, including a general increase in MVT following the lockdown. This visualization effectively illustrates the utility of multiple data sources in tracking and analyzing crime patterns while also highlighting the impact of external events on crime statistics. However, there are notable divergences at specific points and in certain locations. Note that the divergence or overlap might not fully capture the complexities of the data. It is also essential to consider factors like autocorrelation and heteroskedasticity in time-series data. Therefore, a range of statistical tests was employed to further investigate the relationship between GT MVT and other official MVT statistics.

\FloatBarrier

## DMA-Month Correlation

Table \ref{dma_month_cor_table} provides insights into the relationships between GT MVT and other crime data sources such as UCR MVT, CFS 911 MVT, and NIBRS MVT. GT MVT shows a weak to moderate positive relationship with UCR MVT, with correlation coefficients ranging from 0.144 to 0.297. Notably, normalization of official crime statistics leads to an overall increase in correlation coefficients with GT MVT. For UCR MVT, it increases slightly from 0.212 to 0.297; for CFS 911, it improves from -0.035 to 0.037; for NIBRS MVT, it increases from 0.136 to 0.213.

The highest correlation coefficient (0.521) is observed between normalized NIBRS and raw UCR MVT data. The correlation coefficients between UCR and NIBRS, both in raw and scaled forms, range from 0.120 to 0.521, suggesting moderate consistency between these datasets in capturing crime trends. Conversely, the correlations between GT MVT and CFS 911 MVT are weak and, in some cases, not statistically significant. From the correlation tests, I find only a weak to moderate positive relationship between GT MVT and other crime statistics. Moreover, a decreasing trend in correlation coefficients is observed between GT MVT and other official crime statistics as the geographical location becomes more specific and the time unit decreases in scale (from state to DMA and from year to month). This test addresses research questions 1.1 and 1.2, examining the linear relationship between GT MVT and other crime statistics across time and space.


```{r DMA-monthly-correlation-table, results='asis', echo=FALSE, message=FALSE, warning = FALSE}

# Selecting the specified variables from the dataframe
dma_month_variables <- c("GT.MVT","log.GT.MVT", 
                         "UCR.MVT","UCR.MVT_normalized","log.UCR.MVT",
                         "CFS.911.MVT", "CFS.911.MVT_normalized","log.CFS.911.MVT",
                         "NIBRS.MVT", "NIBRS.MVT_normalized","log.NIBRS.MVT")
dma_month_variable_name <- c("GT MVT","GT MVT (log)",
                            "UCR MVT", "UCR MVT (0-100)", "UCR MVT (log)",
                            "CFS 911 MVT", "CFS 911 MVT (0-100)", "CFS 911 MVT (log)", 
                            "NIBRS MVT",  "NIBRS MVT (0-100)", "NIBRS MVT (log)")
dma_month_cor_table <- corstars(year_month_merged_data[dma_month_variables],
                                dma_month_variable_name)


#print latex table
final_dma_month_corr_table <- kable(dma_month_cor_table, longtable = T, 
                                    booktabs = T, align = "l",
                                    format = "latex", linesep = "",
    caption = 'Correlation Table of DMA Level MVT, Monthly, 2017 - 2022\\label{dma_month_cor_table}') %>%
  kable_styling(latex_options="scale_down", font_size = 10,
                #position = "float_left", 
                bootstrap_options = "striped",
                full_width = F) %>%
  footnote("** = p < .01, * = p < .05; GT = Google Trends; MVT = Motor Vehicle Theft", footnote_as_chunk = T, threeparttable = T)

cat("\\begin{sidewaystable}[!h]\n")
print(final_dma_month_corr_table)
cat("\\end{sidewaystable}\n")

```


\FloatBarrier

## DMA-Month GLS Models

Table \ref{tab:dma_month_ols} presents the results from the DMA Monthly GLS Models, underscore the robust statistical relationships between GT MVT and UCR MVT, CFS 911 MVT, and NIBRS MVT. Both the raw and logarithmic transformations of GT MVT are examined. In the models from (1) to (12), all GT MVT and log-transformed GT MVT exhibits a positive linear relationship with UCR MVT, CFS 911 MVT, and NIBRS MVT, all significant at 0.01 level, except for the CFS 911 MVT (log). This consistent result reveals positive and statistically associations between GT MVT and other official crime statistics. These findings suggest that GT MVT data can serve as a reliable predictor alongside official crime statistics such as UCR, CFS 911, and NIBRS at the DMA-month level. This analysis addresses research questions 1.1 and 1.2, which explore the linear relationship between GT MVT and other official crime statistics across time and space.


```{r DMA-monthly-ols-table, results='asis', echo=FALSE}
year_month_merged_data <- as.data.frame(year_month_merged_data)
year_month_merged_data_for_lm <- year_month_merged_data

year_month_merged_data_for_lm <- year_month_merged_data_for_lm %>%
  arrange(DMA, year_month)
year_month_merged_data_for_lm$year_month <- as.integer(year_month_merged_data_for_lm$year_month)
year_month_merged_data_for_lm$DMA <- as.factor(year_month_merged_data_for_lm$DMA)
```





```{r DMA-monthly-ols-table-2, results='asis', echo=FALSE}
# Ensure 'State' is a factor
# Define models
dmo1 <- gls(UCR.MVT_normalized ~ GT.MVT, data =year_month_merged_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)

dmo2 <- gls(UCR.MVT_normalized ~ log.GT.MVT, data =year_month_merged_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)

dmo3 <- gls(log.UCR.MVT ~ GT.MVT, data =year_month_merged_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)

dmo4 <- gls(log.UCR.MVT ~ log.GT.MVT, data =year_month_merged_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)

dmo5 <- gls(CFS.911.MVT_normalized ~ GT.MVT, data =year_month_merged_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)

dmo6 <- gls(CFS.911.MVT_normalized ~ log.GT.MVT, data =year_month_merged_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)

dmo7 <- gls(log.CFS.911.MVT ~ GT.MVT, data =year_month_merged_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)

dmo8 <- gls(log.CFS.911.MVT ~ log.GT.MVT, data =year_month_merged_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)

dmo9 <- gls(NIBRS.MVT_normalized ~ GT.MVT, data =year_month_merged_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)

dmo10 <- gls(NIBRS.MVT_normalized ~ log.GT.MVT, data =year_month_merged_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)

dmo11 <- gls(log.NIBRS.MVT ~ GT.MVT, data =year_month_merged_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)

dmo12 <- gls(log.NIBRS.MVT ~ log.GT.MVT, data =year_month_merged_data_for_lm,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)



stargazer(dmo1, dmo2, dmo3, dmo4, dmo5, dmo6, 
          dmo7, dmo8, dmo9, dmo10, dmo11, dmo12,
          title = "DMA Monthly GLS Models -- Official MVT v.s. GT MVT",
          omit.stat = c("rsq", "ll", "ser"),
          no.space = TRUE,
          table.placement = "!htb",
          notes.append = FALSE, 
          notes = c("* p<0.05; ** p<0.01; GT = Google Trends; MVT = Motor Vehicle Theft"),
          notes.align = "l",
          dep.var.labels=c("UCR MVT", "UCR MVT", 
                           "CFS 911 MVT", "CFS 911 MVT", 
                           "NIBRS MVT", "NIBRS MVT"),
          column.labels = c("(0-100)", "(0-100)", "(log)", "(log)", 
                            "(0-100)", "(0-100)", "(log)", "(log)",  
                            "(0-100)", "(0-100)", "(log)", "(log)"),
          covariate.labels = c("GT MVT", "GT MVT (log)"),
          column.sep.width = "2pt",
          font.size = "scriptsize",
          float.env = "sidewaystable",
          digits = 2,
          df = F,
          header=FALSE,
          star.char = c( "*", "**"),
          star.cutoffs = c(.05, .01),
          label = "tab:dma_month_ols",
          type = "latex")
          #out = "regression_models2.html")

```
\FloatBarrier

## DMA-Month Fixed Effect Models

Table \ref{tab:dma_month_fixed_effect} presents the results from DMA monthly fixed effect models analyzing the linear relationship between GT MVT and official crime statistics, such as UCR, NIBRS, and CFS 911. After using only within-unit variation, the findings indicate that both GT MVT and the logarithmic forms of GT MVT positively and statistically significantly associate with these crime statistics from model (1) to (12). This fixed effect analysis reaffirms the utility of Google Trends as a complementary tool for analyzing motor vehicle theft at the DMA-month level. This analysis addresses research questions 1.1 and 1.2, which examine the linear relationship between GT MVT and other official crime statistics across time and space.


```{r DMA-monthly-fixed-effect-table2, results='asis', echo=FALSE}

year_month_merged_data <- as.data.frame(year_month_merged_data)

# Define models
dm1 <- plm(UCR.MVT_normalized ~ GT.MVT, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

dm2 <- plm(UCR.MVT_normalized ~ log.GT.MVT, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

dm3 <- plm(log.UCR.MVT ~ GT.MVT, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

dm4 <- plm(log.UCR.MVT ~ log.GT.MVT, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

dm5 <- plm(CFS.911.MVT_normalized ~ GT.MVT, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

dm6 <- plm(CFS.911.MVT_normalized ~ log.GT.MVT, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

dm7 <- plm(log.CFS.911.MVT ~ GT.MVT, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

dm8 <- plm(log.CFS.911.MVT ~ log.GT.MVT, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

dm9 <- plm(NIBRS.MVT_normalized ~ GT.MVT, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

dm10 <- plm(NIBRS.MVT_normalized ~ log.GT.MVT, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

dm11 <- plm(log.NIBRS.MVT ~ GT.MVT, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

dm12 <- plm(log.NIBRS.MVT ~ log.GT.MVT, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

dmonth_models <- list(dm1, dm2, dm3, dm4, dm5, dm6, dm7, dm8, dm9, dm10, dm11, dm12)

stargazer(dmonth_models,
          title = "DMA Monthly Fixed Effect Models -- Official MVT v.s. GT MVT",
          omit.stat = c("rsq", "ll", "ser"),
          no.space = TRUE,
          table.placement = "!htb",
          notes.append = FALSE, 
          notes = c("* p<0.05; ** p<0.01; GT = Google Trends; MVT = Motor Vehicle Theft"),
          notes.align = "l",
          se = lapply(dmonth_models , se_robust),
          dep.var.labels=c("UCR MVT", "UCR MVT", 
                           "CFS 911 MVT", "CFS 911 MVT", 
                           "NIBRS MVT", "NIBRS MVT"),
          column.labels = c("(0-100)", "(0-100)", "(log)", "(log)", 
                            "(0-100)", "(0-100)", "(log)", "(log)",  
                            "(0-100)", "(0-100)", "(log)", "(log)"),
          covariate.labels = c("GT MVT", "GT MVT (log)"),
          column.sep.width = "2pt",
          font.size = "footnotesize",
          float.env = "sidewaystable",
          digits = 2,
          df = F,
          header=FALSE,
          star.char = c( "*", "**"),
          star.cutoffs = c(.05, .01),
          label = "tab:dma_month_fixed_effect",
          type = "latex")
          #out = "regression_models2.html")

```


Table \ref{tab:dma_month_fixed_effect_covariates} evaluates the concurrent validity of GT MVT and examines how common crime covariates such as temperature and precipitation interact with these data sources. In models (2), (4), (6), (8) and (10), The models demonstrate that GT MVT is positively and statistically significantly associated with UCR MVT, CFS 911 MVT, and NIBRS MVT across both normalized and log-transformed forms when other crime covariates are included. This result indicates a robust and consistent association with other motor vehicle theft statistics at DMA-month level. 

Temperature is positively associated with all forms of UCR MVT, CFS 911 MVT and NIBRS MVT, except for the CFS 911 MVT (log) and NIBRS MVT (log), suggesting that higher temperatures may correlate with increases in motor vehicle theft incidents from model (11) to (12) Similarly, precipitation is significantly positively associated with UCR MVT, NIBRS MVT, and CFS 911 in both normalized and logarithmic forms, except for GT MVT. 

The non-significant results from GT MVT in relation to common crime covariates indicate no association between temperature and Google search behavior for terms related to MVT. Further study is required before employing GT MVT in DMA-monthly data to explore common crime covariates for theoretical testing. This analysis highlights the usefulness of integrating Google Trends and situational data into crime analysis frameworks, providing a more comprehensive understanding of the dynamics influencing crime patterns at the DMA-month level. Overall, we cannot confirm the concurrent validity of GT MVT at the DMA-month level. This analysis addresses research question 2.1 regarding the concurrent validity of GT MVT.



```{r DMA-monthly-fixed-effect-table3, results='asis', echo=FALSE}
# Define models using common covariates directly in the formula
ddm4 <- plm(UCR.MVT_normalized ~ TAVG + PRCP, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")
ddm5 <- plm(UCR.MVT_normalized ~ GT.MVT + TAVG + PRCP, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

ldm4 <- plm(log.UCR.MVT ~ TAVG + PRCP, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")
ldm5 <- plm(log.UCR.MVT ~ GT.MVT + TAVG + PRCP, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

######################################################
ddm6 <- plm(CFS.911.MVT_normalized ~ TAVG + PRCP, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")
ddm7 <- plm(CFS.911.MVT_normalized ~ GT.MVT + TAVG + PRCP, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")
ldm6 <- plm(log.CFS.911.MVT ~ TAVG + PRCP, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")
ldm7 <- plm(log.CFS.911.MVT ~ GT.MVT + TAVG + PRCP, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

######################################################

ddm8 <- plm(NIBRS.MVT_normalized ~ TAVG + PRCP, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")
ddm9 <- plm(NIBRS.MVT_normalized ~ GT.MVT + TAVG + PRCP, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")
ldm8 <- plm(log.NIBRS.MVT ~ TAVG + PRCP, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")
ldm9 <- plm(log.NIBRS.MVT ~ GT.MVT + TAVG + PRCP, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")
#########################################################

ddm10 <- plm(GT.MVT ~ TAVG + PRCP, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

ldm10 <- plm(log.GT.MVT ~ TAVG + PRCP, data =year_month_merged_data, index=c("DMA", "year_month"), model="within")

# Generate the table using stargazer

dmonth_models_log <- list(ddm4, ddm5, ldm4, ldm5, ddm6, ddm7, ldm6, ldm7,
                           ddm8, ddm9, ldm8, ldm9, ddm10, ldm10)

stargazer(dmonth_models_log,
          title = "DMA Monthly Fixed Effect Models -- MVT and Common Crime Covariates",
          omit.stat = c("rsq", "ll", "ser"),
          no.space = TRUE,
          table.placement = "!htb",
          notes.append = FALSE, 
          notes = "* p<0.05; ** p<0.01; GT = Google Trends; MVT = Motor Vehicle Theft",
          notes.align = "l",
          se = lapply(dmonth_models_log , se_robust),
          dep.var.labels=c("UCR MVT", "UCR MVT", 
                           "CFS 911 MVT", "CFS 911 MVT", 
                           "NIBRS MVT", "NIBRS MVT",
                           "GT MVT", "GT MVT"),
          column.labels = c("(0-100)", "(0-100)", "(log)", "(log)", 
                            "(0-100)", "(0-100)", "(log)", "(log)",  
                            "(0-100)", "(0-100)", "(log)", "(log)",
                            "(0-100)", "(log)"),
          covariate.labels = c("GT MVT", 
                               "Precipitation(inch)", "Temperature(°F)"),
          column.sep.width = "2pt",
          font.size = "scriptsize",
          digits = 2,
          df = F,
          header=FALSE,
          star.char = c( "*", "**"),
          star.cutoffs = c(.05, .01),
          label = "tab:dma_month_fixed_effect_covariates",
          type = "latex",
          float.env = "sidewaystable")
          #type = "html",
          #out = "regression_models2.html")


```
\FloatBarrier


## DMA-Month Instrumental Variables Method


This instrumental variable test assesses the impact of the Car Price Index (Car CPI) on various motor vehicle theft metrics, including UCR MVT, CFS 911 MVT, NIBRS MVT, and GT MVT, presented in both scaled and logarithmic forms. These models utilize the Consumer Price Index for new and used cars, instrumented by the semiconductor price Producer Price Index (PPI), to control for potential endogeneity, ensuring the observed correlations are attributable to the influence of car prices rather than unobserved confounding factors. The selected visualization is presented in Figure \ref{fig:figure 7.4}, while the complete graph, which includes 18 DMAs with available Car CPI and semiconductor PPI data, can be found in Appendix Figure \ref{fig:figure A.5}. The overall trends of GT MVT correlate with car prices and align with other official crime statistics, particularly noting a rapid increase following the COVID-19 lockdown. Additional robust statistical tests are required to confirm their linear relationship.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{selected_mvt_cpi_month_dma.png}
  \caption{Monthly MVT and CPI by DMA with Smoothed Trend Lines (Selected DMA)}
  \label{fig:figure 7.4}
\end{figure}

For the instrumental variable to be effective, one of the assumptions is that the instrumental variable $z$ has an effect on $x$. Here, the effect of semiconductor price on car price is presented in Table \ref{tab:iv_test1}. I observe a significant association. For a one-unit increase in the Semiconductor PPI, we expect a 5.24 unit increase in the Car CPI. This confirms the relationship of $z$ to $x$.

Table \ref{tab:iv_table} demonstrates that Car CPI consistently exhibits a significant positive relationship with normalized MVT, with the exception of CFS 911, in model (1), (5), (7) and (8). Specifically, UCR MVT (100) in model (1) shows a significant and positive coefficient of 0.84, indicating that every unit increase in car prices is associated with 0.84 unit change in UCR MVT. Similarly, in model (5), NIBRS MVT (100) shows a significant and positive coefficient of 0.92, suggesting that every unit increase in car price is associated with a 0.92 point increase in NIBRS MVT. Car prices also have a significant positive impact on GT MVT in both normalized and log-transformed formats, with the coefficients of 1.13 and 0.03, confirming the concurrent validity of GT MVT at the DMA-month level. All IV models passed the weak Instrument test (p < 0.05), affirming the effect of $z$ on $x$. For the Wu-Hausman test, all p-values are also less than 0.05, indicating that the IV model provides a better fit than the OLS model.

Overall, these models highlight the significant role that economic factors, such as car prices, play in influencing MVT rates and Google search behaviors related to vehicle theft. The consistent and robust statistical evidence across various data forms and models underscores the complex relationship between economic indicators and crime trends. Increases in car prices not only correlate with more vehicle thefts but also with heightened online search activity for MVT-related terms. This analysis confirms the concurrent validity of GT MVT at the DMA-month level. This research addresses research question 3.3, examining the effect of car prices on GT MVT and other crime covariates to further confirm the concurrent validity of GT MVT.





```{r}
iv_df <- year_month_merged_data[!is.na(year_month_merged_data$Car.CPI), ]
```


```{r}
test_z_and_x_1 <- plm(Car.CPI ~ Semiconductor.PPI,
                    data = iv_df,
                    model = "random")

test_z_and_x_2 <- plm(Car.CPI ~ Semiconductor.PPI,
                    data = iv_df,
                    model = "within")

test_z_and_x_3 <- gls(Car.CPI ~ Semiconductor.PPI,
                    data = iv_df,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)


```


```{r, results='asis', echo=FALSE}
test_z_list <- list(test_z_and_x_1, test_z_and_x_2,test_z_and_x_3)


stargazer(test_z_and_x_1, test_z_and_x_2,test_z_and_x_3,
          title = "DMA Monthly Instrumental Variable Models Assumption Test -- Semiconductor Price on Car Price",
          omit.stat = c("rsq", "ll", "ser"),
          no.space = TRUE,
          table.placement = "!htb",
          notes.append = FALSE, 
          notes = c("* p<0.05; ** p<0.01; RE = Random Effect;", 
                    "FE = Fixed Effect"),
          notes.align = "l",
          #se = lapply(test_z_list, se_robust),
          dep.var.labels=c("Car CPI"),
          column.labels = c("(RE)", "(FE)", "(GLS)"),
          covariate.labels = c("Semiconductor PPI"),
          column.sep.width = "2pt",
          font.size = "footnotesize",
          digits = 2,
          df = F,
          header=FALSE,
          star.char = c( "*", "**"),
          star.cutoffs = c(.05, .01),
          label = "tab:iv_test1",
          type = "latex")
          #type = "html",
          #out = "regression_models2.html")
```


```{r}
# Correcting the model specification for Hausman-Taylor estimation
library(ivreg)
iv_model_ucr <- ivreg(UCR.MVT_normalized ~ Car.CPI | Semiconductor.PPI,
                    data = iv_df)

iv_model_ucrl <- ivreg(log.UCR.MVT ~ Car.CPI | Semiconductor.PPI,
                    data = iv_df)
#######################################################################
iv_model_nibrs <- ivreg(NIBRS.MVT_normalized ~ Car.CPI | Semiconductor.PPI,
                    data = iv_df)

iv_model_nibrsl <- ivreg(log.NIBRS.MVT ~ Car.CPI | Semiconductor.PPI,
                    data = iv_df)
#######################################################################
iv_model_911 <- ivreg(CFS.911.MVT_normalized ~ Car.CPI | Semiconductor.PPI,
                    data = iv_df)

iv_model_911l <- ivreg(log.CFS.911.MVT ~ Car.CPI | Semiconductor.PPI,
                    data = iv_df)
#######################################################################

iv_model_gt <- ivreg(GT.MVT ~ Car.CPI | Semiconductor.PPI,
                    data = iv_df)

iv_model_gtl <- ivreg(log.GT.MVT ~ Car.CPI | Semiconductor.PPI,
                    data = iv_df)
###############################################


alt_iv_model_ucr <- ivreg(GT.MVT ~ UCR.MVT_normalized | Car.CPI,
                    data = iv_df)

alt_iv_model_ucrl <- ivreg(log.GT.MVT ~ log.UCR.MVT | Car.CPI,
                    data = iv_df)


alt_iv_model_911 <- ivreg(GT.MVT ~ CFS.911.MVT_normalized | Car.CPI,
                    data = iv_df)

alt_iv_model_911l <- ivreg(log.GT.MVT ~ log.CFS.911.MVT | Car.CPI,
                    data = iv_df)

alt_iv_model_nibrs <- ivreg(GT.MVT ~ NIBRS.MVT_normalized | Car.CPI,
                    data = iv_df)

alt_iv_model_nibrsl <- ivreg(log.GT.MVT ~ log.NIBRS.MVT | Car.CPI,
                    data = iv_df)

```


```{r, results='asis', echo=FALSE}
iv_models_list <- list(iv_model_ucr, iv_model_ucrl, 
          iv_model_911, iv_model_911l,
          iv_model_nibrs, iv_model_nibrsl,
          iv_model_gt, iv_model_gtl)

#Report robust errors and the results of diagnose tests for iv
iv_se_robust <- function(x) coeftest(x, vcovHC(x, type="HC0"))[, "Std. Error"]

```


```{r, results='asis', echo=FALSE}

summ.fit1 <- summary(iv_model_ucr, vcov. = function(x) vcovHC(x, type="HC0"), diagnostics=T)
summ.fit2 <- summary(iv_model_ucrl, vcov. = function(x) vcovHC(x, type="HC0"), diagnostics=T)
summ.fit3 <- summary(iv_model_911, vcov. = function(x) vcovHC(x, type="HC0"), diagnostics=T)
summ.fit4 <- summary(iv_model_911l, vcov. = function(x) vcovHC(x, type="HC0"), diagnostics=T)
summ.fit5 <- summary(iv_model_nibrs, vcov. = function(x) vcovHC(x, type="HC0"), diagnostics=T)
summ.fit6 <- summary(iv_model_nibrsl, vcov. = function(x) vcovHC(x, type="HC0"), diagnostics=T)
summ.fit7 <- summary(iv_model_gt, vcov. = function(x) vcovHC(x, type="HC0"), diagnostics=T)
summ.fit8 <- summary(iv_model_gtl, vcov. = function(x) vcovHC(x, type="HC0"), diagnostics=T)



stargazer(iv_models_list, 
          title = "DMA Monthly Instrumental Variable Models -- Semi-conductor and Car Price on MVT",
          omit.stat = c("rsq", "ll", "ser"),
          no.space = TRUE,
          table.placement = "!htb",
          notes.append = FALSE, 
          notes = c("* p<0.05; ** p<0.01; GT = Google Trends", 
                    "MVT = Motor Vehicle Theft"),
          notes.align = "l",
          se = lapply(iv_models_list , iv_se_robust),
          dep.var.labels=c("UCR MVT", "UCR MVT", 
                           "CFS 911 MVT", "CFS 911 MVT", 
                           "NIBRS MVT", "NIBRS MVT", 
                           "GT MVT", "GT MVT"),
          column.labels = c("(0-100)", "(log)", 
                            "(0-100)", "(log)", 
                            "(0-100)", "(log)", 
                            "(0-100)",  "(log)"),
          covariate.labels = c("Car CPI"),
          column.sep.width = "2pt",
          font.size = "scriptsize",
          digits = 2,
          df = F,
          header=FALSE,
          star.char = c( "*", "**"),
          star.cutoffs = c(.05, .01),
          add.lines = list(c(rownames(summ.fit1$diagnostics)[1], 
                             round(summ.fit1$diagnostics[1, "p-value"], 3), 
                             round(summ.fit2$diagnostics[1, "p-value"], 3), 
                             round(summ.fit3$diagnostics[1, "p-value"], 3), 
                             round(summ.fit4$diagnostics[1, "p-value"], 3), 
                             round(summ.fit5$diagnostics[1, "p-value"], 3), 
                             round(summ.fit6$diagnostics[1, "p-value"], 3), 
                             round(summ.fit7$diagnostics[1, "p-value"], 3), 
                             round(summ.fit8$diagnostics[1, "p-value"], 3)), 
                           c(rownames(summ.fit1$diagnostics)[2], 
                             round(summ.fit1$diagnostics[2, "p-value"], 3), 
                             round(summ.fit2$diagnostics[2, "p-value"], 3), 
                             round(summ.fit3$diagnostics[2, "p-value"], 3), 
                             round(summ.fit4$diagnostics[2, "p-value"], 3), 
                             round(summ.fit5$diagnostics[2, "p-value"], 3), 
                             round(summ.fit6$diagnostics[2, "p-value"], 3), 
                             round(summ.fit7$diagnostics[2, "p-value"], 3), 
                             round(summ.fit8$diagnostics[2, "p-value"], 3)) ),
          label = "tab:iv_table",
          type = "latex")
          #type = "html",
          #out = "regression_models2.html")
```






Table \ref{tab:iv_table2} employs Car CPI as an instrumental variable to illustrate significant positive relationships between crime reporting rates and GT MVT. The hypothesis when car price increases, the incidents of MVT will also increase. It then, in turn, boosts online searches related to MVT because of the need for victimization information. However, this increase in online searches of MVT is very unlikely to be impacted by car prices themselves, but rather by the actual occurrences of vehicle theft. Thus, car prices serve as a valuable instrumental variable for confirming the relationship between GT MVT and other crime statistics. In this analysis, I apply official crime statistics as the independent variable, with an additional instrumental variable to further validate their impacts on GT MVT.

Similarly, we also need to examine the assumption that the instrumental variable $z$ affects $x$. Here, the effect of Car CPI on official crime statistics is presented in Table \ref{tab:iv_test2}. Across all models, including random effect, fixed effect, and GLS models, we observe a significant association between Car CPI and official crime statistics, such as UCR MVT, NIBRS MVT, and CFS 911 MVT, from model (1) to (9). This observation satisfies the instrumental variable assumption regarding the linear relationship of $z$ on $x$.

```{r}
test_z_and_x_4 <- plm(UCR.MVT ~ Car.CPI,
                    data = iv_df,
                    model = "random")

test_z_and_x_5 <- plm(UCR.MVT ~ Car.CPI,
                    data = iv_df,
                    model = "within")

test_z_and_x_6 <- gls(UCR.MVT ~ Car.CPI,
                    data = iv_df,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)



test_z_and_x_7 <- plm(NIBRS.MVT ~ Car.CPI,
                    data = iv_df,
                    model = "random")

test_z_and_x_8 <- plm(NIBRS.MVT ~ Car.CPI,
                    data = iv_df,
                    model = "within")

test_z_and_x_9 <- gls(NIBRS.MVT ~ Car.CPI,
                    data = iv_df,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)


test_z_and_x_10 <- plm(CFS.911.MVT ~ Car.CPI,
                    data = iv_df,
                    model = "random")

test_z_and_x_11 <- plm(CFS.911.MVT ~ Car.CPI,
                    data = iv_df,
                    model = "within")

test_z_and_x_12 <- gls(CFS.911.MVT ~ Car.CPI,
                    data = iv_df,
                  correlation = corARMA(p = 1, q = 1, form = ~ year_month | DMA),
                  na.action = na.omit)

```


```{r, results='asis', echo=FALSE}

test_z_and_x_list_2 <- list(test_z_and_x_4, test_z_and_x_5,test_z_and_x_6,
          test_z_and_x_7, test_z_and_x_8,test_z_and_x_9,
          test_z_and_x_10, test_z_and_x_11,test_z_and_x_12)

stargazer(test_z_and_x_4, test_z_and_x_5,test_z_and_x_6,
          test_z_and_x_7, test_z_and_x_8,test_z_and_x_9,
          test_z_and_x_10, test_z_and_x_11,test_z_and_x_12,
          title = "DMA Monthly Instrumental Variable Models Assumption Test -- Car Price on Official Crime Statistics",
          omit.stat = c("rsq", "ll", "ser"),
          no.space = T,
          table.placement = "!htb",
          notes = c("* p<0.05; ** p<0.01; RE = Random Effect; FE = Fixed Effect"),
          notes.align = "l",
          notes.append = F,
          #se = lapply(test_z_and_x_list_2, iv_se_robust),
          dep.var.labels=c("UCR MVT", "NIBRS MVT", "CFS 911 MVT"),
          column.labels = c("(RE)", "(FE)", "(GLS)", 
                            "(RE)", "(FE)", "(GLS)",
                            "(RE)", "(FE)", "(GLS)"),
          covariate.labels = c("Car CPI"),
          column.sep.width = "2pt",
          font.size = "scriptsize",
          digits = 2,
          df = F,
          header=FALSE,
          star.char = c( "*", "**"),
          star.cutoffs = c(.05, .01),
          label = "tab:iv_test2",
          type = "latex")
          #type = "html",
          #out = "regression_models2.html")
```

For UCR MVT, significant coefficients of 1.09 in GT MVT (0-100) for model (1) and -1.87 in GT MVT (log) for model (2), demonstrating mixed result because of the difference of data formats. However, other crime statistics such as NIBRS and CFS 911 consistently demonstrate a positive and significant relationship with GT MVT from model (3) to (6), reinforcing the need to use various sources of criminal statistics to verify the findings. 

For CFS 911 MVT, it shows coefficients of 0.46 in GT MVT (0-100) for model (3) and 0.43 in GT MVT (log) for model (4), suggesting that a higher volume of 911 calls associates with increased searches on Google Trends. For NIBRS MVT, the coefficients is 0.53 for GT MVT (0-100) in model (5) and 0.65 for GT MVT (log) in model (6), indicating positive and significant relationships between GT MVT and CFS 911 and NIBRS. All models passed the weak instrument test, confirming the effect of $z$ on $x$. The Wu-Hausman test results show a p-value of less than 0.01 in all models, indicating that the IV model provides a better fit than the OLS model.

These findings underscore how real-world MVT incidents significantly shapes and drives online search behaviors related to MVT. The comprehensive results across various forms of crime data (UCR, CFS 911, NIBRS) reveal robust public responsiveness to increases in MVT incidents, with higher crime rates prompting more searches. 

The instrumental variable analysis provides significant insights into the complex relationship among economic indicators, crime rates, and Google search behaviors related to motor vehicle theft at the DMA-month level. This study addresses research questions 1.1 and 1.2, examining the linear relationship between GT MVT and official crime statistics across time and space.


```{r, results='asis', echo=FALSE}
alt_iv_models_list <- list(alt_iv_model_ucr, alt_iv_model_ucrl,
          alt_iv_model_nibrs, alt_iv_model_nibrsl,
          alt_iv_model_911, alt_iv_model_911l)

alt_summ.fit1 <- summary(alt_iv_model_ucr, vcov. = function(x) vcovHC(x, type="HC0"), diagnostics=T)
alt_summ.fit2 <- summary(alt_iv_model_ucrl, vcov. = function(x) vcovHC(x, type="HC0"), diagnostics=T)
alt_summ.fit3 <- summary(alt_iv_model_911, vcov. = function(x) vcovHC(x, type="HC0"), diagnostics=T)
alt_summ.fit4 <- summary(alt_iv_model_911l, vcov. = function(x) vcovHC(x, type="HC0"), diagnostics=T)
alt_summ.fit5 <- summary(alt_iv_model_nibrs, vcov. = function(x) vcovHC(x, type="HC0"), diagnostics=T)
alt_summ.fit6 <- summary(alt_iv_model_nibrsl, vcov. = function(x) vcovHC(x, type="HC0"), diagnostics=T)

stargazer(alt_iv_model_ucr, alt_iv_model_ucrl,
          alt_iv_model_nibrs, alt_iv_model_nibrsl,
          alt_iv_model_911, alt_iv_model_911l,
          title = "DMA Monthly Instrumental Variable Models -- Official Crime Statistics on GT MVT",
          omit.stat = c("rsq", "ll", "ser"),
          no.space = TRUE,
          table.placement = "!htb",
          notes.append = FALSE, 
          notes = c("* p<0.05; ** p<0.01; GT = Google Trends; MVT = Motor Vehicle Theft"),
          notes.align = "l",
          se = lapply(alt_iv_models_list , iv_se_robust),
          dep.var.labels=c("GT MVT", "GT MVT", "GT MVT", "GT MVT", "GT MVT", "GT MVT"),
          column.labels = c("(0-100)", "(log)",
                            "(0-100)", "(log)",
                            "(0-100)", "(log)"),
          covariate.labels = c("UCR MVT", "UCR MVT (log)",
                               "CFS 911 MVT", "CFS 911 MVT (log)",
                               "NIBRS MVT", "NIBRS MVT (log)"), 
          column.sep.width = "2pt",
          font.size = "scriptsize",
          digits = 2,
          df = F,
          header=FALSE,
          star.char = c( "*", "**"),
          star.cutoffs = c(.05, .01),
          add.lines = list(c(rownames(alt_summ.fit1$diagnostics)[1], 
                             round(alt_summ.fit1$diagnostics[1, "p-value"], 3), 
                             round(alt_summ.fit2$diagnostics[1, "p-value"], 3), 
                             round(alt_summ.fit3$diagnostics[1, "p-value"], 3), 
                             round(alt_summ.fit4$diagnostics[1, "p-value"], 3), 
                             round(alt_summ.fit5$diagnostics[1, "p-value"], 3), 
                             round(alt_summ.fit6$diagnostics[1, "p-value"], 3)), 
                           c(rownames(alt_summ.fit1$diagnostics)[2], 
                             round(alt_summ.fit1$diagnostics[2, "p-value"], 3), 
                             round(alt_summ.fit2$diagnostics[2, "p-value"], 3), 
                             round(alt_summ.fit3$diagnostics[2, "p-value"], 3), 
                             round(alt_summ.fit4$diagnostics[2, "p-value"], 3), 
                             round(alt_summ.fit5$diagnostics[2, "p-value"], 3), 
                             round(alt_summ.fit6$diagnostics[2, "p-value"], 3)) ),
          label = "tab:iv_table2",
          type = "latex")
          #type = "html",
          #out = "regression_models2.html")
```

```{r}
cpi_ppi_long <- iv_df %>%
  mutate(year_month = ymd(year_month)) %>%  # Ensure 'year_month' is a Date type
  dplyr::select(DMA, year_month, UCR.MVT_normalized, NIBRS.MVT_normalized, CFS.911.MVT_normalized, GT.MVT, Car.CPI, Semiconductor.PPI) %>%  # Select necessary columns
  pivot_longer(
    cols = c("GT.MVT", "UCR.MVT_normalized", 
             "CFS.911.MVT_normalized", "NIBRS.MVT_normalized",
             "Car.CPI", "Semiconductor.PPI"),  # Variables to plot
    names_to = "Variable",
    values_to = "Value"
  )
# Give Orders in varialbes
cpi_ppi_long <- cpi_ppi_long %>%
  mutate(Variable = factor(Variable, levels = c("GT.MVT", "UCR.MVT_normalized", 
                                                "CFS.911.MVT_normalized", 
                                                "NIBRS.MVT_normalized", 
                                                "Car.CPI", "Semiconductor.PPI")))

```

\FloatBarrier


## DMA-Month Natural Experiments & Interrupted Time Series

Table \ref{tab:its} presents an interrupted time series analysis (ITS) that evaluates the impact of the COVID-19 lockdown on MVT using various data including UCR MVT, CFS 911 MVT, NIBRS MVT, and GT MVT, incorporating both normalization (0-100 scale) and logarithmic transformations. Before the intervention, the motor vehicle theft metrics display varying trends. For example, the UCR MVT in scaled form in model (1) decreases by 0.01 units per month, indicating a slight reduction in reported vehicle thefts prior to the intervention. Conversely, CFS 911 and NIBRS MVT, both scaled and log-transformed in models (3), (4), (5) and (6), exhibits a monthly and positively increase before the treatment.

At the moment of the intervention, the MVT metrics exhibit significant immediate changes. UCR MVT (0-100) experiences an immediate increase of 7.70 units. NIBRS MVT (0-100) in model (5) sees a substantial increase of 4.00 units, reflecting a sharp rise in MVT immediately following the intervention. Conversely, CFS 911 (0-100) shows a negative coefficient, but not significant. GT MVT (0-100) and GT MVT (log) in models (7) and (8) display considerable decreases of 9.55 units and 0.26 units, respectively, indicating an abrupt drop in Google searches related to vehicle theft immediately following the intervention^[The sudden fall of GT MVT after the treatment does not align with other official crime data but CFS 911. One explanation could be a small lag effect observed in the GT MVT data. However, the results of the lag effect test do not support this speculation (see Table \ref{tab:its_lag}), as it is still negative effect for treatment whether lagged 1 or -1 time units. Another explanation might be that during the pandemic lockdown, most people stayed at home using computers, which increased the overall population using Google search. This increase in the base search population, used for Google Trends data calculation, could result in a sudden decrease in the original search trends. If this were the case, we would expect to observe unusual trends in search terms before and after the COVID-19 lockdown, suggesting that the GT data may not accurately reflect real-world searching behavior during the lockdown. However, this is also not the case. For example, during the COVID-19 lockdown, we might expect a decrease in searches related to weather (due to most people working from home and not needing weather updates) and an increase in searches related to computers (for remote learning), games (to pass the time), and recipes (for home cooking). All results appear to be quite normal, and the trends during April 2020 are as expected, presenting the correct patterns. The discussed GT results before/after COVID-19 lockdown can be seen in Figure \ref{fig:figure A.7}. However, the negative significant effect on GT MVT might indicate the limitation of GT MVT, I will discuss more in the limitation section. The third explanation could be that some agencies stopped reporting data in 2020 or 2021, which coincided with the UCR discontinuing data collection in 2021. Additionally, the COVID-19 pandemic could have potentially impacted police recording behaviors during this period. Consequently, the data imputed for treatment month (April, 2020) was higher than it should have been due to the interpolation of rising values in subsequent months. As a result, we observe increasing trends in UCR and NIBRS data, yet a contrasting decline in CFS 911 trends.].

Following the intervention, the overall trends across various MVT metrics generally continue to increase.units For instance, in the post-intervention coefficient of "Time Since Treatment", UCR MVT (100) in model (1) exhibits an expected monthly increase of 1.05 . Similar upward trends are observed in UCR MVT (log) in model (2) and NIBRS MVT (0-100) in model (5), and GT MVT in model (7) and (8).

These findings indicate that while pre-COVID-19 MVT trends from various statistics showed mixed results, the immediate impact of the intervention also remains mixed. However, following the COVID-19 lockdown, the post-treatment effects demonstrate an overall increase in UCR, NIBRS MVT, and also in Google Search metrics related to MVT terms. This helps confirm the concurrent validity of GT MVT. This analysis addresses research question 2.2, examining whether GT MVT varies together with other MVT statistics in response to a natural experiment and its intervention effects. The selected interrupted time series table with prediction and counterfactual lines can be found in Table \ref{fig:figure 7.5}, with the full figure available in Appendix Table \ref{fig:figure A.6}.




```{r}
#https://rpubs.com/chrissyhroberts/1006858

its_data = year_month_merged_data[c("DMA", "year_month", "GT.MVT", "log.GT.MVT", 
                                    "UCR.MVT_normalized", "log.UCR.MVT",
                                    "NIBRS.MVT_normalized", "log.NIBRS.MVT",
                                    "CFS.911.MVT_normalized","log.CFS.911.MVT")]



its_data <- its_data %>%
  mutate(year_month = as.Date(year_month, format = "%Y-%m-%d")) %>%
  arrange(DMA, year_month) %>%
  group_by(DMA) %>%
  mutate(D = ifelse(year_month >= as.Date("2020-04-01"), 1, 0),
         P = ifelse(year_month >= as.Date("2020-04-01"), 
                     (row_number()+1) - which.max(
                       year_month >= as.Date("2020-04-01")), 0)) # P counts months after April 2020
its_data_for_fig <- its_data %>% dplyr::select(DMA, year_month, GT.MVT, 
                                        UCR.MVT_normalized, 
                                        NIBRS.MVT_normalized,
                                        CFS.911.MVT_normalized, D, P)

colnames(its_data_for_fig) <- c("DMA", "Year.Month", 
                                "GT.MVT",  "UCR.MVT", 
                                "NIBRS.MVT", "CFS_911.MVT",
                                "D", "P")

```


\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{selected_its_plot.png}
  \caption{Interrupted Time Series of COVID-19 Lockdown on MVT Data (Selected DMA)}
  \label{fig:figure 7.5}
\end{figure}




```{r, results='asis', echo=FALSE}
#https://ds4ps.org/pe4ps-textbook/docs/p-020-time-series.html


regTS_GT = gls(GT.MVT ~ year_month + D + P, data = its_data, method="ML", na.action = na.omit)  
regTS_UCR = gls(UCR.MVT_normalized ~ year_month + D + P, data = its_data, method="ML", na.action = na.omit)
regTS_911 = gls(CFS.911.MVT_normalized ~ year_month + D + P, data = its_data, method="ML", na.action = na.omit)
regTS_nibrs = gls(NIBRS.MVT_normalized ~ year_month + D + P, data = its_data, method="ML", na.action = na.omit)


regTS_GTl = gls(log.GT.MVT ~ year_month + D + P, data = its_data, method="ML", na.action = na.omit)  
regTS_UCRl = gls(log.UCR.MVT ~ year_month + D + P, data = its_data, method="ML", na.action = na.omit)
regTS_911l = gls(log.CFS.911.MVT ~ year_month + D + P, data = its_data, method="ML", na.action = na.omit)
regTS_nibrsl = gls(log.NIBRS.MVT ~ year_month + D + P, data = its_data, method="ML", na.action = na.omit)


stargazer( regTS_UCR, regTS_UCRl,
           regTS_911, regTS_911l, 
           regTS_nibrs, regTS_nibrsl, 
           regTS_GT, regTS_GTl, 
           header = FALSE, 
           title = "Interrupted Time Series Analysis",
           dep.var.labels = c("UCR MVT", "UCR MVT", 
                              "CFS 911 MVT", "CFS 911 MVT",
                              "NIBRS MVT", "NIBRS MVT",
                              "GT MVT", "GT MVT"),
           column.labels = c("(0-100)", "(log)", 
                             "(0-100)", "(log)",
                             "(0-100)", "(log)", 
                             "", "(log)"),
           covariate.labels = c("Time", "Treatment", 
                                "Time Since Treatment"),
           omit.stat = c("rsq", "ll", "ser"),
           no.space = TRUE,
           table.placement = "!htb",
           notes.append = FALSE, 
           notes = c("* p<0.05; ** p<0.01; GT = Google Trends", "MVT = Motor Vehicle Theft"),
           notes.align = "l",
           digits = 2,
           column.sep.width = "-2pt",
           font.size = "scriptsize",
           df = F,
           star.char = c( "*", "**"),
           star.cutoffs = c(.05, .01),
           type = "latex",
           label = "tab:its")
```

\FloatBarrier


# Chapter Summary

This chapter explored the validity of GT MVT data by examining its association with official crime statistics across various levels: state-year, DMA-year, and DMA-month. It also considered additional common crime covariates and control variables to enrich the analyses.

A strong association was observed between GT MVT and UCR MVT, as well as NICB at the DMA-year level, and with NIBRS at the DMA-month level across different geographic and temporal units. However, the overall correlation coefficient diminishes when the focus shifts to smaller geographic units (DMA) and shorter time frames (months). Despite this, various statistical analyses, including GLS, fixed-effect models, instrumental variable analysis, and interrupted time series, consistently affirm a significant positive relationship between GT MVT and other official crime statistics, especially with UCR MVT. GT MVT is significantly associated with UCR MVT at state-year, DMA-year and DMA-month using GLS or fixed effect, with and without control variables. Same results can also be found in NICB DMA-year, CFS 911 DMA-month, and NIBRS DMA-month level, except for NCVS, with only one model significant using GLS with controls. Additionally, the instrumental variable model, which uses crime statistics as the independent variable on GT MVT and car price as the instrument, also confirms their relationship with UCR, CFS 911, and NIBRS. This result affirms the linear relationship of GT MVT with official statistics, establishing it as a reliable predictor of crime data. 

The summary of these statistical relationships and the effect size can be found in Figure \ref{fig:figure7.6}. This figure summarizes the findings from testing the linear relationship between GT MVT and various official crime statistics, emphasizing the effect size. It includes tests where GT MVT is the independent variable and the dependent variables are official crime statistics across different spatial and temporal units. Figure \ref{fig:figure7.6_log} presents the results of the regression analysis using logged official crime statistics as the dependent variable and GT MVT as the independent variable. The linear relationship is mostly significant and positive, with the exception of NCVS and one CFS 911 data point. All other results are positive and either significant or nearly significant. 

Similarly, the concurrent validity of GT MVT is confirmed at the state-year and DMA-year levels, and partially confirmed at the DMA-month level. Analyses at the state-year and DMA-year levels utilize the concentrated disadvantage index, heterogeneity index, mobility index, percentage of foreign born, and percentage of young males as predictors of these crime statistics. The predictor which has the most significant relationship across MVT estimates is the heterogeneity index, which is significant with GT at the state-year level in both GLS and fixed-effect models, and significant in the GLS model at the GT DMA-year level. The heterogeneity index is also significant with UCR at the state-year level in the GLS model, and at the DMA-year level in both GLS and fixed-effect models. 

Figure \ref{fig:figure7.7} offers a comprehensive overview of the statistical test results for common crime covariates on GT MVT and other official MVT statistics. As depicted, the Heterogeneity Index emerges as the factor with the most significant relationships across MVT estimates. For a more detailed view of how other variables cluster around the zero line, please refer to Figure \ref{fig:figure7.8}.  

In Figure \ref{fig:figure7.8}, we can see that mobility index has the most significant relationship across MVT estimates, as shown in the state-year GLS/FE and DMA-year GLS/FE models. For UCR, mobility index is significant at state-year with both GLS and FE models, and significant with UCR at DMA-year with both GLS and FE models, and NICB with both GLS and FE models at DMA-year level. The third predictor is concentrate disadvantage index. Unlike other variables, this significant relationship is surprisingly negative. It is significantly associated with GT at the state-year level in both GLS and FE models, and nearly significant with GT at the DMA-year GLS model. It is also significant with UCR at the state-year level in both GLS and FE models; and significant with NICB at the DMA-year level in both GLS and FE models. All relationships are significantly negative, with no significant positive relationships observed. This finding partially confirms the social disorganization theory, suggesting that heterogeneity and mobility are associated with higher crime rates. However, it challenges the notion that concentrated disadvantage leads to more crime; conversely, when it comes to MVT, more concentrated disadvantage associates less MVT crime. We will discuss this further in the next chapter. Overall, with the same significance and direction as with other common crime covariates, GT has demonstrated its usefulness in criminology theory and confirmed its concurrent validity.

The other tests of the instrumental variable, such as car price's influence on GT and other crime statistics, also confirm GT's concurrent validity. The only test where we found no significant evidence for GT's concurrent validity was with temperature and precipitation, which we will discuss in the next chapter. The interrupted time series analysis further confirms GT's concurrent validity, demonstrating an overall increase in GT MVT after the COVID-19 lockdown, a trend that aligns with UCR, NIBRS, and CFS 911 data, all showing significant and positive changes. Additionally, these crime statistics, along with GT MVT, consistently display parallel trends with other crime metrics across most DMA and exhibit increasing trends post-COVID-19 lockdown, as illustrated in Figure \ref{fig:figure A.6}.

To address these variations and confirm the robust linear relationship between GT MVT and other crime statistics, the study employs several strategies. By integrating diverse crime data from multiple sources, including UCR, NCVS, NICB, NIBRS, and CFS 911, the analysis gains robustness. Additionally, the inclusion of common crime covariates and other control variables not only corroborates the linear relationship of GT MVT with other datasets but also confirms its concurrent validity. Moreover, utilizing multiple crime statistics along with other statistical methods provides strong empirical support for testing criminological theories. Finally, the application of advanced statistical methods to account for extraneous variables, autoregression, and heteroskedasticity in time series data enhances the comprehensiveness and robustness of the findings.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{forestplot.png}
  \caption{Forest Plot of the Linear Relationship Between GT MVT and Various Crime Statistics}
  \caption*{This figure summarizes the findings from testing the linear relationship between GT MVT and various official crime statistics, focusing on the effect size. It includes tests where GT MVT serves as the independent variable, while the dependent variables are official crime statistics across different spatial and temporal units. The figure presents models both with and without control variables. Please refer to Figure \ref{fig:figure7.6_log} for the logged dependent variable from official crime statistics.}
  \label{fig:figure7.6}
\end{figure}


\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{log_forestplot.png}
  \caption{Forest Plot of the Linear Relationship Between GT MVT and Various Logged Crime Statistics}
  \caption*{This figure summarizes the findings from testing the linear relationship between GT MVT and various logged official crime statistics, focusing on the effect size. It includes tests where GT MVT serves as the independent variable, while the dependent variables are official crime statistics across different spatial and temporal units. The figure presents models both with and without control variables.}
  \label{fig:figure7.6_log}
\end{figure}


\begin{figure}[!htb]
  \centering
  \includegraphics[width=5.5in]{common_forestplot.png}
  \caption{Forest Plot: Effect of Common Crime Covariates on GT MVT and Other Official MVT Statistics}
  \caption*{This forest plot offers a comprehensive overview of the statistical test results for common crime covariates on GT MVT and other official MVT statistics. As depicted, the Heterogeneity Index emerges as the factor which has the most significant relationship across MVT estimates. For a more detailed view of how other variables cluster around the zero line, please refer to Figure \ref{fig:figure7.8}. Results from log-transformed variables or those derived from GT MVT (log) are not included here, as they are mostly consistent, and has been shown in the tables.}
  \label{fig:figure7.7}
\end{figure}


\begin{figure}[!htb]
  \centering
  \includegraphics[width=5.5in]{common_forestplot_closer.png}
  \caption{Forest Plot: Effect of Common Crime Covariates on GT MVT and Other Official MVT Statistics (A Detailed Examination Near Zero Line by Regression Models)}
  \caption*{This is a closer look at the relationship between common crime covariets and other MVT statistics. From this forest plot, we can see that Mobility Index has the most significant relationship across MVT estimates, except for NCVS. Other than that, CD Index seems to have negetive effect on MVT, can be shown in the models in GT and UCR state-level models, and in NICB at DMA models. Results from log-transformed variables or those derived from GT MVT (log) are not included here, as they are mostly consistent, and has been shown in the tables.}
  \label{fig:figure7.8}
\end{figure}



\FloatBarrier

<!-- Chapter 8 -->
\newpage
\fancyhead[L]{Chapter 8: Discussion and Conclusion}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}

\chapter{DISCUSSION AND CONCLUSION}

The dissertation aimed to examine the linear relationship between GT MVT and other official MVT data from UCR, NIVS, NICB, NIBRS, and CFS 911, as well as assess the concurrent validity of GT MVT by analyzing it alongside other official crime statistics and common crime covariates. The findings are summarized and discussed in the subsequent sections.

# The Linear Relationships between GT MVT and Official MVT

Based on the outcomes of the state-year, DMA-year, and DMA-month data points, along with GLS, fixed-effect, and instrumental variable models, the results are robust and consistently indicate that GT is positively associated with official crime statistics such as UCR, NICB, NIBRS, and CFS 911 (except NCVS). Considering the well-known issue of reporting error and measurement error in official crime statistics, I chose motor vehicle theft, a type of crime which has the least reporting error [@o1980empirical; @gove1985uniform; @blumstein1991trend; @hart2003reporting; @tarling2010reporting; @cohen1984discrepancies; @weatherburn2011uses], to validate the effectiveness of GT MVT. Secondly, to ensure a comprehensive understanding of measurement error mechanisms and how to validate a new measure against established ones, I applied log-transformed models, instrumental variable analysis, GLS regression, fixed-effect analysis, and interrupted time-series analysis. These methods adopted a more conservative approach for statistical testing to ensure robust results, going beyond cross-sectional studies by incorporating temporal variations and rigorous statistical models. Additionally, the concurrent validity with other common covariates of crime further supports the robustness of the findings.

The linear relationship between GT MVT and UCR MVT, along with other official crime statistics (except NCVS), remains consistent and significant across state-year, DMA-year, and DMA-month analyses. This relationship also holds when using log-transformed data. 

Their correlation coefficient, however, decreases as the unit of analysis becomes smaller^[The correlation coefficient of GT MVT with UCR MVT(100) is 0.747 at the state-year level, validating it as a satisfactory proxy for MVT. Furthermore, the correlation coefficients are 0.653 with UCR MVT(100) and 0.586 with NICB MVT at DMA-year level, demonstrating that GT MVT can also serve effectively as a proxy at the DMA-year level. However, at the monthly data level, the correlation coefficients drop to 0.321 with UCR MVT(100), 0.246 with NIBRS(100), and 0.038 with CFS 911. Surprisingly, despite all deriving from police reports, the highest observed correlation was only 0.550 between UCR MVT(100) and NIBRS(100), and 0.186 between CFS 911(100) and UCR MVT(100).]. This outcome highlights the existence of measurement errors, especially recording errors within crime statistics, and it is proportionate to the size of the population of the analysis. This aligns with previous research that argues that the rarity of crime incidents can cause significant fluctuations in crime rate estimates in areas with smaller populations [@maltz2002note; @maltz2003measurement; @pridemore2005cautionary; @ucrbook]. @rapepolicehidekaplan2021ucrbook argues that the sample size for surveys or datasets should exceed 100,000 individuals to ensure reliability, and there should be extra caution regarding survey sample size and methodology [@skogan1981issues; @rand2005bigger]. Similarly, GT crime estimates also were affected by population size. My analyses across state-year, DMA-year, and DMA-month levels showed that as the unit of analysis became smaller, the overall correlation between crime statistics decreased--not only with GT MVT but also among the official crime estimates themselves. It is crucial to reiterate the importance of employing various crime statistics in criminological studies to triangulate findings. Given that data inherently contain unknown measurement errors, which increase as the estimated population decreases. I will explore the relationship between population size and the correlation coefficient between GT MVT and UCR MVT in the next section. 

Surprisingly, NCVS does not show significant association with GT MVT, despite GT being characterized as a 'victim-oriented' method. This method assumes that victims will search for related information online, thus more searches indicate more victims. The divergence of NCVS from not only GT MVT but also other crime statistics may suggest that NCVS is not valid for sub-regional estimates, which correspond with previous findings [@fay2015developmental; @shook2015assessing; @groves2008surveying; @berg2016telling]. However, it might be too early to conclude this, as the NCVS data available for sub-national estimates is limited and includes only a small sample size. The author acknowledges that sample size available from the public data of NCVS at the MSA level is very limited. The NCVS from 2000 to 2015 included only 52 MSAs, and not all of these MSAs had continuous data points for each year. The filling rate of missing values is 26.22% after imputation. This high filling rate is considered substantial in data imputation, making it a less attractive option for crime estimates in sub-regional analysis. 

As previously discussed, another issue is that the NCVS sample size of the surveyed population remains insufficient for reliable regional estimates. Considering the rarity of crime events, estimating crime incidents is considerably more challenging than surveying household income or influenza infection rate. Producing a meaningful sample would require tremendous time, money, and effort. Moreover, as the types of crime surveyed become rarer, the need for larger sample sizes increases, making the cost unimaginable [@rapepolicehidekaplan2021ucrbook]. Although the NCVS may not provide effective sub-national estimates, it does offer valuable insights into the types of victimizations occurring at the national level. This dissertation is inspired by the research from NCVS, which highlights that MVT has the highest reporting rate among crimes, making it one of the most reliable crime statistics, second only to homicide [@ncvs_2021_bjs]. Without the insights provided by the NCVS, understanding the dynamics of crime reporting, including the reasons why people choose to report or not report crimes, and the relationships between victims and offenders, would be significantly hindered. 

That said, a free, open-source dataset sourced from GT, with the advantage of digital trace data and comparable time and geolocation utility, along with a comparable sample size with UCR, NICB, NIBRS, and CFS 911, proves to be exceptionally effective. The evidence suggests that GT may be a more effective tool for predicting MVT statistics at the DMA-year level compared to NCVS. As additional sub-regional data from NCVS becomes available, it seems likely that GT could continue to offer more reliable estimates than NCVS at other temporal or geographical levels of analysis.






# The Concurrent Validity of GT MVT and the Test of Criminological Theories
The predictors which had the most significant relationship across MVT estimates have been confirmed to be the two social disorganization constructs of ‘residential mobility’ and ‘heterogeneity’ and the routine activities construct of ‘car price’ used as a measure of ‘suitable target’ in this dissertation. These findings align with previous studies [@dao2022crimescape; @suresh2013locations; @rice2002socioecological; @piza2018predicting]. Social disorganization theory highlights how rapid social changes can outpace existing regulations, weakening informal social control. Consequently, in areas with high tenant turnover or predominantly rental housing, where there are fewer homeowners, neighbors tend to know each other less compared to areas with predominantly owner-occupied homes. This lack of familiarity leads to diminished trust and weaker social networks, which are associated with reduced informal social control and fewer social ties, ultimately resulting in higher crime rates. Moreover, in social disorganization theory, higher heterogeneity often implies different cultural values system and a lack of cohesion and mutual communication channels, further eroding community bonds and making effective communal crime prevention more challenging. These factors collectively exacerbate vulnerabilities to motor vehicle theft, as communities struggle to maintain effective surveillance and collective guardianship [@dao2022crimescape; @suresh2013locations; @rice2002socioecological; @piza2018predicting]. 

From the result, the concentrated disadvantage index, contrary to traditional findings, shows a negative effect on MVT. This might be explained through the routine activity theory. In routine activity theory, the more valuable a target, the more it attracts potential offenders. Similarly, with MVT, the higher the price of the car, the more attractive it becomes. Thus, in economically challenged areas with less attractive targets--vehicles in this case--attracts fewer potential offenders. Our findings contradict the traditional wisdom of social disorganization theory.

This study also confirms the concurrent validity of GT MVT via routine activity theory. Although there are studies suggesting a relationship between the price (value of the target) and crime [@cohen2010social; @draca2019changing; @cohen1981social; @felson1980human], no research has specifically explored the link between car prices and motor vehicle theft. The evidence from this dissertation suggest that the surge in car price has directly contributed to a spike in cases of motor vehicle theft. I use semiconductor prices as an instrument to account for other confounding factors affecting automobile prices on MVT. The findings consistently validate that elevated car prices have contributed to increasing rates of MVT since the COVID-19 pandemic. This discovery is consistent with routine activity theory, which posits that when the value of the target increase, the motivation to engage in criminal activities also increases [@cohen2010social; @lee2018use; @cohen1981social; @felson1980human]. Consequently, the increase in the value of potential targets -- car price, motivates the offenders and leads to more MVT cases.

Another discovery related to routine activity theory concerns the influence of temperature and precipitation. Findings from our fixed effect model at the DMA-month level indicate a significant relationship between these situational factors and increased UCR and NIBRS MVT rates (not with GT MVT and CFS 911). This is consistent with multiple previous studies that investigate the impact of temperature and precipitation on crime, such as those by @field1992effect, @cohn1990weather and @baryshnikova2021you. The discrepancy between GT MVT and official MVT highlights the limitations of using GT data. The varying results between these data sources underscore the necessity of not relying solely on one statistic, which can be prone to unique biases and limitations. A more robust approach involves leveraging multiple data sources and corroborating findings with previous research on similar topics. This method enhances the credibility and reliability of the conclusions drawn.

The interrupted time series analysis examining the impact of COVID-19 on MVT supports the concurrent validity of GT. It reveals the trends in MVT before and after the COVID-19 incident, which indicating that there are no delayed effects and closely mirroring the increasing trends observed in official MVT data after the lockdown. This prompt responsiveness highlights GT MVT's value as a dependable indicator of real-time crime trends, effectively capturing sudden societal changes and their impact on crime rates. However, it should be noted that the sudden drop observed immediately after the COVID-19 lockdown in GT MVT is only mirrored in CFS 911 data, and not in UCR MVT and NIBRS MVT (The sudden drop in GT MVT at the beginning of the lockdown in April 2020). 

Given that MVT is a well-established measure of crime statistics and GT MVT is a relatively new estimation method for crime, one conservative interpretation might be that GT MVT is not very effective at capturing sudden impacts. Alternatively, it is possible that GT MVT may actually be more sensitive to reflecting such abrupt changes, as CFS 911 data also shows a sudden decrease in MVT cases. Previous studies have documented both increase and decrease in motor vehicle theft during the COVID-19 lockdown across different U.S. cities [@ashby2020initial]. However, some observed significant reductions during the lockdown periods in other countries [@halford2020crime; @balmori2021u]. This variability underscores the importance of using digital trace data cautiously, as relying solely on sudden changes in GT data can lead to misleading conclusions. A more comprehensive analysis that considers long-term trends and cross-regional comparisons is essential for drawing more reliable insights.


# The Interaction Between Population and The Correlation Coefficient between GT MVT and UCR MVT

The results indicate that the correlation coefficient between official crime statistics and Google Trends decreases as the unit of analysis becomes smaller. As discussed in the previous section, larger populations generally provide a more accurate estimate of official crime statistics. Similarly, the overall trends observed in digital trace data, such as Google Trends, are less susceptible to sudden spikes or drops in search activity as well.

Consequently, I conducted a further investigation into the relationship between the correlation coefficient of GT MVT and UCR MVT and its relationship with population size in each DMA on a monthly basis.^[NIBRS and CFS 911 data are excluded due to their very limited available DMA.] The results, illustrated in Figure \ref{fig:figure8.1}, reveal a general trend of increasing correlation coefficients between GT MVT and UCR MVT as population (logged) increases.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{corr_and_population.png}
  \caption{GT MVT and UCR MVT Correlation Coefficient vs. Population (log) at DMA-Month Level}
  \caption*{}
  \label{fig:figure8.1}
\end{figure}


Notably, MVT is a type of crime that is reported to the police nearly 80% of the time [@morgan2021criminal], suggesting that these statistics should be highly reliable. However, according to the observation from UCR, NIBRS, and CFS 911 in this study, we still observed discrepancy in certain areas^[In \ref{fig:figure A.4}, we may observed some discrepancy in UCR, NIBRS, and CFS 911 in Los Angeles, New Orleans, and Atlanta, for example] despite these data originating from the same source—the police department. This underscores the potential for recording errors within the police system. Consequently, it is crucial for police statistics, including data reported to UCR, NIBRS, and CFS 911, to be made publicly accessible. Allowing public access enables scrutiny of data quality. Uniformity of one type of crime data (e.g., MVT or robbery) across various data sources in one police department may suggest consistent recording practices and their attention to reduced errors. This is particularly important as these data are usually used to guide policy, and the manner it handled data can reflect an agency's diligence and helps minimize errors. 

Following this, I identified police departments with publicly accessible, downloadable crime data on their website, coding them as '1', and others as '0'^[I define publicly accessible data as information that can be freely downloaded from a website without any application process and is suitable for further analysis. This specifically includes data that is ready for direct analysis and excludes formats like pdf file, summary reports, map formats, or non-spreadsheet files. The data should cover at least two years to enable meaningful, long-term comparisons in crime statistics. Data covering only short periods, such as the most recent seven days, lacks traceability and raises questions about its credibility. Acceptable examples include crime incidents or statistics and CFS 911 data.]. I discovered that when police departments make their data available online, their correlation with Google Trends data also increases. Please refer to the box plot in Figure \ref{fig:figure8.2} and a correlation analysis in Figure \ref{fig:figure8.3}. 

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{boxplot_corr.png}
  \caption{Boxplot of Correlation Coefficient Between GT MVT and UCR MVT with/without Public Accessible Crime Data}
  \caption*{}
  \label{fig:figure8.2}
\end{figure}


\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{corr_and_population2_interact.png}
  \caption{GT MVT and UCR MVT Correlation Coefficient vs. Population (log) at DMA-Month Level (Interaction)}
  \caption*{}
  \label{fig:figure8.3}
\end{figure}


The influence of population on crime data may be attributed to larger police departments, which typically have more resources and are thus more likely to make their data publicly available. It may also be that generally, the larger populations tend to provide better estimations of crime rates. As noted by @langton2017second, while property crimes might be estimated from data collected over a single year, crimes prone to underreporting, such as rape and sexual assault, often require datasets spanning at least three years to yield reliable estimates. My analysis here supports the assertion that larger populations and longer data collection periods enhance the robustness and accuracy of crime data.

<!--
As the factor of the size of population and the public accessibility data from the police department both play significant roles in affecting the correlation coefficient between UCR MVT and GT MVT, I conducted a small interaction analysis to examine their relationship. The results are shown in Table \ref{tab:corr_population}. It indicates that both population size and public access to police data are significantly positively associated with the correlation coefficient between GT MVT and UCR MVT. This supports the explanation of the population size influencing the correlation between UCR MVT and GT MVT at the DMA-month level. Moreover, when police data are made publicly accessible, it is also associated with a higher correlation coefficient between UCR MVT and GT MVT. 

When I included both population and the dichotomous variable of public accessible police data in one OLS regression model, the effect of police public access disappeared, as did the interaction model. The explanation may be that, usually, larger police departments with larger populations have the capability to make their data available to the public. Thus, when presented together in one OLS model, only the population (log) shows a significant association with the correlation coefficient between GT MVT and UCR MVT. This analysis again shows that the larger population can provide a better estimation in crime rates. As noted by @langton2017second, while property crimes might be estimated from data collected over a single year, other crime types prone to underreporting, such as rape and sexual assault, may require datasets spanning at least three years to generate reliable estimates. This analysis also supports the argument that the larger the population and the longer the period of data collection, the more robust and accurate the crime data becomes.
-->

```{r loop_for_coeff_population, results = 'hide', echo=FALSE}
mvt_vars <- c("UCR.MVT_normalized")
list911 <- read.csv("911_call_police_departments.csv") 
public_data <- read.csv("public_data_departments.csv") 

# Initialize an empty data frame to store results
results <- data.frame(DMA = character(),
                      MVT_Type = character(),
                      corr_coeff = numeric(),
                      population = numeric(),
                      internet = numeric(),
                      automobilehh = numeric(),
                      stringsAsFactors = FALSE)
groups <- unique(year_month_merged_data$DMA)
# Loop through each group
for(group_id in groups) {
  # Subset data for the group
  subset_data <- year_month_merged_data[year_month_merged_data$DMA == group_id,]

  # Calculate population mean if there are enough data
  if (sum(complete.cases(subset_data$Population)) > 0) {
    population_mean <- mean(subset_data$Population, na.rm = TRUE)
    internet_mean <- mean(subset_data$Percentage.of.Internet.Subscription.per.Household, na.rm = TRUE)
    automobile_mean <- mean(subset_data$Average.Vehicle.HH, na.rm = TRUE)
  } else {
    population_mean <- NA  # Not enough data
    internet_mean <-  NA
    automobile_mean <- NA
  }

  # Loop through each MVT variable
  for(mvt_var in mvt_vars) {
    # Calculate correlation if there are enough complete cases
    if(sum(complete.cases(subset_data$GT.MVT, subset_data[[mvt_var]])) > 1) {
      cor_value <- cor(subset_data$GT.MVT, subset_data[[mvt_var]], use = "complete.obs")
    } else {
      cor_value <- NA  # Not enough data to calculate correlation
    }

    # Append to results data frame
    results <- rbind(results, 
                     data.frame(DMA = group_id, 
                                MVT_Type = mvt_var, 
                                corr_coeff = cor_value, 
                                population = population_mean,
                                internet = internet_mean,
                                vehicle = automobile_mean))
  }
}

results$population.log <- log(results$population)
results <- results %>%
  left_join(public_data, by = "DMA")

# Replacing case_when with if_else for the first mutate
results <- results %>%
  mutate(
    public.data = if_else(
      !is.na(Data.Type) & (Data.Type == "CFS 911" | Data.Type == "Crime Incidents"), 1, 0
    )
  )

# Replacing case_when with if_else for the second mutate
results <- results %>%
  mutate(
    CFS_911_or_not = if_else(
      !is.na(Data.Type) & (Data.Type == "CFS 911"), 1, 0
    )
  )

results <- results %>%
  dplyr::select(where(~ !any(is.na(.))))


```


<!--
```{r lm_for_corr_coeff, results = 'asis', echo=FALSE}
#discussion_m1_ucr <- lm(corr_coeff ~ vehicle, results)
#discussion_m2_ucr <- lm(corr_coeff ~ internet, results)
discussion_m3_ucr <- lm(corr_coeff ~ population.log, results)
discussion_m4_ucr <- lm(corr_coeff ~ public.data, results)
discussion_m5_ucr <- lm(corr_coeff ~ population.log + public.data , results)
discussion_m6_ucr <- lm(corr_coeff ~ population.log + public.data + population.log*public.data, results)


stargazer(discussion_m3_ucr, discussion_m4_ucr, discussion_m5_ucr, discussion_m6_ucr, 
           header = F, 
           escape = F,
           title = "OLS Regression on the UCR MVT and GT MVT Correlation Coefficient on Population and Interaction with Public Accessibility of Data",
           dep.var.labels = c("Correlation Coefficient (UCR MVT and GT MVT at DMA-Month Level)"),
           covariate.labels = c("Population (log)", "Public Crime Data Accessibility", 
                                "Population (log) X Public Crime Data Accessibility"),
           omit.stat = c("rsq", "ll", "ser"),
           no.space = TRUE,
           table.placement = "!htb",
          notes.append = FALSE, 
           notes = c("* p<0.05; ** p<0.01"),
           notes.align = "l",
           digits = 2,
           column.sep.width = "-2pt",
           font.size = "scriptsize",
           star.char = c( "*", "**"),
           star.cutoffs = c(.05, .01),
           type = "html",
           label = "tab:corr_population")

```
-->

# The Counterfactual Analysis of GT and Official Crime Statistics -- Beyond GT MVT

The counterfactual analysis involves enumerating all possible alternatives to observed events and investigating the potential consequences if different choices had been made. Essentially, it asks: 'If this had happened, then what would have been the outcome? And if it hadn't happened, what would have been the outcome?' This approach allows researchers to explore and understand the causal impacts of various scenarios by considering both what did happen and what could have happened under different circumstances.

When considering the reporting of crime statistics, there are eight possible combinations of relationships involving the occurrence of a crime, the victim's decision to report to the police, and their choice to search on Google for relevant information:

\begin{itemize}
    \item \textbf{Type I:} Victimization occurred, the victim googled for related information, and reported the crime to the police, thus both GT and police data accurately recorded this crime incident.
    \item \textbf{Type II:} The victimization occurred, the victim googled for related information but did not report to the police; thus, there is no police data but the incident might still be reflected in GT data. This leads to underestimation of real crime in the police data, but no effect on GT data.
    \item \textbf{Type III:} Victimization occurred, the victim reported to the police but did not search online, leading to underestimation of real crime in GT data but police data is not affected.
    \item \textbf{Type IV:} Victimization occurred, but the victim neither reported it to the police nor searched online, resulting in underestimation in both GT and police data.
    \item \textbf{Type V:} False crime or no victimization; the non-victim both reported and searched online, causing overestimation in both GT and police data.
    \item \textbf{Type VI:} False crime or no victimization; the non-victim did not report to the police but search online, causing overestimation in GT data only.
    \item \textbf{Type VII:} False crime or no victimization; the non-victim did not searched online but report to the police, leading to no impact on GT data but overestimation in police data.
    \item \textbf{Type VIII:} False crime or no victimization; with neither reporting nor searching online, resulting in no impact on either GT or police data.
\end{itemize}

```{r, results = 'asis'}
counter_factual_table <- read.csv("counter_factual_gt_police.csv", 
                                  row.names = NULL)

colnames(counter_factual_table) <- c("Type", "Victimization", "Use Google search", "Report to the Police", "Impact on GT Data", "Impact on Police Data")
cft_table <- xtable(counter_factual_table, type="latex", 
       caption = "The Relationships between Google Trends Victim’s Search and Crime Reporting ", 
       label = "tab:counterfactualtable",
       size = "tiny", include.rownames = FALSE
       )
print(cft_table, scalebox = 0.8, caption.placement = "top")
```

These scenarios are as presented in Table \ref{tab:counterfactualtable}. Note that in types I, and V, where true or the false incidents are both consistently recorded, it would not cause discrepancy in both GT or police data. Similarly in types IV and VIII, both GT and police data are not recorded, whether they are true or false incident. Thus, there is no discrepancy between the two sources as well. Therefore, these scenarios (I, IV, V, and VIII) can be excluded from discussions about divergences between GT and police data. Instead, we should focus on Types II, III, VI, and VII, where there is a discrepancy between GT and police data. Type II is particularly the main focus for this analysis, as it represents situations where individuals choose not to report to the police but still search for related information online, demonstrating the unique utility of GT data. However, to understand the implications fully, it is crucial to determine if individuals also search online in scenarios where they do report crimes to the police, such as in cases of MVT. The findings suggest that for crimes typically reported to the police, such as MVT, there is also significant online search activity (high correlation and small discrepancy in the MVT between GT and police data). This insight is crucial. Because when we hold all other conditions constant, as the victim's willingness to report crimes decreases (e.g., more sensitive crimes like domestic violence or rape), the propensity to search online will remain unaffected, but the accuracy of police data will decrease^[In 2019, 79.5% of motor vehicle theft victims reported the crime to law enforcement, whereas only 48.5% of burglary victims and 33.9% of rape victims did the same [@bjs2022ncvs_2019].].

For instance, considering MVT as an example, we can hold Types II, III, VI, and VII constant (both overreporting and underreporting scenarios in GT MVT and police MVT). These categories represent unknown discrepancies between GT and police data that could potentially decrease the correlation coefficient or the linear relationship between GT MVT and police MVT data. Nevertheless, under the crime category of MVT, the linear relationship between GT MVT and other types of police data is confirmed using rigorous methods and data, ensuring that the discrepancy from Types II, III, VI, and VII is minimal and does not compromise the validity of GT in estimating crime statistics. 

Now, holding all other variables constant, if the focus shifts to crimes that are heavily underreported to the police, such as rape or domestic violence, the primary change observed would be in the proportion of victims who report the crime to the police. Consequently, there would be a shift from Type I to Type II scenarios—Type I would decrease as victims tend not to report to the police, while Type II would increase as victims refrain from reporting to the police yet continue their online search behavior. All other GT-related scenarios remain unchanged and are not influenced (Type III, VI, VIII remain unchanged). The increase in errors thus is confined to police data, attributed from a decline in reporting rates. Thus, the value of GT data lies in its capacity to reveal unknown/undetectable aspects of official crime statistics. This finding aligns with @liu2023big observations on GT data's effectiveness in identifying underreported cases of rape crimes across the United States. 

@GIBSON2008247 contended the reliability of crime data is proportionate to its reporting rates. As previously discussed, crime statistics are often considered within a multiplicative model, the reporting rate, $U$, significantly affects the accuracy of crime data [@GIBSON2008247; @pina2022impact]. This is evident when we revisit \eqref{myeqn11}, which reveals that in the multiplicative model, the extent of measurement error is directly proportional to the reporting error, $U$. As highlighted in @liu2023big, for heavily underreported crimes such as rape, the correlation between GT Rape and other official violent crime rates (such as homicide and robbery) is lower compared to more reliably reported crime statistics, such as MVT. This decrease in correlation serves as further evidence that our counterfactual reasoning is valid, especially as the influence of $U$ -- representing a type II scenario—also increases.

# Future Development of Google Trends on Criminology
Google Trends, as a form of digital trace data that is continuously on and honestly recording user's search terms, providing a unique insight into people's private intentions which they might otherwise be reluctant to disclose publicly. This method proves particularly useful when traditional data sources are unavailable or suffering from underreporting. Future research could benefit significantly from merging multiple search terms in Google Trends to capture the "victim's tone." For instance, in cases such as rape crime statistics, the established validity of the Google Trends approach from this dissertation allows for the use of search terms related to rape to estimate rate estimates from Google Trends. The GT estimates can then be compared with national data. Moreover, this approach facilitates innovative research opportunities, such as analyzing GT rape estimates during the summer months in college towns or DMAs with large student populations to assess significant changes in activity when students are away. Such studies could reveal previously unexplored dynamics in rape rates across various geographic units and times.


Similarly, when data is scarce due to regulatory constraints, such as with gun sales data, GT can play an important role. For example, federal authorities are explicitly prohibited by law from creating a database of firearm ownership or using data collected through the National Instant Criminal Background Check System (NICS) to develop a record of gun owners [@rand_gun]. This restriction leaves the NICS monthly security check data at the national level as the only available source for research, which lacks detailed sub-national breakdowns. By using Google Trends to simulate the search behavior of potential gun buyers, researchers can generate valuable data on gun purchases at state or DMA levels. This information can then be used to explore correlations between the volume of the intention of gun purchases and fluctuations in gun violence within those regions, providing critical insights into the impact of gun sales on public safety.


# Limitations and Challenges

The study acknowledges several limitations that must be considered. The behaviors and interests observed online might not accurately mirror real-world behaviors, and certain segments of the population may be underrepresented in digital spaces. Firstly, the representativeness of digital trace data presents a challenge. These data sources typically reflect the online behaviors of specific user demographics, which could result in a skewed or biased sample. Populations without internet access or those not using prevalent languages, such as English, may not be represented in the GT MVT data. Furthermore, it is important to note that not all individuals or demographic groups participate in online activities with the same frequency, potentially introducing a source of selection bias [@robertson2018auditing]. However, given that the Google Search engine dominates over 93% of online search activity and its users span different demographics [@center2012searchengine; @statcounter2022search], these factors might mitigate some concerns regarding representation and accuracy.

Secondly, Google Trends data, in particular, has its own set of limitations. The data is normalized across regions or time, providing relative interest scores rather than absolute counts. This normalization process can be influenced by changes in overall search volume, potentially confounding the interpretation of trends. Additionally, Google Trends does not provide the actual number of searches of the interested terms, making it challenging to quantify the absolute prevalence of certain queries. 

Third, the dissertation exclusively relies on a single type of crime estimates, specifically motor vehicle theft. While MVT is chosen for its high reliability and validity, it does not entirely capture the diversity of crime statistics. To address this limitation, diversifying data sources is crucial, so I try to incorporate as many data sources as possible. Future studies can expand upon the results by exploring the dark figure of crime of official crime statistics, encompassing a broader range of crime types.

Another limitation is the lack of finer resolution in both geographic and temporal units of analysis. This study primarily operates at a broader geographic level, which is restricted by the geographic level provided by Google Trends, potentially overlooking localized variations in crime. A finer-grained analysis at the neighborhood level could reveal nuances and variations not captured in the current study. The other limitation of this study is that the data is restricted to monthly and yearly intervals.  The absence of daily or real-time data imposes constraints on the precision of temporal analyses. A more granular temporal resolution could provide a more nuanced exploration of short-term fluctuations and timely responses to events. On the other hand, applying state-level analysis to meso and micro theories that focus on neighborhoods, such as social disorganization and routine activities related to crime situations, presents another limitation due to the availability of GT data at only macro level. 

Another limitation of this dissertation is the high incidence of missing values in the data used. While the NICB and CFS 911 data has no missing values, UCR data experiences significant gaps, with nearly 13.76% of values imputed at the agency level for the state-year/DMA-year level analysis and nearly 7.15% at the agency level for the DMA-month level analysis. NCVS data, although experiencing fewer missing values, is limited by an insufficient sample size, which constrains its testing capability. Despite UCR having 7.15% to 13.76% portion of its values imputed, its comparison with other crime data which has none imputed values remains valid at state-year, DMA-year, and DMA-month level, which alleviates concerns about the impact of missing values on this research.

In the DMA-month level analysis, the CFS 911 data is sourced from only one metropolitan police department for each of the 15 DMAs providing CFS 911 data. As indicated in Table \ref{tab:Appendix1}, the CFS 911 data is acquired from just one metropolitan police department, even though the DMA area includes more than one police jurisdiction. Consequently, I use data from one police department's 911 calls to represent the entire DMA’s 911 data. This is a shortcoming of the analysis, as a single agency’s data is not entirely representative of a DMA. Acquiring 911 data from multiple police departments within a DMA would make the analysis more comprehensive and could actually strengthen the association between GT and CFS 911 data. 

In summary, while digital trace data and Google Trends offer valuable insights, researchers should remain mindful of these limitations, critically assess the generalizability of findings, and consider complementary data sources to enhance the robustness and validity of their research. Furthermore, while the study contributes valuable insights using MVT data, future research endeavors should consider incorporating diverse data sources, conducting neighborhood-level analyses, and exploring different time units such as day or real-time analysis for a comprehensive and nuanced understanding of crime statistics.


# Contribution on Methodology
This dissertation is the first to apply rigorous analyses to substantiate the linear relationship between Google Trends motor vehicle theft (GT MVT) and other MVT data sources including UCR, NICB, NIBRS, CFS 911, and NCVS. I have detailed how to extract GT data from the Google Trends website; the replicable data and Python and R code are available at https://github.com/Yu-Hsuan-Liu. The data query initiated with a set of terms: 'car stolen + find stolen car + report police stolen car + insurance car stolen - dream - check'^[These terms were developed in 2018 during my first Ph.D. semester using the 'related terms' feature on Google Trends. This approach helped gather MVT-related terms others used and combine them. Crucially, this set of terms was established well before 2020, precluding any possibility of 'data mining' to 'fit' the trends of GT MVT with other crime data or covariates.] utilizing the 'related terms' feature on the Google Trends website ensured that the selection of terms was not arbitrary. Moreover, by combining multiple terms, the sample size pulled from GT was increased. This is an essential step since crime incidents are rare. The more terms combined, the more sample points appear, providing valuable data insights.

The second step was to collect as many samples from GT as possible, whether by region or by time units. The more samples pulled from GT, the more comprehensive the data is, and the more it represents the entire universe of search activity. It is noteworthy that repeated sampling required waiting at least three hours for GT to refresh the sampled data; otherwise, if we repeatedly sampled 100 samples within one hour, they would all be estimates from the same sample, rendering the process meaningless.

Finally, to facilitate comparison with GT data, other variables needed to be 'normalized' from 0 to 100, just as GT does. Mere correlation with GT data or even correlation in time series would not be sufficient. To account for autocorrelation or endogeneity, I applied rigorous statistical methods, utilizing various crime data or common crime covariates to provide supporting evidence for its validity. I took the most conservative approach possible in this dissertation, and the findings support the utility of GT. This is the first study in the criminology field to apply time-series data on various crime statistics and demonstrate the utility of GT. It marks progress in the discussion of digital trace data in criminal justice studies.

# Conclusion, Contribution and Policy Implications

"Measuring crime has always been one of the most challenging tasks for criminal justice researchers" [@maxfield1999national, p.119]. Thus, finding a metric that accurately captures "true" crime figures without measurement error is unrealistic. Due to the underreporting issue in crime statistics, an ideal measurement for crimes that heavily underreported would entail recording victimization without victims' fear, concern, or reluctance to report the incident to authorities, as well as without privacy concerns during interviews. Furthermore, it requires an adequate sample size and timely recording close to the event of the victimization. The findings from this dissertation suggest that GT MVT data provides credibility alongside official MVT data and has the potential to enhance the estimation of victimization. As noted by the @bjs2022ncvs_2019, it is crucial to provide other resources than UCR or NIBRS in understanding the sub-national crime estimates. The GT crime estimates could be one of the few approaches that effectively "illuminate" the dark figure of crime.

The study's value lies not just in uncovering GT's usefulness in social science but also in building the credibility, reliability, and validity of GT data using the already established metric, MVT. After establishing MVT's credibility, researchers can confidently extend GT's utility to other crime statistics, even when its correlations with the official crime statistics are low, such as rape or domestic violence estimates. GT becomes a valuable resource providing victimization estimates other than NCVS, to analyze victimization risks through victims' online search data at the state, and metro levels.

One study utilizing Google Trends data to 'audit' suspicious company activities in revenue management could offer insights into further applications of GT. @chiu2023using found that Google Search data can discover dishonest revenue reporting by companies. Their methodology identified that firms whose quarterly Google Trends search volume ranked in the bottom quantile of their industry, while their reported revenue growth ranked in the top quantile, were 165% more likely to restate their initial revenue reports to the SEC. This study shows that easily accessible, free data from Google Trends, which is independent of management control, can be a valuable new tool for auditors to detect dubious revenue reporting. Similarly, if we can apply GT in crime estimates and go beyond MVT, we can move to other crimes suffering from underreporting. In this way, we may use GT to 'audit' sketchy reporting statistics, which has low records in official statistic data, but has a high GT search in victimizations, as demonstrated in @liu2023big, which identified potential underreporting areas for rape crime.


The author acknowledges the inherent characteristics of digital trace data--dirty, incomplete, and algorithmically confounded. Despite its limitations, this dissertation confirms the reliability and validity of a metric designed to detect real-world victimization incidents, thus enhancing the toolkit for crime statistics. It confirms social theories through the human and computer interaction and augments traditional crime statistics collection methods. I demonstrate the unique ability of digitally traced data--Google Trends, a free and open tool--to scrutinize crime reports made to the police, reveal the often unseen "dark figure" of crime, and the potential to 'audit' the accountability of crime statistics from police.


Further research using Google Trends can extend beyond traditional crime statistics. GT can also help in understanding people's fear of crime or interest in purchasing firearms and gun ownership. By analyzing search trends related to fear of crime or gun ownership, GT provides insights that were previously difficult to obtain, acting as a proxy measure for understanding fear of crime and gun sales—an area that remains underdeveloped due to the scarcity of relevant data. Ultimately, the versatility of GT is extensive, making it applicable to a wide range of topics within criminal justice and criminology.

In conclusion, this dissertation has successfully demonstrated that Google Trends can serve as a reliable proxy for traditional crime data. The significant correlation between Google Trends data and motor vehicle theft statistics, validated across state-year, DMA-year, and DMA-month scales, reinforces the utility of digital trace data in capturing public interest and concern, which often correlates with actual crime trends. Moreover, the nuanced analysis using various statistical methods such as correlation analysis, GLS regression, and fixed-effect models, have provided a robust framework for understanding the dynamics of crime reporting and the potential biases inherent in traditional crime statistics. These methods have helped to identify and adjust for potential errors and biases, ensuring a more accurate representation of crime trends.


<!-- Chapter 5

## The Application of GT MVT in "Complementing" Official Crime Statistics


I will employ the Vector Auto-regression Model (VAR) in the time series analysis to explore the relationship between GT MVT and official MVT and conduct forecasting. The VAR model is designed for multivariate time series forecasting and is applicable to both stationary and non-stationary time series data [@zivot2006vector; @haslbeck2021tutorial; @freeman1989vector; @toda1994vector; @stock2001vector; @holtz1988estimating; @abrigo2016estimation]. In contrast to uni-variate models that focus solely on a single variable, the VAR model proves advantageous by concurrently capturing the interdependence and dynamic interactions among multiple variables observed over a period. This approach becomes particularly beneficial in scenarios where the behavior of a variable is influenced not only by its own historical values but also by the historical values of other variables within the system. Thus, applying VAR in the time-series analysis facilitates a comprehensive exploration of the intricate relationship dynamics between GT MVT and official MVT, enhancing the accuracy of forecasting analyses.

The Long Short-Term Memory (LSTM) neural network model stands out as a highly effective approach for time series forecasting when dealing with multivariable data [@tai2015improved; @hochreiter1997long; @gers2000learning; @salman2018single; @staudemeyer2019understanding; @sherstinsky2020fundamentals; @graves2012long]. LSTM belongs to the family of recurrent neural networks (RNNs) but are uniquely designed to address the challenges associated with capturing and modeling long-term dependencies within sequential data. Their specialized architecture makes them especially adept at tasks involving time series forecasting. One of the key strengths of LSTMs lies in their ability to automatically learn and extract pertinent features from historical sequences, enabling them to effectively capture intricate temporal patterns. In the context of this project, the implementation of an LSTM model for time series forecasting will be carried out using the TensorFlow deep learning library in Python, leveraging its capabilities for robust and accurate forecasting applications.

The training phase involves utilizing a dataset that includes both Google Trends MVT data and recorded official motor vehicle thefts in the chosen regions over the same timeframe. Typically, this dataset is divided, allocating, for instance, 80% for training and 20% for testing purposes. Accuracy is evaluated by comparing predicted MVT cases with actual incidents in the test data. The model demonstrating the highest accuracy is then selected for forecasting future MVT occurrences when official data is unavailable at specific regional levels. Both the VAR model and the LSTM approach offer in-depth analyses of underlying patterns and trends in the data, contributing to the formulation and refinement of urban safety policies and strategies.

### Forecasting Models (VAR and LSTM)

The VAR model and Long LSTM model are anticipated to exhibit proficient forecasting abilities. The potential of internet search trends in augmenting established crime investigation methods will be highlighted by the accuracy of these models in forecasting MVT based on GT data.


At the state-annual and MSA-annual levels, I will conduct correlation analyses and employ a fixed effect model to assess the relationship between GT MVT and official crime statistics. These evaluations will employ common covariates of crime, such as changes in the percentage of concentrated disadvantages, residential mobility, the heterogeneity index, and the percentage of foreign-born individuals. The goal is to assess the explanatory power and model fit of both GT data and other official crime statistics.

On the city-monthly level, I will investigate the link between GT MVT and MVT data in CFS 911 records. To achieve this, I will utilize the fluctuation of semiconductor prices as an instrumental variable for automobile prices, aiming to understand its impact on GT MVT and its connection to MVT in CFS 911 data. Furthermore, I will leverage the COVID-19 lockdown as a natural experiment to examine its effects on GT MVT and MVT data in CFS 911 records with interrupted-time series analysis. 

In the final analysis, I will employ two time-series forecasting models, Vector Autoregression Model (VAR) and Long Short-Term Memory (LSTM), to thoroughly investigate the effectiveness of GT MVT in predicting MVT from various official crime statistics, such as UCR, NCVS, and CFS 911 data. This examination will take place at the city-monthly level, which offers a closer view of the data.

In doing so, my aim is to uncover the nuances of GT's predictive power and its ability to provide insights into the dynamics of crime statistics. This comprehensive analysis will allow us to not only assess the potential advantages of using GT data in conjunction with official crime statistics but also explore any limitations or challenges that may arise during this integration.



## Internet Usage & The Validity of GT MVT — Interaction Terms (Metro Level)
To assess whether the association between GT estimates and official crime data has increased overtime within the rise in internet access I use interaction terms. Interaction terms will allow us to vary the slope of a single variable based on the value of another variable. Interaction terms are essential for testing conditional hypotheses. For instance, 
Arnio and colleagues (2012) employed an interaction terms model to evaluate how levels of structural disadvantage condition the impact of foreclosure on robberies. Rosenfeld and colleagues (2019) use income as an interaction term to condition the effect of inflation on crime in 17 cities in the United States from 1960 to 2013. 
Internet usage may substantially impact Google's estimates when examining the reliability of GT MVT. Using GT MVT as an independent variable and official crime rates as a dependent variable, I will condition the effect of GT MVT on official crime rate by the moderator of internet usage. This experiment aims to confirm our assumption that Google Search statistics are most valuable when internet usage is prevalent, and Google Search is more accurate when more individuals use the internet.



Cautious about the "weak instrument" [@kovandzic2016police]. We need to apply for other sensitivity test (https://link.springer.com/article/10.1007/s10940-015-9257-6).
### Semi-conductor price as an Instrumental Variable (Validity Comparison)
### GT MVT as an Instrumental Variable (Measurement Error Correction)

-->

<!--
# Litmitations
## Are Criminal Cases Comparable? Does every count of crime can be viewed as the same of similar "counts?"
Do these "measures" have "equal" intervals? [@bruton2000reliability]
The Dilemma of Quantity and Quality 
A homicide with self-defense and Mass Killing? 
Mental Illness v.s. Gang Revenge

-->



\FloatBarrier

\centering
\fancyhead[L]{Appendices}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}
\chapter{APPENDICES}

\beginsupplement
\centering{\textbf{\normalsize Appendix A - Figures and Tables}}



```{r dmayearlist, results='asis', message=FALSE, warning = FALSE, echo = FALSE}

summary_by_dma <- summary_by_dma %>%
  mutate(across(where(is.numeric), ~ round(., 1)))


summary_by_dma <- summary_by_dma %>%
  mutate(orders = row_number()) %>%
  dplyr::select(orders, everything())

colnames(summary_by_dma) <- c("No.", "DMA", "GT", "UCR", "NCVS", "NICB")

dma_year_summary_table_counts <- knitr::kable(summary_by_dma, 
                                        format = "latex", booktabs = T, row.names = F,
                                        linesep = "", longtable = T,
                                        caption = "DMA-year (2011-2022) Mean Value across Years for Each DMA/Data Type") %>%
  kable_styling(latex_options = c("scale_down", "repeat_header"), 
                bootstrap_options = "striped",
                font_size = 7,
                full_width = F)




print(dma_year_summary_table_counts)

```
\clearpage
\newpage

```{r dmamonthlist, results='asis', message=FALSE, warning = FALSE, echo = FALSE}


summary_by_dma_m <- summary_by_dma_m %>%
  mutate(across(where(is.numeric), ~ round(., 1)))



summary_by_dma_m <- summary_by_dma_m %>%
  mutate(orders = row_number()) %>%
  dplyr::select(orders, everything())

colnames(summary_by_dma_m) <- c("No.","DMA", "GT", "UCR", "NIBRS", "CFS911")

dma_year_summary_table_counts_m <- knitr::kable(summary_by_dma_m, 
                                        format = "latex", booktabs = T, row.names = F,
                                        linesep = "", longtable = T,
                                        caption = "DMA-month (2017-2022) Mean Value across Months for Each DMA/Data Type") %>%
  kable_styling(latex_options = c("scale_down", "repeat_header"), 
                bootstrap_options = "striped",
                font_size = 7,
                full_width = F)



print(dma_year_summary_table_counts_m)

```
\clearpage
\newpage

```{r Appendix0, results='asis', message=FALSE, warning = FALSE, echo = FALSE}
#get the hyperlink format

month_files <- read.csv("C:/Users/tosea/GT-MVT-Monthly-City/dma_counts.csv", 
                                  row.names = NULL)


month_counts  <- knitr::kable(month_files, 
             align = "cll",
             format = "latex", 
             booktabs = T, 
             row.names = F, 
             linesep = "", 
             longtable = T,
             caption = "Counts of DMA-Month Files", 
             escape = F,  
             col.names = linebreak(
               c("No.", "DMA", "File Count"), align = "c")) %>%
  kable_styling(latex_options = "hold_position", font_size = 8, 
                full_width = F) 

print(month_counts)

```

```{r Appendix01, results='asis', message=FALSE, warning = FALSE, echo = FALSE}
#get the hyperlink format

msa_dma_walk <- read.csv("dma_msa_walk2_for_dissertation.csv", row.names = NULL)

msa_dma_walk <-  msa_dma_walk %>% dplyr::select(No.,DMA, MSA.Name)
msa_dma_table  <- knitr::kable(msa_dma_walk, 
             align = "cll",
             format = "latex", 
             booktabs = T, 
             row.names = F, 
             linesep = "", 
             longtable = T,
             caption = "DMA-MSA Crosswalk", 
             escape = F,  
             col.names = linebreak(
               c("No.", "DMA", "MSA"), align = "c")) %>%
  kable_styling(latex_options = "hold_position", font_size = 8, 
                full_width = F) 

print(msa_dma_table)

```

\clearpage
\newpage

```{r Appendix1, results='asis', message=FALSE, warning = FALSE, echo = FALSE}
#get the hyperlink format
list911 <- list911 %>%
  mutate(Data.Source = paste0("\\href{", Data.Source, "}{Source Link}"))



table_911 <- knitr::kable(list911, 
             align = "cllccrrrrll",
             format = "latex", 
             booktabs = T, 
             row.names = F, 
             linesep = "", 
             longtable = T,
             caption = "Sources and Timeframe of the Official Motor Vehicle Theft Monthly CFS 911 Data", 
             escape = F,  
             col.names = linebreak(
               c("No.", "Google Trends DMA", "Law Enforcement Agencies", "Starting Year", "Ending Year", "Raw MVT", "Duplicated MVT", "Duplicated MVT(\\%)","Population (2020)", "Geographic Unit", "Data Source"), align = "c")) %>%
  kable_styling(latex_options = "hold_position", font_size = 6.5, 
                full_width = F) %>% 
  #column_spec(column = 1, width = "12em") %>%
  footnote(number = c("Population data is from Census Bureau (2023)"), footnote_as_chunk = T, threeparttable = T) %>%
  column_spec(1, width = "0.5cm") %>%
  column_spec(2, width = "2.5cm") 
  #column_spec(7, width = "1.5cm") %>%


cat("\\begin{sidewaystable}[!tp]\n")
print(table_911)
cat("\\end{sidewaystable}\n")
```
\clearpage


```{r Appendix1_1, results='asis', message=FALSE, warning = FALSE, echo = FALSE}

car_price_region_dma_walk <- read.csv("DMA_region_walk.csv") 

table_car_walk <- knitr::kable(car_price_region_dma_walk, 
             align = "cll",
             format = "latex", 
             booktabs = T, 
             row.names = F, 
             linesep = "", 
             longtable = F,
             caption = "DMA and Region Walkcross for Car Price Index", 
             escape = F,  
             col.names = linebreak(
               c("No.", "Car CPI Region", "DMA"), align = "c")) %>%
  kable_styling(latex_options = "hold_position", font_size = 9) %>% 
  #column_spec(column = 1, width = "12em") %>%
  #footnote(number = c(""), footnote_as_chunk = T, threeparttable = T)

print(table_car_walk, type="latex", table.placement = "!htb", scalebox = 0.8)

```


<!--
```{r Appendix2,  results='hide', message=FALSE, warning = FALSE, echo = FALSE}
# For state-year level data
file_path= "state_year_level_correlation_matrix.png"
png(height=800, width=800, file=file_path, type = "cairo")

state_data.cor <- cor(merged_state_year_data[variables_state], method = "spearman")

colnames(state_data.cor) <- variable_name_state
rownames(state_data.cor) <- variable_name_state

corrplot(state_data.cor, method = 'circle', diag = FALSE, type = 'lower', tl.cex = 1.5)
dev.off()
```



\clearpage \newpage
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\linewidth]{UCR_MVT_florida_missing.png}
  \caption{The Systematic Smooth Line for Florida UCR MVT Data}
  \caption*{The smooth decrease in data points from December 2017 to June 2018, followed by a smooth increase from June 2018 to December 2019, along with other smooth trends (December 2020, December 2021, and December 2022), suggests that substantial and systematic missing values in the Florida data have led to these smooth transitions due to interpolation between different data points. It is impractical to rely on several sporadic data points to estimate the entire 72-month trend for a DMA. Therefore, the UCR Florida data at the DMA-month level has been excluded from this analysis}
  \label{fig:figure_missing}
\end{figure}
\clearpage
-->

```{r Appendix3,  results='hide', message=FALSE, warning = FALSE, echo = FALSE}
# For state-year level data
file_path= "dma_year_level_correlation_matrix.png"
png(height=800, width=800, file=file_path, type = "cairo")

my_dma_data.cor <- cor(merged_dma_year_data[dma_variables], method = "spearman", use = "pairwise.complete.obs")

colnames(my_dma_data.cor) <- dma_variable_name
rownames(my_dma_data.cor) <- dma_variable_name

corrplot(my_dma_data.cor, method = 'circle', diag = FALSE, type = 'lower', tl.cex = 1.5)
dev.off()

```

\begin{figure}[tb]
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
    \caption{Correlation Matrix for State-Year Level MVT Data}
    \label{figure A.1.1}
    % Inserting R-generated plot, assuming the plot is saved as an image
    \includegraphics[width=\linewidth]{state_year_level_correlation_matrix.png}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.5\textwidth}
    \caption{Correlation Matrix for DMA-Year Level MVT Data}
    \label{figure A.1.2}
    % Inserting R-generated plot, assuming the plot is saved as an image
    \includegraphics[width=\linewidth]{dma_year_level_correlation_matrix.png}
  \end{subfigure}
  \caption{Correlation Matrix of GT and Official MVT}
  \label{figure A.1}
\end{figure}


\clearpage \newpage
\begin{sidewaysfigure}[!h]
  \centering
  \includegraphics[width=\linewidth]{mvt_year_state.png}
  \caption{Motor Vehicle Theft Statistics by State and Year}
  \label{fig:figure A.2}
\end{sidewaysfigure}
\clearpage


\clearpage \newpage
\begin{sidewaysfigure}[!h]
  \centering
  \includegraphics[width=\linewidth]{mvt_year_dma.png}
  \caption{Motor Vehicle Theft Statistics by DMA and Year}
  \label{fig:figure A.3}
\end{sidewaysfigure}
\clearpage


\clearpage \newpage
\begin{sidewaysfigure}[!h]
  \centering
  \includegraphics[width=\linewidth]{mvt_year_month_with_911_plot.png}
  \caption{Motor Vehicle Theft Statistics by DMA and Month}
  \label{fig:figure A.4}
\end{sidewaysfigure}
\clearpage

\clearpage \newpage
\begin{sidewaysfigure}[!h]
  \centering
  \includegraphics[width=\linewidth]{mvt_cpi_month_dma.png}
  \caption{Monthly MVT and CPI by DMA with Smoothed Trend Lines}
  \label{fig:figure A.5}
\end{sidewaysfigure}
\clearpage

\clearpage \newpage
\begin{sidewaysfigure}[!h]
  \centering
  \includegraphics[width=\linewidth]{its_plot.png}
  \caption{Interrupted Time Series of COVID-19 Lockdown on MVT Data}
  \label{fig:figure A.6}
\end{sidewaysfigure}
\clearpage





```{r, results='asis', echo=FALSE}

its_data <- its_data %>%
  group_by(DMA) %>%
  arrange(year_month) %>%
  mutate(
    GT_MVT_lag = dplyr::lag(GT.MVT, 1),
    log_GT_MVT_lag = dplyr::lag(log.GT.MVT, 1),
    GT_MVT_lead = dplyr::lead(GT.MVT, 1),
    log_GT_MVT_lead = dplyr::lead(log.GT.MVT, 1)
  ) %>%
  ungroup()


regTS_GT_lag = gls(GT_MVT_lag ~ year_month + D + P, data = its_data, method="ML", na.action = na.omit)  
regTS_GTl_lag = gls(log_GT_MVT_lag ~ year_month + D + P, data = its_data, method="ML", na.action = na.omit)  

regTS_GT_lead = gls(GT_MVT_lead ~ year_month + D + P, data = its_data, method="ML", na.action = na.omit)  
regTS_GTl_lead = gls(log_GT_MVT_lead ~ year_month + D + P, data = its_data, method="ML", na.action = na.omit)  

stargazer( regTS_GT_lag, regTS_GTl_lag,
           regTS_GT_lead, regTS_GTl_lead,
           header = FALSE, 
           title = "Interrupted Time Series Analysis -- lagged GT MVT and lead GT MVT",
           dep.var.labels = c( "GT MVT", "GT MVT",
                              "GT MVT", "GT MVT"),
           column.labels = c( "lag", "(log)+lag",
                             "lead", "(log)+lead"),
           covariate.labels = c("Time", "Treatment", 
                                "Time Since Treatment"),
           omit.stat = c("rsq", "ll", "ser"),
           no.space = TRUE,
           table.placement = "!htb",
           notes.append = FALSE, 
           notes = c("* p<0.05; ** p<0.01; GT = Google Trends", "MVT = Motor Vehicle Theft"),
           notes.align = "l",
           digits = 2,
           column.sep.width = "-2pt",
           font.size = "small",
           df = F,
           star.char = c( "*", "**"),
           star.cutoffs = c(.05, .01),
           type = "latex",
           label = "tab:its_lag")
```



\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{GT_before_after_covid_lockdown.png}
  \caption{Google Trends Before/After Covid-19 Lockdown -- Computer, Weather, Recipe and Game}
  \label{fig:figure A.7}
\end{figure}



```{r, results = 'hide', echo=FALSE}

vif_results <- list()


state_plm_vif_model <- plm(formula1111, merged_state_year_data,index=c("State", "Year"), model = "pooling")

state_gls_vif_model <-gt_state_gls_common


dma_plm_vif_model <- plm(GT.MVT ~ Concentrated.Disadvantaged.Index + 
             Mobility.Index + Heterogeneity.Index + 
             Percentage.of.Foreign.Born + Percentage.of.Young.Males+
             Population..logged. + Average.Vehicle.HH +
             Percentage.of.Internet.Subscription.per.Household, 
           data = merged_dma_year_data, index=c("DMA", "Year"), model="pooling")

dma_gls_vif_model <- m1



vif_check_models <- list(state_gls_vif_model, state_plm_vif_model, 
                         dma_gls_vif_model, dma_plm_vif_model)

```


```{r, results='hide', echo=FALSE}
#Check VIF multicollinearity

vif_results <- list()

for (i in seq_along(vif_check_models)) {
  model <- vif_check_models[[i]]
  # VIF calculation for gls model
  vif_results[[i]] <- car::vif(model)
}

# Print or return VIF results
vif_df <- data.frame(
  `State-year (GLS)` = vif_results[[1]],
  `State-year (PLM)` = vif_results[[2]],
  `DMA-year (GLS)` = vif_results[[3]],
  `DMA-year (PLM)` = vif_results[[4]]
)
vif_df <- round(vif_df, 2)

```


```{r, results='hide', echo=FALSE}
# Assuming vif_df is already created as shown in your message
row_names <- c("Concentrated Disadvantaged Index", "Mobility Index",
               "Heterogeneity Index", "\\% Foreign Born", 
               "\\% Young Males", "Population (logged)", 
               "Average Vehicle HH", "\\% Internet Subscription HH")

# Set the row names
rownames(vif_df) <- row_names
```


```{r Appendixvif, results='asis', echo=FALSE}
kable_vif <- kable(vif_df, format = "latex", booktabs = TRUE, linesep = "", 
                   caption = "Variance Inflation Factor (VIF) of State-year and DMA-year Common Crime Covariates",
                   escape = FALSE, align = 'ccccc', col.names = linebreak(
               c("Variables", "State-year\n(GLS)", "State-year\n(FE)", "DMA-year\n(GLS)", "DMA-year\n(FE)"), align = "c")) %>%
  kable_styling(latex_options = "hold_position", font_size = 9) %>%
  column_spec(1, bold = TRUE) %>% 

# Print the kable table
print(kable_vif)

```


\FloatBarrier
\newpage
\backmatter
\fancyhead[L]{References}
\fancyhead[R]{}
\fancyfoot[C]{\thepage}

\chapter{REFERENCES}

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{6pt}
\noindent

<div id="refs"></div>
\clearpage
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand*{\theHtable}{\thetable}

<!--
\clearpage
\newpage


# Appendix B - Supplement Materials
## Discussion of Population on Crime Statistics

\RaggedRight
\setlength{\parindent}{1.27cm}
-->


<!--
\addcontentsline{toc}{chapter}{APPENDICES}
-->





<!--
D is independent from Z given Y [@d2010new]. $D \perp\kern-5pt\perp Z \mid Y^*$.

Denote $U$ as the reporting error which equals to $y^*-y = y^*-Dy^*=(1-D)y^*=U$, $\hat{U} = E[U|D]$ and $0 \leq \hat{U} \leq 1$. When the average reporting rate $\hat{D}$ increases, the expected reporting error $\hat{U}$ will decrease. For example, the reporting error of $\hat{D} = 0.9$ is $(1-0.9)y^*=0.1y^*$, and the reporting error of $\hat{D}=0.3$ is $(1-0.3)y^* = 0.7y^*$. In this way, the reporting error $\hat{U}$ has a reverse relationship with the reporting rate, $\hat{D}$






Introduction





Sean's Notes

[
The partial correlation coefficients are most attenuated for crimes that are the least reliably measured. Robbery and assault have the lowest reporting rates in Table 1 and the greatest measurement error in both the between effects and fixed effects results in Table 2. When reported rates of these two types of crime are the dependent variable, the partial correlation with inequality is less than 20% of the corresponding coefficient for experienced crime, averaged over the three sets of estimation results in Table 3. Conversely, car theft has the highest reporting rate, the lowest measurement error in Table 2 and the smallest attenuation when using reported rather than experienced crime in Table 3.

The analyses reported here suggest time-varying, mean-reverting, measurement errors in official crime data. Such errors are not amenable to standard treatments like fixed effects and instrumental variables. Other methods such as reverse regression and bias bounds (Black et al., 2000) may have greater efficacy, which is a topic for further research. In the absence of these appropriate treatments, the measurement errors in reported crime data are likely to attenuate estimated correlations between inequality and crime.

[@GIBSON2008247]
]





 Asher (Reference Asher1974)
 
 Krueger and Laitin (Reference Krueger and Laitin2004, 8) state that “terrorism reports produced by the U.S. government do not have nearly as much credibility as its economic statistics, because there are no safeguards to ensure that the data are as accurate as possible and free from political manipulation.” 
 
 Imai and Yamamoto (Reference Imai and Yamamoto2010, 543) conclude that “existing research has either completely ignored the problem or exclusively focused on classical measurement error in linear regression models where the error is assumed to arise completely at random.”
 
 For instance, Lacina and Gleditsch (2012, p. 1118) state that the Peace Research Institute Oslo battle deaths data may overstate the true death toll due to a “blurring between expertise and advocacy” as “experts may overestimate deaths because they seek to draw attention to ongoing conflicts or to underline the importance of the conflict on which they specialize.” 
 
 
Measurement of corruption, or criminal activity more generally, may also suffer from one-sided measurement error. Goel and Nelson (Reference Goel and Nelson1998) and subsequent research on corruption in the United States utilize data on the number of public officials convicted for abuse of office. This measure only captures illegal activities that are discovered and prosecuted. Measurement of local pollution, necessary for analyses of environmental justice or other determinants of local environmental conditions, may also suffer from one-sided measurement error. For example, Daniels and Friedman (Reference Daniels and Friedman1999) and others rely on self-reported emissions by establishments in the United States obtained from the Toxic Release Inventory. The self-reported and public nature of the data make systematic under-reporting likely. Finally, survey data on actions or attitudes that may lead to social judgement may suffer from skewed measurement error, including data on political or charitable contributions, attitudes concerning racial or ethnic issues, etc.

Wiedmann (Reference Wiedmann2016, 206–207) states: “Thus, for scholars trying to explain the occurrence of political violence, this means that their dependent variable may be measured with error. This alone would not be a problem if this error was random; however, as is well known, systematic measurement error that is correlated with an independent variable can introduce statistical bias and lead to erroneous conclusions. Both conceptually and methodologically, the new wave of event-level analysis has not taken this issue serious enough.”



::: ]


"Quite often, we find evidence that errors are negatively correlated with true values.

The usefulness of validation data in telling us about errors in survey measures can be enhanced if validation data is collected for a random portion of major surveys (rather than, as is usually the case, for a separate convenience sample for which validation data could be obtained relatively easily); if users are more actively involved in the design of validation studies; and if micro data from validation studies can be shared with researchers not involved in the original data collection. 

[Chapter 59 - Measurement Error in Survey Data☆
Author links open overlay panelJohn Bound *, Charles Brown, Nancy Mathiowetz 2010]"


Finally, errors in CFS  crime  counts vary systematically  across  space. Dispatch data  are more likely to under-count the total number of crimes that come to the attention of the police in neighborhoods where residents believe that officers respond more slowly to their calls, where residents are more fearful of crime, and where they experience more criminal victimizations. That overall errors in dispatch data are related to such neighbor- hood   characteristics   has important implications  for   research on the macro-level correlates of crime.  Research  relying on  CFS data to measure crime may produce misleading results about how crime is related to other neighborhood  characteristics.

[KLINGER, MEASUREMENT ERROR IN CALLS-FOR- SERVICE AS AN INDICATOR OF CRIME, 1997]

-----------------------------------------------------------------------------

# Chapter Summary



-->

<!--

\noindent When the instrumental variable is not possible, the reversed regression coefficient of $x$ on $y$ is $plim_{N\rightarrow\infty}\hat{\beta}^{-1}_{x,y} = \beta + \frac{\sigma_{\varepsilon}^2 + \sigma_{\nu}^2}{\beta\sigma_{x^*}^2}$, and can be used as an upper bound of $\beta$ when $\beta > 0$. Under A1-A5, it implies that the lower and the upper bound of $\beta$ would be [@pepper2010measurement]:

\begin{equation}
plim_{N\rightarrow\infty}\hat{\beta}_{y,x} < \beta < plim_{N\rightarrow\infty}\hat{\beta}^{-1}_{x,y} \label{myeqn9}
\end{equation}

\noindent If we further relax A4, the lower bound of $\beta$ become $plim_{N\rightarrow\infty}\hat{\beta}_{y,x}=\beta\frac{\sigma_{x^*}^2+\sigma_{x^*,\mu}}{\sigma_{x^*}^2+\sigma_{\mu}^2+2\sigma_{x^*,\mu}}$ and the upper bound of $\beta$ become the probability limit of $\beta$ with instrumental variable of $x^*$. The final lower and upper bound when A4 is relaxed would be [@pepper2010measurement]: 

\begin{equation}
plim_{N\rightarrow\infty}\hat{\beta}_{y,x} < \beta < plim_{N\rightarrow\infty}\hat{\beta}_{y,x(z)}^{IV} \label{myeqn10}
\end{equation}

\noindent The inequalities reverse when $\beta < 0$. 


 Chapter 4 
\newpage
\fancyhead[L]{Chapter 4: MOTOR VEHICLE THEFT}
\fancyhead[R]{\thepage}
\fancyfoot[C]{}

\chapter{Chapter 4: MOTOR VEHICLE THEFT}
# The Reliability of Motor Vehicle Theft in Crime Statistics

# The Criminological Theories Perspective on Motor Vehicle Theft

# Research on Motor Vehicle Theft

# COVID-19, Semiconductors and Motor Vehicle Theft




Motor vehicle theft, also known as auto theft or car theft, is defined as the criminal act of stealing or attempting to steal a motor vehicle (National Crime Prevention Council, 2019). According to the FBI’s Uniform Crime Reporting Program (UCR), motor vehicle theft is the most commonly reported property crime in the United States, with an estimated 748,841 motor vehicle thefts reported in 2018 (FBI, 2019). Research has shown that motor vehicle theft is a serious problem, with a significant economic impact on individuals, businesses, and society as a whole (Lundberg, 2018).


## Car Price and Motor Vehicle Theft
The relationship between car prices and motor vehicle theft has been studied by numerous criminologists over the years, with varying results. In a study conducted by D.T. Farrington and D.J. Walsh (1985), the authors found that car prices had a significant effect on motor vehicle theft rates, even after controlling for other factors such as the degree of urbanization and the availability of public transport. Their findings indicated that the higher the price of a car, the greater the likelihood of it being stolen. In a similar study conducted by K.A. Gartner and L.P. Bloch (1988), the authors found that the price of a car was an important factor in determining motor vehicle theft rates, but that other factors such as the availability of public transport had a greater impact on the rate of motor vehicle theft.

In more recent studies, the relationship between car prices and motor vehicle theft has been looked at from a different angle. F.T. Cullen and D.J. Levitt (1999) investigated the impact of car prices on the incidence of motor vehicle theft in a sample of urban counties in the United States. They found that car prices had a significant effect on motor vehicle theft rates, with higher car prices leading to a greater incidence of theft. They also found that the availability of public transport had a small but statistically significant effect on the rate of motor vehicle theft.

In a study conducted by M.S. LaFree (2003), the author found that the price of cars was significantly related to motor vehicle theft rates, but that other factors such as the availability of public transport and the level of urbanization had a greater impact on theft rates. LaFree's findings also indicated that higher car prices were associated with lower motor vehicle theft rates in areas with higher levels of urbanization.

Finally, in a study conducted by M.D. Maltz and T.A. Piquero (2005), the authors found that car prices had a significant effect on motor vehicle theft rates, but that the effect was mediated by other factors such as the availability of public transport and the level of urbanization. The authors also found that car prices had a greater influence on motor vehicle theft rates in areas with higher levels of urbanization.

Overall, the research conducted on the relationship between car prices and motor vehicle theft suggests that car prices have a significant effect on motor vehicle theft rates, but that other factors such as the availability of public transport and the level of urbanization also play an important role. Moreover, the findings of these studies indicate that higher car prices are associated with lower motor vehicle theft rates in areas with higher levels of urbanization.

## COVID-19 Lockdown and Motor Vehicle Theft
The impact of the COVID-19 pandemic on motor vehicle theft has been the focus of recent criminology studies. The pandemic has had a significant impact on our society, resulting in a drastic decrease in motor vehicle theft rates across the world. A study conducted by Gavilán-López et al. (2020) investigated the effects of the pandemic on motor vehicle theft in Spain. The study found that the total number of motor vehicle thefts decreased by over 50% during the COVID-19 lockdown period, compared to the same period in 2019. This decrease was mainly attributed to the reduction in mobility due to the lockdown measures, which resulted in fewer opportunities for thieves to commit motor vehicle theft. 

A similar study conducted by O’Connor et al. (2020) examined the effects of the COVID-19 lockdown on motor vehicle theft in the United Kingdom. The study found that the rate of motor vehicle theft decreased by 54% during the lockdown period compared to the same period in 2019. The authors attributed this decrease to the decreased levels of mobility due to the lockdown restrictions, which reduced the opportunity for thieves to commit motor vehicle theft. 

A study conducted by Liu et al. (2020) investigated the effects of the COVID-19 lockdown on motor vehicle theft in China. The study found that motor vehicle theft decreased by over 70% during the lockdown period compared to the same period in 2019. The authors attributed the decrease to the lockdown restrictions, which limited the mobility of people and the opportunity for thieves to commit motor vehicle theft. 

Overall, the studies conducted by Gavilán-López et al. (2020), O’Connor et al. (2020), and Liu et al. (2020) suggest that the COVID-19 lockdown had a significant impact on motor vehicle theft rates across the world. The lockdown restrictions and decreased levels of mobility due to the pandemic resulted in fewer opportunities for thieves to commit motor vehicle theft, leading to a dramatic decrease in motor vehicle theft rates.

# Chapter Summary
-->




<!--
##Code for figures
```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
## put all graphs here to avoid delay of knitr
df_dma_long <- merged_dma_year_data %>%
  pivot_longer(
    cols = c("GT.MVT", "UCR.MVT_normalized","NCVS.MVT_normalized", "NICB.MVT_normalized"),
    names_to = "Variable",
    values_to = "Value"
  )
```



```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
# Now, plot and map the aesthetics in a way that will include them in the legend
pdma <- ggplot(df_dma_long, aes(x=Year, y=Value, color = Variable, group = interaction(DMA, Variable))) +
  geom_line() +
  facet_wrap(~ DMA, scales = "free_y") +
  scale_color_manual(values = c("GT.MVT" = "#CC79A7", 
                                "UCR.MVT_normalized" =  "#0072B2",
                                "NCVS.MVT_normalized" = "#E69F00",
                                "NICB.MVT_normalized" = "#56B4E9"), 
                     labels = c("GT MVT", "UCR MVT (0-100)", 
                                "NCVS MVT (0-100)", "NICB MVT (0-100)"),
                     breaks = c("GT.MVT", "UCR.MVT_normalized", 
                                "NCVS.MVT_normalized", "NICB.MVT_normalized")) +
  scale_linetype_manual(values = "dashed", 
                        guide = guide_legend(title = "Event")) +
  #theme_minimal() +
  labs(
    x = "Year",
    y = "MVT (Scaled from 0 to 100)",
    color = "Variable"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),  # Adjust x-axis labels
    axis.text.y = element_text(size = 14),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12),   # Increase legend text size
    strip.text = element_text(size = 12) # Adjust y-axis labels
  )


ggsave("mvt_year_dma.png", pdma, width = 45, height = 30, bg = "white") 

```



```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
selected_df_dma_long <- df_dma_long %>%
  filter(DMA %in% c("Houston TX","Los Angeles CA", "New York NY",  "Seattle-Tacoma WA"))

selected_df_dma_long$DMA <- factor(selected_df_dma_long$DMA, 
                                   levels = c("Houston TX","Los Angeles CA", "New York NY",  "Seattle-Tacoma WA"))
```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
# Now, plot and map the aesthetics in a way that will include them in the legend
pdma <- ggplot(selected_df_dma_long, aes(x=Year, y=Value, color = Variable, group = interaction(DMA, Variable))) +
  geom_line() +
  facet_wrap(~ DMA, scales = "free_y") +
  scale_color_manual(values = c("GT.MVT" = "#CC79A7", 
                                "UCR.MVT_normalized" =  "#0072B2",
                                "NCVS.MVT_normalized" = "#E69F00",
                                "NICB.MVT_normalized" = "#56B4E9"), 
                     labels = c("GT MVT", "UCR MVT (0-100)", 
                                "NCVS MVT (0-100)", "NICB MVT (0-100)"),
                     breaks = c("GT.MVT", "UCR.MVT_normalized", 
                                "NCVS.MVT_normalized", "NICB.MVT_normalized")) +
  scale_linetype_manual(values = "dashed", 
                        guide = guide_legend(title = "Event")) +
  #theme_minimal() +
  labs(
    x = "Year",
    y = "MVT (Scaled from 0 to 100)",
    color = "Variable"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),  # Adjust x-axis labels
    axis.text.y = element_text(size = 14),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12),   # Increase legend text size
    strip.text = element_text(size = 12) # Adjust y-axis labels
  )


ggsave("selected_mvt_year_dma.png", pdma, width = 15, height = 10, bg = "white") 

```

```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
df_state_long <- merged_state_year_data %>%
  pivot_longer(
    cols = c("GT.MVT", "UCR.MVT_normalized"),
    names_to = "Variable",
    values_to = "Value"
  )
```

```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
# Now, plot and map the aesthetics in a way that will include them in the legend
p <- ggplot(df_state_long, aes(x=Year, y=Value, color = Variable, group = interaction(State, Variable))) +
  geom_line() +
  facet_wrap(~ State, scales = "free_y") +
  scale_color_manual(values = c("GT.MVT" = "#CC79A7", 
                                "UCR.MVT_normalized" =  "#0072B2"), 
                     labels = c("GT MVT", "UCR MVT (0-100)")) +
  scale_linetype_manual(values = "dashed", 
                        guide = guide_legend(title = "Event")) +
  #theme_minimal() +
  labs(
    x = "Year",
    y = "MVT (Scaled from 0 to 100)",
    color = "Variable"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 14),  # Adjust x-axis labels
    axis.text.y = element_text(size = 14),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12),   # Increase legend text size
    strip.text = element_text(size = 12) # Adjust y-axis labels
  )

ggsave("mvt_year_state.png", p, width = 25, height = 18, bg = "white") 

```



```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
selected_df_state_long <- df_state_long %>%
  filter(State %in% c("Texas", "California", "New York", "Washington"))

selected_df_state_long$State <- factor(selected_df_state_long$State, 
                                   levels = c("Texas",  
                                              "California", 
                                              "New York",
                                              "Washington" 
                                              ))


```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
# Now, plot and map the aesthetics in a way that will include them in the legend
p <- ggplot(selected_df_state_long, aes(x=Year, y=Value, color = Variable, group = interaction(State, Variable))) +
  geom_line() +
  facet_wrap(~ State, scales = "free_y") +
  scale_color_manual(values = c("GT.MVT" = "#CC79A7", 
                                "UCR.MVT_normalized" =  "#0072B2"), 
                     labels = c("GT MVT", "UCR MVT (0-100)")) +
  scale_linetype_manual(values = "dashed", 
                        guide = guide_legend(title = "Event")) +
  #theme_minimal() +
  labs(
    x = "Year",
    y = "MVT (Scaled from 0 to 100)",
    color = "Variable"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 14),  # Adjust x-axis labels
    axis.text.y = element_text(size = 14),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12),   # Increase legend text size
    strip.text = element_text(size = 12) # Adjust y-axis labels
  )

ggsave("selected_mvt_year_state.png", p, width = 15, height = 10, bg = "white") 

```





```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
year_month_long <- year_month_merged_data %>%
  pivot_longer(
    cols = c("GT.MVT", "UCR.MVT_normalized", "NIBRS.MVT_normalized", "CFS.911.MVT_normalized"),
    names_to = "Variable",
    values_to = "Value"
  )


lockdown_line <- data.frame(xintercept = as.Date("2020-04-01"), label = "COVID-19 lockdown")
```

```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
# Now, plot and map the aesthetics in a way that will include them in the legend
p <- ggplot(year_month_long , aes(x = year_month, y = Value, color = Variable, group = interaction(DMA, Variable))) +
  geom_line() +
  facet_wrap(~ DMA, scales = "free_y") +
  scale_color_manual(values = c("GT.MVT" = "#CC79A7", 
                                "UCR.MVT_normalized" =  "#0072B2",
                                "CFS.911.MVT_normalized" = "#E69F00",
                                "NIBRS.MVT_normalized" = "#56B4E9"), 
                     labels = c("GT MVT", "UCR MVT (0-100)", 
                                "CFS 911 MVT (0-100)", "NIBRS MVT (0-100)"),
                     breaks = c("GT.MVT", "UCR.MVT_normalized", 
                                "CFS.911.MVT_normalized", "NIBRS.MVT_normalized")) +
  geom_vline(data = lockdown_line, aes(xintercept = xintercept, linetype = label), color = "#009E73") +
  scale_linetype_manual(values = "dashed", guide = guide_legend(title = "Event")) +
  #theme_minimal() +
  labs(
    title = "Monthly MVT by DMA",
    x = "Year-Month",
    y = "MVT (Scaled from 0 to 100)",
    color = "Variable"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 14),  # Adjust x-axis labels
    axis.text.y = element_text(size = 14),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12),   # Increase legend text size
    strip.text = element_text(size = 12) # Adjust y-axis labels
  )


ggsave("mvt_year_month_with_911_plot.png", p, width = 25, height = 18, bg = "white") 

# Save the plot to a file with specified dimensions
#ggsave("my_plot.png", plot = p, width = 20, height = 20, units = "cm")
```

```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
selected_year_month_long <- year_month_long %>%
  filter(DMA %in% c("Houston TX","Los Angeles CA", "New York NY",  "Seattle-Tacoma WA"))

selected_year_month_long$DMA <- factor(selected_year_month_long$DMA, 
                                   levels = c("Houston TX","Los Angeles CA", "New York NY",  "Seattle-Tacoma WA"))

```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
# Now, plot and map the aesthetics in a way that will include them in the legend
p <- ggplot(selected_year_month_long, aes(x = year_month, y = Value, color = Variable, group = interaction(DMA, Variable))) +
  geom_line() +
  facet_wrap(~ DMA, scales = "free_y") +
  scale_color_manual(values = c("GT.MVT" = "#CC79A7", 
                                "UCR.MVT_normalized" =  "#0072B2",
                                "CFS.911.MVT_normalized" = "#E69F00",
                                "NIBRS.MVT_normalized" = "#56B4E9"), 
                     labels = c("GT MVT", "UCR MVT (0-100)", 
                                "CFS 911 MVT (0-100)", "NIBRS MVT (0-100)"),
                     breaks = c("GT.MVT", "UCR.MVT_normalized", 
                                "CFS.911.MVT_normalized", "NIBRS.MVT_normalized")) +
  geom_vline(data = lockdown_line, aes(xintercept = xintercept, linetype = label), color = "#009E73") +
  scale_linetype_manual(values = "dashed", guide = guide_legend(title = "Event")) +
  #theme_minimal() +
  labs(
    title = "Monthly MVT by DMA",
    x = "Year-Month",
    y = "MVT (Scaled from 0 to 100)",
    color = "Variable"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 14),  # Adjust x-axis labels
    axis.text.y = element_text(size = 14),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12),   # Increase legend text size
    strip.text = element_text(size = 12) # Adjust y-axis labels
  )


ggsave("selected_mvt_year_month_with_911_plot.png", p, width = 15, height = 10, bg = "white") 
```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}

pdma <- ggplot(cpi_ppi_long, aes(x = year_month, y = Value, color = Variable, group = interaction(DMA, Variable))) +
  geom_point() +
  geom_smooth(data = cpi_ppi_long %>% filter(Variable %in% c("GT.MVT", "UCR.MVT_normalized", "CFS.911.MVT_normalized", "NIBRS.MVT_normalized")), 
              method = "loess",   # Using loess for a smoother, more flexible curve
              se = TRUE,         # Set to FALSE if you do not want to show the confidence interval
              aes(group = Variable, color = Variable),  # Ensure color matches the line color
              formula = y ~ x) +  # Default formula for loess
  facet_wrap(~ DMA, scales = "free_y") +
  scale_color_manual(values = c("GT.MVT" = "#CC79A7", 
                                "UCR.MVT_normalized" = "#0072B2", 
                                "CFS.911.MVT_normalized" = "#E69F00",
                                "NIBRS.MVT_normalized" = "#56B4E9",
                                "Car.CPI" = "#D55E00", 
                                "Semiconductor.PPI" = "#009E73"), 
                     labels = c("GT MVT", "UCR MVT (0-100)", 
                                "CFS 911 MVT (0-100)", "NIBRS MVT (0-100)", 
                                "Car CPI", "Semiconductor PPI"),
                     breaks = c("GT.MVT", "UCR.MVT_normalized", 
                                "CFS.911.MVT_normalized", "NIBRS.MVT_normalized",
                                "Car.CPI", "Semiconductor.PPI")) +
  labs(
    title = "Monthly MVT and CPI by DMA with Smoothed Trend Lines",
    x = "Year and Month",
    y = "Value",
    color = "Variable"
  ) +
  scale_linetype_manual(values = "dashed", 
                        guide = guide_legend(title = "Event")) +
  #theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 14),  # Adjust x-axis labels
    axis.text.y = element_text(size = 14),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 14),   # Increase legend text size
    strip.text = element_text(size = 14) # Adjust y-axis labels
  )

# Save the plot
ggsave("mvt_cpi_month_dma.png", pdma, width = 32, height = 25, bg = "white")


```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
selected_cpi_ppi_long <- cpi_ppi_long %>%
  filter(DMA %in% c("Houston TX","Los Angeles CA", "New York NY",  "Seattle-Tacoma WA"))

selected_cpi_ppi_long$DMA <- factor(selected_cpi_ppi_long$DMA, 
                                   levels = c("Houston TX","Los Angeles CA", "New York NY",  "Seattle-Tacoma WA"))
```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}

selected_pdma <- ggplot(selected_cpi_ppi_long, aes(x = year_month, y = Value, color = Variable, group = interaction(DMA, Variable))) +
  geom_point() +
  geom_smooth(data = selected_cpi_ppi_long %>% filter(Variable %in% c("GT.MVT", "UCR.MVT_normalized", "CFS.911.MVT_normalized", "NIBRS.MVT_normalized")), 
              method = "loess",   # Using loess for a smoother, more flexible curve
              se = TRUE,         # Set to FALSE if you do not want to show the confidence interval
              aes(group = Variable, color = Variable),  # Ensure color matches the line color
              formula = y ~ x) +  # Default formula for loess
  facet_wrap(~ DMA, scales = "free_y", nrow = 2) +
  scale_color_manual(values = c("GT.MVT" = "#CC79A7", 
                                "UCR.MVT_normalized" = "#0072B2", 
                                "CFS.911.MVT_normalized" = "#E69F00",
                                "NIBRS.MVT_normalized" = "#56B4E9",
                                "Car.CPI" = "#D55E00", 
                                "Semiconductor.PPI" = "#009E73"), 
                     labels = c("GT MVT", "UCR MVT (0-100)", 
                                "CFS 911 MVT (0-100)", "NIBRS MVT (0-100)", 
                                "Car CPI", "Semiconductor PPI"),
                     breaks = c("GT.MVT", "UCR.MVT_normalized", 
                                "CFS.911.MVT_normalized", "NIBRS.MVT_normalized",
                                "Car.CPI", "Semiconductor.PPI")) +
  labs(
    title = "Monthly MVT and CPI by DMA with Smoothed Trend Lines",
    x = "Year and Month",
    y = "Value",
    color = "Variable"
  ) +
  scale_linetype_manual(values = "dashed", 
                        guide = guide_legend(title = "Event")) +
  #theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 14),  # Adjust x-axis labels
    axis.text.y = element_text(size = 14),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 14),   # Increase legend text size
    strip.text = element_text(size = 14) # Adjust y-axis labels
  )

# Save the plot
ggsave("selected_mvt_cpi_month_dma.png", selected_pdma, width = 15, height = 10, bg = "white")


```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
# Function to plot data by DMA for MVT columns


plot_MVT_by_DMA <- function(data, dma) {
  data_filtered <- filter(data, DMA == dma)
  
  # Extracting names of columns that end with 'MVT'
  mvt_cols <- grep("MVT", names(data_filtered), value = TRUE)
  
  # List to store plots for each MVT column
  plots <- list()
  
  # Create a plot for each MVT column
  for(mvt_col in mvt_cols) {
    if (any(is.na(data_filtered[[mvt_col]]))) {
      next  # 'next' skips this iteration, effectively doing nothing for this row
    } else {
      p <- ggplot(data_filtered, aes(x = Year.Month, y = !!sym(mvt_col))) +
        geom_point(color = "gray", size = 1) +
        #geom_line(aes(y = !!sym(mvt_col)), color = "gray", size = 1) +
        #labs(title = paste(mvt_col)) +
        geom_vline(xintercept = as.numeric(as.Date("2020-04-01")), 
                   color = "firebrick", linetype = "dashed", ) +
        annotate("text", x = as.Date("2020-04-01"), y = max(data_filtered[[mvt_col]])-3, label = "COVID-19 Lockdown", hjust = 0.5, color = "firebrick", size = 3)+
        theme_minimal()  +  labs(x = " ")
      
      # Fit a regression model and add the fitted line
      ts_model <- lm(formula = as.formula(paste(mvt_col, "~ Year.Month + D + P")), data = data_filtered)
      ts_fitted <- predict(ts_model, newdata = data_filtered)
      
      # Add regression line in parts
      fitted_data <- data_filtered %>%
        mutate(fitted = ts_fitted,
               is_after = Year.Month > as.Date("2020-04-01"))
      
      # Add regression line after treatment as counter-factual
      fitted_data_counter_factual <- filter(fitted_data, !is_after) 
      ts_model_counter_factual <- lm(formula = as.formula(paste(mvt_col, "~ Year.Month + D + P")), data = fitted_data_counter_factual)
      ts_fitted_counter <- predict(ts_model_counter_factual, newdata = data_filtered) 
      
      fitted_data <- fitted_data %>% mutate(fitted2 = ts_fitted_counter)
      
       # Adding lines with legends
      p <- p + geom_line(data = filter(fitted_data, !is_after), 
                         aes(y = fitted, color = "Prediction (Before)"), 
                         lwd = 1, show.legend = FALSE) +
               geom_line(data = filter(fitted_data, is_after), 
                         aes(y = fitted2, color = "Counterfactual (After)"),
                         linetype = "dashed", lwd = 1, show.legend = FALSE) +
               geom_line(data = filter(fitted_data, is_after), 
                         aes(y = fitted, color = "Prediction (After)"), 
                         lwd = 1, show.legend = FALSE) + scale_color_manual(values = c("Prediction (Before)" = "lightblue",
                                "Counterfactual (After)" = "#D55E00",
                                "Prediction (After)" = "#0072B2"))

      # Add the plot to the list
      plots[[mvt_col]] <- p
    }
  }
  combined_plot <- plot_grid(plotlist = plots, ncol = 1, nrow = length(plots), align = "v")+ 
    theme(plot.background = element_rect(color = "black", size = 1, fill = NA)) 
  
  title_plot <- ggdraw() + draw_label(paste(dma), fontface = 'bold', x = 0.5, hjust = 0.5) +
    theme(plot.margin = margin(0, 0, 10, 0))
  
  final_plot <- plot_grid(title_plot, combined_plot, ncol = 1, rel_heights = c(0.05, 0.95))   # Adjust relative heights if necessa
  return(final_plot)

}
```



```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
unique_DMA <- unique(its_data_for_fig$DMA)
combined_plots <- lapply(unique_DMA, function(dma) plot_MVT_by_DMA(its_data_for_fig , dma))
```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
# Create a dummy plot for the purpose of generating a legend
legend_plot <- ggplot(its_data_for_fig, aes(x = Year.Month, y = GT.MVT)) +
  geom_line(aes(color = "Prediction Line (Before)")) +
  geom_line(aes(color = "Counterfactual Line (After)")) +
  geom_line(aes(color = "Prediction Line (After)")) +
  geom_point(aes(color = "Observed")) +
  scale_color_manual(values = c("Prediction Line (Before)" = "lightblue",
                                "Counterfactual Line (After)" = "#D55E00",
                                "Prediction Line (After)" = "#0072B2",
                                "Observed" = "grey"),
                     breaks = c("Prediction Line (Before)", "Counterfactual Line (After)", "Prediction Line (After)", "Observed")) +
  labs(color = "Legend") +
  theme(legend.position = "bottom",
        legend.title = element_text(size = 15), 
        legend.text = element_text(size = 15),
        legend.direction = "vertical")+
  guides(color = guide_legend(title.position = "top"))

# Extract the legend
legend <- get_legend(legend_plot)
```




```{r, eval=params$run_figure, results = 'hide', echo=FALSE}

final_combined_plot <- plot_grid(plotlist = combined_plots, ncol = 7, 
                                 align = 'v')  

legend_plot_2 <- ggdraw(legend)

final_plot_with_legend <- plot_grid(final_combined_plot, legend_plot_2, ncol = 2, rel_widths = c(0.95, 0.05))

ggsave("its_plot.png", final_plot_with_legend, width = 40, height = 30, bg = "white")

```



```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
#selected DMA
selected_its_data_for_fig <- its_data_for_fig %>%
  filter(DMA %in% c("Houston TX","Los Angeles CA", "New York NY",  "Seattle-Tacoma WA"))


selected_its_data_for_fig$DMA <- factor(selected_its_data_for_fig$DMA, 
                                   levels = c("Houston TX","Los Angeles CA", "New York NY",  "Seattle-Tacoma WA"))

```




```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
selected_unique_DMA <- unique(selected_its_data_for_fig$DMA)
selected_combined_plots <- lapply(selected_unique_DMA, function(dma) plot_MVT_by_DMA(selected_its_data_for_fig , dma))


selected_combined_plots <- plot_grid(plotlist = selected_combined_plots, ncol = 2,   align = 'v')  

selected_final_plot_with_legend <- plot_grid(
  selected_combined_plots, 
  legend, 
  ncol = 2, 
  rel_widths = c(1, 0.2)  # Adjust the relative width of the legend column if needed
)

ggsave("selected_its_plot.png", selected_final_plot_with_legend, width = 15, height = 10, bg = "white")

```




##Code For Forest plots
```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
ucr_list_1 <- list(ucr_state_gls, 
                   ucr_state_plm, 
                   dma_m1, 
                   dx1, 
                   dmo1, 
                   dm1, 
                   dma_m3, dx3, dma_m5, dx5, dmo5, dm5, dmo9, dm9)
ucr_list_1_names <- c(
  "UCR State-Year GLS", "UCR State-Year FE", 
  "UCR DMA-Year GLS", "UCR DMA-Year FE", 
  "UCR DMA-Month GLS", "UCR DMA-Month FE",
  "NCVS DMA-Year GLS", "NCVS DMA-Year FE",
  "NICB DMA-Year GLS", "NICB DMA-Year FE",
  "CFS 911 DMA-Month GLS", "CFS 911 DMA-Month FE",
  "NIBRS DMA-Month GLS", "NIBRS DMA-Month FE")
```




```{r, eval=params$run_figure, results = 'hide', echo=FALSE}

extract_stats_single <- function(model_, model_name) {
    # Detect model type based on class
    model_type <- class(model_)[1]  # Assuming the primary class is indicative of the model type
    
    if (model_type == "plm") {
        # Extract data for plm models
        coefficient <- coef(model_)[1]  # Gets the coefficients
        se <- summary(model_)$coefficients[, "Std. Error"][1]  # Standard errors
        
        data.frame(
            Model = model_name,
            Estimate = coefficient,
            LowerCI = coefficient - 1.96 * se,
            UpperCI = coefficient + 1.96 * se,
            N = nrow(model_$model),
            SE = se,
            variance = se^2
        )
    } else if (model_type == "gls") {
        # Extract data for gls models
        coefficient <- summary(model_)$tTable[, "Value"][2]
        se <- summary(model_)$tTable[, "Std.Error"][2]
        
        data.frame(
            Model = model_name,
            Estimate = coefficient,
            LowerCI = coefficient - 1.96 * se,
            UpperCI = coefficient + 1.96 * se,
            N = model_$dims[1],
            SE = se,
            variance = se^2
        )
    } 
}



```

```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
ucr_list_1_control <- list(ucr_state_gls_control, 
                       ucr_state_plm_control,
                       #dma year gls control common
                       dma_m11,
                       #dma year plm control common
                       dx11,
                       dma_m31, dx31, dma_m51, dx51)
ucr_list_1_control_names <- c(
  "UCR State-Year GLS with Controls", 
  "UCR State-Year FE with Controls", 
  "UCR DMA-Year GLS with Controls", 
  "UCR DMA-Year FE with Controls", 
  "NCVS DMA-Year GLS with Controls", "NCVS DMA-Year FE with Controls",
  "NICB DMA-Year GLS with Controls", "NICB DMA-Year FE with Controls")

```

```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
extract_stats_control <- function(model_, model_name) {
    # Detect model type based on class
    model_type <- class(model_)[1]  # Assuming the primary class is indicative of the model type
    
    if (model_type == "plm") {
        # Extract data for plm models
        coefficient <- coef(model_)[1]  # Gets the coefficients
        se <- summary(model_)$coefficients[, "Std. Error"][1]  # Standard errors
        
        data.frame(
            Model = model_name,
            Estimate = coefficient,
            LowerCI = coefficient - 1.96 * se,
            UpperCI = coefficient + 1.96 * se,
            N = nrow(model_$model),
            SE = se,
            variance = se^2
        )
    } else if (model_type == "gls") {
        # Extract data for gls models3
        coefficient <- summary(model_)$tTable[, "Value"][2]
        se <- summary(model_)$tTable[, "Std.Error"][2]
        
        data.frame(
            Model = model_name,
            Estimate = coefficient,
            LowerCI = coefficient - 1.96 * se,
            UpperCI = coefficient + 1.96 * se,
            N = model_$dims[1],
            SE = se,
            variance = se^2
        )
    } 
}
```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
#UCR single regression
model_list <- mapply(FUN = extract_stats_single, 
                           model = ucr_list_1, 
                           model_name = ucr_list_1_names,
                           SIMPLIFY = FALSE) 
```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
#UCR regression with control varaibles
model_stats_control_list <- mapply(FUN = extract_stats_control, 
                           model = ucr_list_1_control, 
                           model_name = ucr_list_1_control_names,
                           SIMPLIFY = FALSE) 

all_combined_stats_a <- do.call(rbind, model_list)
all_combined_stats_b <- do.call(rbind, model_stats_control_list)

final_fp <- rbind(all_combined_stats_a, all_combined_stats_b)

#unmatched <- setdiff(unique(final_fp$Model), custom_order)
```






```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
#SORT by custom order
# Define custom order
custom_order <- c(
  "UCR State-Year GLS", "UCR State-Year GLS with Controls",
  "UCR State-Year FE", "UCR State-Year FE with Controls", 
  "UCR DMA-Year GLS",  "UCR DMA-Year GLS with Controls", 
  "UCR DMA-Year FE", "UCR DMA-Year FE with Controls", 
  "UCR DMA-Month GLS",  "UCR DMA-Month FE", 
  "NCVS DMA-Year GLS", "NCVS DMA-Year GLS with Controls", 
  "NCVS DMA-Year FE",  "NCVS DMA-Year FE with Controls",
  "NICB DMA-Year GLS", "NICB DMA-Year GLS with Controls",
  "NICB DMA-Year FE",   "NICB DMA-Year FE with Controls",
  "CFS 911 DMA-Month GLS", "CFS 911 DMA-Month FE",
  "NIBRS DMA-Month GLS", "NIBRS DMA-Month FE")

# Convert the 'Model' column to a factor with levels defined by custom order
final_fp$Model <- factor(final_fp$Model, levels = custom_order)

# Arrange the dataframe by the 'Model' column
final_fp <- final_fp %>% 
  arrange(Model)
```




```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
library(forestplot)
table_text <- cbind(
  custom_order,
  sprintf("%.2f", final_fp$Estimate),
  paste("(", sprintf("%.2f", final_fp$LowerCI), "-", sprintf("%.2f", final_fp$UpperCI), ")"),
  final_fp$N
)

# Convert to matrix for the forestplot function, ensuring no header names are included
table_text_matrix <- matrix(table_text, ncol = 4, byrow = FALSE)
rownames(table_text_matrix) <- NULL


# Forestplot call

tiff("forestplot.tiff", units = "in", width = 9, height = 7, res = 300)
forestplot(labeltext = table_text_matrix,
           mean = as.numeric(final_fp$Estimate),
           lower = as.numeric(final_fp$LowerCI),
           upper = as.numeric(final_fp$UpperCI),
           zero = 0,  # Line of no effect
           xlab = "Effect size",
           boxsize = 0.1,
           title = "Forest Plot of the Linear Relationship Between GT MVT and Various Crime Statistics (By Regression Models)",
           txt_gp = fpTxtGp(cex=0.7, ticks=gpar(cex= 0.5)),
           lineheight = "auto",
           col = fpColors(box = "royalblue", line = "darkblue", summary = "darkred"),
           vertices = TRUE) %>%
  fp_set_style(box = "royalblue", line = "darkblue", summary = "royalblue", hrz_lines = "#999999") %>%
  fp_add_header("Model Name", "Estimates", "95% CI", "N") %>%
  fp_set_zebra_style("#f9f9f9")
dev.off()
```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
# Load the magick package
library(magick)

# Read the TIFF image
image <- image_read("forestplot.tiff")

# Convert and save the image as PNG
image_write(image, "forestplot.png", format = "png")
```

###################################################

##########Log Forest Plot##########################
##Code For Forest plots


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
ucr_list_log <- list(ucr_state_gls_log, 
                   ucr_state_plm_log, 
                   dma_m2, 
                   dx2, 
                   dmo3, 
                   dm3, 
                   dma_m4, dx4, dma_m6, dx6, dmo7, dm7, dmo11, dm11)
ucr_list_log_names <- c(
  "UCR(log) State-Year GLS", "UCR(log) State-Year FE", 
  "UCR(log) DMA-Year GLS", "UCR(log) DMA-Year FE", 
  "UCR(log) DMA-Month GLS", "UCR(log) DMA-Month FE",
  "NCVS(log) DMA-Year GLS", "NCVS(log) DMA-Year FE",
  "NICB(log) DMA-Year GLS", "NICB(log) DMA-Year FE",
  "CFS 911(log) DMA-Month GLS", "CFS 911(log) DMA-Month FE",
  "NIBRS(log) DMA-Month GLS", "NIBRS(log) DMA-Month FE")
```



```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
ucr_list_log_control <- list(ucr_state_gls_control_log, 
                       ucr_state_plm_control_log,
                       #dma year gls control common
                       dma_m21,
                       #dma year plm control common
                       dx21,
                       dma_m41, dx41, dma_m61, dx61)
ucr_list_log_control_names <- c(
  "UCR(log) State-Year GLS with Controls", 
  "UCR(log) State-Year FE with Controls", 
  "UCR(log) DMA-Year GLS with Controls", 
  "UCR(log) DMA-Year FE with Controls", 
  "NCVS(log) DMA-Year GLS with Controls", "NCVS(log) DMA-Year FE with Controls",
  "NICB(log) DMA-Year GLS with Controls", "NICB(log) DMA-Year FE with Controls")

```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
#UCR single regression
log_model_list <- mapply(FUN = extract_stats_single, 
                           model = ucr_list_log, 
                           model_name = ucr_list_log_names,
                           SIMPLIFY = FALSE) 
```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
#UCR regression with control varaibles
log_model_stats_control_list <- mapply(FUN = extract_stats_control, 
                           model = ucr_list_log_control, 
                           model_name = ucr_list_log_control_names,
                           SIMPLIFY = FALSE) 

all_combined_stats_log_a <- do.call(rbind, log_model_list)
all_combined_stats_log_b <- do.call(rbind, log_model_stats_control_list)

log_final_fp <- rbind(all_combined_stats_log_a, all_combined_stats_log_b)
```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
# Define custom order
log_custom_order <- c(
  "UCR(log) State-Year GLS", "UCR(log) State-Year GLS with Controls", 
  "UCR(log) State-Year FE", "UCR(log) State-Year FE with Controls", 
  "UCR(log) DMA-Year GLS", "UCR(log) DMA-Year GLS with Controls", 
  "UCR(log) DMA-Year FE", "UCR(log) DMA-Year FE with Controls", 
  "UCR(log) DMA-Month GLS", 
  "UCR(log) DMA-Month FE",
  "NCVS(log) DMA-Year GLS", "NCVS(log) DMA-Year GLS with Controls",
  "NCVS(log) DMA-Year FE", "NCVS(log) DMA-Year FE with Controls",
  "NICB(log) DMA-Year GLS", "NICB(log) DMA-Year GLS with Controls",
  "NICB(log) DMA-Year FE", "NICB(log) DMA-Year FE with Controls",
  "CFS 911(log) DMA-Month GLS", 
  "CFS 911(log) DMA-Month FE",
  "NIBRS(log) DMA-Month GLS", 
  "NIBRS(log) DMA-Month FE")

unmatched <- setdiff(unique(log_final_fp$Model), log_custom_order)
#unmatched

```






```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
#SORT by custom order
# Convert the 'Model' column to a factor with levels defined by custom order
log_final_fp$Model <- factor(log_final_fp$Model, levels = log_custom_order)

# Arrange the dataframe by the 'Model' column
log_final_fp <- log_final_fp %>% 
  arrange(Model)


```




```{r, eval=params$run_figure, results = 'hide', echo=FALSE}

log_table_text <- cbind(
  log_custom_order,
  sprintf("%.4f", log_final_fp$Estimate),
  paste("(", sprintf("%.4f", log_final_fp$LowerCI), "-", sprintf("%.4f", log_final_fp$UpperCI), ")"),
  log_final_fp$N
)

# Convert to matrix for the forestplot function, ensuring no header names are included
log_table_text_matrix <- matrix(log_table_text, ncol = 4, byrow = FALSE)
rownames(log_table_text_matrix) <- NULL


# Forestplot call

tiff("log_forestplot.tiff", units = "in", width = 9, height = 7, res = 300)
forestplot(labeltext = log_table_text_matrix,
           mean = as.numeric(log_final_fp$Estimate),
           lower = as.numeric(log_final_fp$LowerCI),
           upper = as.numeric(log_final_fp$UpperCI),
           zero = 0,  # Line of no effect
           xlab = "Effect size",
           boxsize = 0.1,
           clip = c(-0.01, 0.01),
           title = "Forest Plot of the Linear Relationship Between GT MVT and Various Logged Crime Statistics (By Regression Models)",
           txt_gp = fpTxtGp(cex=0.7, ticks=gpar(cex= 0.5)),
           lineheight = "auto",
           col = fpColors(box = "royalblue", line = "darkblue", summary = "darkred"),
           vertices = TRUE) %>%
  fp_set_style(box = "royalblue", line = "darkblue", summary = "royalblue", hrz_lines = "#999999") %>%
  fp_add_header("Model Name", "Estimates", "95% CI", "N") %>%
  fp_set_zebra_style("#f9f9f9")
dev.off()
```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
# Load the magick package
library(magick)

# Read the TIFF image
log_image <- image_read("log_forestplot.tiff")

# Convert and save the image as PNG
image_write(log_image, "log_forestplot.png", format = "png")
```


###################################################


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
common_covariate_model_list <- list(
  gt_state_gls_common, gt_state_plm_common,
  m7, x13,
  ucr_state_gls_common, ucr_state_plm_common,
  m1, x7,
  m3, x9,
  m5, x13
  )



#library(QuantPsyc)

common_covariate_model_names <- c("GT State-Year GLS", "GT State-Year FE",
                 "UCR State-Year GLS", "UCR State-Year FE",
                 "GT DMA-Year GLS", "GT DMA-Year FE",
                 "UCR DMA-Year GLS", "UCR DMA-Year FE",
                 "NCVS DMA-Year GLS", "NCVS DMA-Year FE",
                 "NICB DMA-Year GLS", "NICB DMA-Year FE")
```





```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
extract_stats_common <- function(model_, model_name) {
    # Detect model type based on class
    model_type <- class(model_)[1]  # Assuming the primary class is indicative of the model type
    #browser()
    if (model_type == "plm") {
        # Extract data for plm models
        coefficient <- coef(model_)  # Gets the coefficients
        se <- summary(model_)$coefficients[, "Std. Error"]  # Standard errors
        
        x <- data.frame(
            Model = model_name,
            Estimate = coefficient,
            LowerCI = coefficient - 1.96 * se,
            UpperCI = coefficient + 1.96 * se,
            N = nrow(model_$model),
            SE = se,
            variance = se^2
        )
      #browser()
    } else if (model_type == "gls") {
        # Extract data for gls models3
        coefficient <- summary(model_)$tTable[, "Value"]
        se <- summary(model_)$tTable[, "Std.Error"]
        
        data.frame(
            Model = model_name,
            Estimate = coefficient,
            LowerCI = coefficient - 1.96 * se,
            UpperCI = coefficient + 1.96 * se,
            N = model_$dims[1],
            SE = se,
            variance = se^2
        )
    } 
}

```



```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
common_model_list <- mapply(FUN = extract_stats_common, 
                           model = common_covariate_model_list, 
                           model_name = common_covariate_model_names,
                           SIMPLIFY = FALSE) 

all_combined_common <- do.call(rbind, common_model_list)
```


```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
# Convert the 'Model' column to a factor with levels defined by custom order
all_combined_common$Model <- factor(all_combined_common$Model, levels = common_covariate_model_names)

#Get the index to column
library(tibble)
all_combined_common <- as_tibble(all_combined_common, rownames = "Common.Crime.Covariates")

#get rid of the end digits
all_combined_common$Common.Crime.Covariates <- str_remove(all_combined_common$Common.Crime.Covariates, "\\d+$")



filtered_common_data <- all_combined_common[!all_combined_common$Common.Crime.Covariates %in% c(
  'Population..logged.', 'Average.Vehicle.HH', 'Percentage.of.Internet.Subscription.per.Household', '(Intercept)'
),]

# Add a new column extracting the first word from the Model column
filtered_common_data  <- filtered_common_data  %>%
  mutate(Label = sub("([^ ]+).*", "\\1", Model))

# Arrange the dataframe by the 'Model' column
all_combined_common <- all_combined_common %>% 
  arrange(Model, Common.Crime.Covariates)
nrow(all_combined_common)
```



```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
tiff("common_forestplot.tiff", units = "in", width = 9, height = 12, res = 500)
filtered_common_data %>%
  group_by(Common.Crime.Covariates) %>%
  forestplot(labeltext = c(Model),
             mean = Estimate, 
             lower = LowerCI,
             upper = UpperCI,
             zero = 0, 
             boxsize = .1,
             txt_gp = fpTxtGp(label=gpar(cex=0.8), ticks=gpar(cex=0.8)),
             lineheight = "auto",
             clip = c(-35, 103),
             line.margin = .8,
             legend = c("CD Index", "Mobility Index",
                        "Heterogeneity Index", 
                        "% Foreign Born", "% Young Males"),
             legend_args = fpLegend(pos = list(x = 0.85, y = 0.5), gp = gpar(col = "#CCCCCC", fill = "#F9F9F9")),
             title = "Forest Plot: Impact of Common Crime Covariates on GT MVT and Other Official MVT Statistics by Regression Models",
             xlab = "Effect Size") %>%
  fp_set_style(box = c("#D55E00", "#0072B2", "#F0E442", "skyblue","#CC79A7" ) |> lapply(function(x) gpar(fill = x, col = "#555555")),
               default = gpar(vertices = TRUE)) %>% 
  fp_set_zebra_style("#F5F9F9")
dev.off()
```

```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
tiff("common_forestplot_closer.tiff", units = "in", width = 9, height = 12, res = 500)
filtered_common_data %>%
  group_by(Common.Crime.Covariates) %>%
  forestplot(labeltext = c(Model),
             mean = Estimate, 
             lower = LowerCI,
             upper = UpperCI,
             zero = 0, 
             boxsize = .1,
             txt_gp = fpTxtGp(label=gpar(cex=0.8), ticks=gpar(cex=0.8)),
             lineheight = "auto",
             pvalue = 0.05,
             clip = c(-5, 5),
             line.margin = .8,
             legend = c("CD Index", "Mobility Index",
                        "Heterogeneity Index", 
                        "% Foreign Born", "% Young Males"),
             legend_args = fpLegend(pos = list(x = 0.87, y = 0.45), 
                                    gp = gpar(col = "#CCCCCC", 
                                              fill = "#F9F9F9"),
                                    title = "Variables"),
             title = "Forest Plot: Effect of Common Crime Covariates on GT MVT and Other Official MVT Statistics\n(A Detailed Examination Near Zero Line by Regression Models)",
             xlab = "Effect Size",
             ) %>%
  fp_set_style(box = c("#D55E00", "#0072B2", "#F0E442",  "skyblue","#CC79A7" ) %>% lapply(function(x) gpar(fill = x, col = "#555555")),
               default = gpar(vertices = TRUE)) %>% 
  fp_set_zebra_style("#F5F9F9")
dev.off()
```

```{r, eval=params$run_figure, results = 'hide', echo=FALSE}

image <- image_read("common_forestplot_closer.tiff")
image_write(image, "common_forestplot_closer.png", format = "png")

image <- image_read("common_forestplot.tiff")
image_write(image, "common_forestplot.png", format = "png")
```

-->



<!--
# These are codes for testing 1. Forest plots and meta analysis, no time for this so far. 




\clearpage
\newpage
```{r, eval=FALSE}
# These are summay tables for the statistical findings, it is better to use forest plot

sum_t <- read.csv('test_result_summary.csv')
sum_t1 <- xtable(sum_t, align = 'm{0.5cm}|m{1.5cm}|m{5cm}|m{1.2cm}|m{4.5cm}|m{6.5cm}|m{1cm}', 
                 caption = 'Test Result Summary', 
                 label = 'tab:summary_result_table')

# Print the xtable with specific LaTeX modifications
print.xtable(sum_t1, floating.environment = "sidewaystable", 
             type = 'latex', table.placement = '!h', 
             caption.placement = 'top', size = 'tiny', width = '0.8\\textheight', tabular.environment= 'tabularx',
             include.rownames = FALSE, include.colnames = TRUE
             )
             
```
\clearpage

-->


<!--
#unused forestplot



```{r, eval=FALSE}
library(metafor)


# Perform the meta-analysis
res <- rma(yi = Estimate, vi = variance, mods = factor(Model), data = final_fp, method = "REML")

# View the summary
summary(res)


dat <- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg,
              data=final_fp, slab=paste(MVT, N, year, sep=", "))


# Create a forest plot
forest(res, slab = final_fp$ModelName, 
       main = "Forest Plot of Linear Relationship between GT MVT and Official MVT",
       xlab = "Effect Size",
       cex = 0.75, mlab="", shade=TRUE) # This orders the plot by the order of appearance in the data frame

```

```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
library(forestplot)
common_table_text <- cbind(
  common_covariate_model_names,
  sprintf("%.2f", filtered_common_data$Estimate),
  paste("(", sprintf("%.2f", filtered_common_data$LowerCI), "-", sprintf("%.2f", filtered_common_data$UpperCI), ")"),
  filtered_common_data$N
)

# Convert to matrix for the forestplot function, ensuring no header names are included
common_table_text_matrix <- matrix(common_table_text, ncol = 4, byrow = FALSE)
rownames(common_table_text_matrix) <- NULL


# Forestplot call

# Organize data by model for grouping in the plot
df_grouped <- filtered_common_data %>% 
  group_by(Label) %>%
  arrange(Model, Common.Crime.Covariates)

# Initialize vectors for forestplot
label_vector <- c()
mean_vector <- c()
lower_vector <- c()
upper_vector <- c()

# Construct the vectors for plotting
for (grp in unique(df_grouped$Model)) {
  label_vector <- c(label_vector, grp, "", df_grouped$Common.Crime.Covariates[df_grouped$Model == grp])
  mean_vector <- c(mean_vector, NA, NA, df_grouped$Estimate[df_grouped$Model == grp])
  lower_vector <- c(lower_vector, NA, NA, df_grouped$LowerCI[df_grouped$Model == grp])
  upper_vector <- c(upper_vector, NA, NA, df_grouped$UpperCI[df_grouped$Model == grp])
}
```

```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
tiff("common_forestplot.tiff", units = "in", width = 10, height = 16, res = 300)
# Creating the forest plot
forestplot(labeltext = label_vector,
           mean = mean_vector,
           lower = lower_vector,
           upper = upper_vector,
           zero = 0,  # Line of no effect
           xlab = "Effect Size",
           boxsize = 0.1,
           col = fpColors(box = "royalblue", lines = "darkblue", summary = "red"),
           txt_gp = fpTxtGp(label=gpar(cex=0.8), ticks=gpar(cex=0.8)),
           lineheight = "auto",
           new_page = TRUE)
dev.off()
```

```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
# Plotting

corr_and_population <- ggplot(results, aes(x = population.log , y = corr_coeff)) +
  geom_point(alpha = 0.6, size = 1.5) +  # Points with semi-transparency and sized for visibility
  #geom_line(aes(color = "Gap"), size = 1) +
  geom_text(aes(label = DMA), check_overlap = TRUE, hjust = 0.3, vjust = 1, size = 5) +
  geom_smooth(method = "loess")+
  #scale_linetype_manual(values = "solid", name = "Line Type", labels = "Gaps Between UCR and NIBRS") + 
  #scale_color_brewer(palette = "Set2", name = "Legends") + # Color scale for different MVT types
  labs(
    title = "GT MVT and UCR MVT Correlation Coefficient vs. Population (log) at DMA-Month Level",
    x = "Population (log)",
    y = "UCR MVT Correlation Coefficient with GT MVT"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")  # Enhance plot aesthetics and position the legend
print(corr_and_population)
ggsave(filename = "corr_and_population.png", plot = corr_and_population, width = 12, height = 9, dpi = 400, bg = "white")
```

```{r, eval=params$run_figure, results = 'hide', echo=FALSE}


# Ensure 'public.data' is a factor
results$public.data <- as.factor(results$public.data)

# Updated ggplot code with color attribute for both points and text
corr_and_population2 <- ggplot(results, aes(x = population.log, y = corr_coeff)) +
  geom_point(aes(color = public.data, shape = public.data), alpha = 0.6, size = 3) +  # Use color instead of fill for points
  geom_text(aes(label = DMA, color = public.data), check_overlap = FALSE, hjust = 0.5, vjust = 1.3, size = 4, show.legend = FALSE) +
  labs(
    title = "GT MVT and UCR MVT Correlation Coefficient vs. Population (log) at DMA-Month Level",
    x = "Population (log)",
    y = "UCR MVT Correlation Coefficient with GT MVT"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("brown", "skyblue"), name = "Public Access", labels = c("Without Access", "With Access")) +
  scale_shape_manual(values = c(4, 1), name = "Public Access", labels = c("Without Access", "With Access")) +  # Use shapes 4 and 1 for 'X' and 'O'
  theme(legend.position = "bottom", legend.box = "vertical")  # Adjust legend positioning and orientation

# Save the plot
ggsave(filename = "corr_and_population2_interact.png", plot = corr_and_population2, width = 12, height = 9, dpi = 400, bg = "white")



```



```{r, eval=params$run_figure, results = 'hide', echo=FALSE}
results2 <- results
results2$public.data = as.character(results$public.data)

boxplot_corr <- ggplot(results2, aes(x = public.data, y = corr_coeff, fill = public.data)) + 
  geom_boxplot() +
  labs(title = "Boxplot of Correlation Coefficient Between GT MVT and UCR MVT with/without Public Accessible Crime Data",
       x = "Group",
       y = "Correlation Coefficient Between GT MVT and UCR MVT at DMA-Month Level")+ 
  scale_fill_manual(name = "Crime Data Public Access",
                    values = c("0" = "brown", "1" = "#89CFF0"),  # baby blue color code
                    labels = c("Without Access", "With Access")) +
  theme_minimal() +
  theme(legend.title = element_text(size = 14),  # Increases the legend title size
        legend.text = element_text(size = 12)) 
ggsave(filename = "boxplot_corr.png", plot = boxplot_corr, width = 10, height = 8, dpi = 400, bg = "white")
```

-->


<!--

```{r, eval=F, results = 'hide', echo=FALSE}
library(diffr)
diffr("DissertateCUNY2.cls", "DissertateCUNY4.cls", contextSize = 1000, minJumpSize = 100000)
```

-->